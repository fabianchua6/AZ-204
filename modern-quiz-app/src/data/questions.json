[
  {
    "id": "432139a6157dbd4579692cf8d715f98d2f6ec623499dbab8d67b5efb86acd59a",
    "question": "Which of the following components of the API Management service would a developer use if they need to create an account and subscribe to get API keys?",
    "answer": "The Developer portal serves as the main web presence for developers, and is where they can subscribe to get API keys.  \nThe API gateway routes calls, performs API transforms, and verifies keys.  \nThe Azure portal is the administrative interface where you set up your API program.",
    "options": ["API gateway", "Azure portal", "Developer portal"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "04363fe5a3cabd750c902fd5844d51c5c5d22d467cea7bd36b9ba94bc131e569",
    "question": "Which of the following API Management policies would one use if one wants to apply a policy based on a condition?",
    "answer": "The choose policy applies enclosed policy statements based on the outcome of evaluation of boolean expressions.  \nThe forward-request policy forwards the incoming request to the backend service specified in the request context.  \nThe return-response policy aborts pipeline execution and returns either a default or custom response to the caller.",
    "options": ["forward-request", "choose", "return-response"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "cdbaddde8c7828ef08990a136c7aa561123e53ea0ca9f210b5ae9408617471db",
    "question": "Your organization offers web services to third-party clients. These services require non-anonymous access, authentication through OpenID Connect, and are accessed via APIs. To ensure secure Entra ID authentication, you decide to base it on a specific value embedded in the request query parameter. Which policy within Azure API Management should you enforce to meet this requirement?",
    "answer": "The JWT Validation or \"validate-jwt\" policy in Azure API Management is used to validate the JWT (JSON Web Token) extracted from a specified HTTP Header or a URI query parameter. In this scenario, it allows you to securely support Entra ID authentication based on a value passed as a request query parameter. The other options do not provide this specific functionality.",
    "options": ["check-header", "validate-jwt", "set-header", "control-client-flow"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "68c45edc79c3ebcfe23302a9599723d2d52b73cf06728e07197b1a88443e503e",
    "question": "When attempting to access an API through Microsoft's API Management, a developer receives a `401 Access Denied error`. What could be the possible reasons for this error, and how can it be fixed? Select the correct options.",
    "answer": "The key can be included in the request header as \"Ocp-Apim-Subscription-Key\" or as a query string \"subscription-key.\" Additionally, the API must be added to a product in the Azure Portal, which applies to a particular product (a collection of APIs) in API Management.",
    "options": [
      "Add the API to product in Azure Portal",
      "Include the `Ocp-Apim-Subscription-Key` header in the HTTP request.",
      "Add the check-header policy statement for the Authorization header.",
      "Disable OAuth2.0 in the API Management gateway."
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "ee0fad95310e552de6085faa70c5a66149974904c6606257da0db5ee8d59eac5",
    "question": "What rule should be used to serve this URL from cache: `https://myapi.azure-api.net/items/123456`?",
    "answer": "This URL has no query parameters, so you use an empty `<vary-by-query-parameter />` to cache based on the full path.  \n`itemId` and `items` are invalid here, and `cache-lookup-value` is not applicable - it's for retrieving named values from the cache directly.",
    "options": [
      "`<cache-lookup><vary-by-query-parameter /></cache-lookup>`",
      "`<cache-lookup><vary-by-query-parameter>itemId</vary-by-query-parameter></cache-lookup>`",
      "`<cache-lookup-value key=\"itemId\" />`",
      "`<cache-lookup-value value=\"itemId\" />`",
      "`<cache-lookup><vary-by-query-parameter>items</vary-by-query-parameter></cache-lookup>`",
      "`<cache-lookup-value key=\"items\" />`",
      "`<cache-lookup-value value=\"items\" />`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "17f2f622b2afa624a1eb75816e02d614fb888d9bd41bbf4abef8dc62303f8c62",
    "question": "What rule should be used to serve this URL from cache: `https://myapi.azure-api.net/items?id=123456`?",
    "answer": "The URL has a query parameter `id`. To cache per distinct ID, you must vary by `id`.  \n`items` is not valid query parameter. `cache-lookup-value` is for retrieving cached value by name.",
    "options": [
      "`<cache-lookup><vary-by-query-parameter>items</vary-by-query-parameter></cache-lookup>`",
      "`<cache-lookup><vary-by-query-parameter>id</vary-by-query-parameter></cache-lookup>`",
      "`<cache-lookup-value key=\"id\" />`",
      "`<cache-lookup-value value=\"id\" />`",
      "`<cache-lookup-value key=\"items\" />`",
      "`<cache-lookup-value value=\"items\" />`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "a1be67b55483c8a6f3a01d72dcb08fb4202accdd6b7c26b543aa8db48e5c0deb",
    "question": "What rule should be used to serve this URL from cache: `https://myapi.azure-api.net/me`?",
    "answer": "This looks like a user-specific endpoint. The response is typically user-dependent, and the `Authorization` header is often used to distinguish identities.  \nSo, vary the cache by `Authorization` header. The rest of the options are irrelevant or malformed.",
    "options": [
      "`<cache-lookup><vary-by-header>Authorization</vary-by-header></cache-lookup>`",
      "`<cache-lookup><vary-by-query-parameter /></cache-lookup>`",
      "`<cache-lookup><vary-by-query-parameter>me</vary-by-query-parameter></cache-lookup>`",
      "`<cache-lookup-value key=\"me\" />`",
      "`<cache-lookup-value value=\"me\" />`",
      "`<cache-lookup-value key=\"\" />`",
      "`<cache-lookup-value value=\"\" />`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "503302d30fbc18ef15b61c65374973fcc4ca3fd1297bdbc25af27b46791a8e59",
    "question": "In what unit is renewal period for Rate limiting / Quota policy?",
    "answer": "All times are in seconds",
    "options": ["Milliseconds", "Seconds", "Minutes", "Hours", "Days"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "2ad205a4b5f59049a6b33423753323dfe7a4904b9e240f7571325202bdfd42af",
    "question": "In what unit is the bandwidth limit in Quota policy?",
    "answer": "All sizes are in KB",
    "options": ["KB", "MB", "GB"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "8a857bd281f3cf43da00da29e45430c07d13be300c35285f75c8d80412a652db",
    "question": "An API is integrated into an Azure API Management (APIM) gateway and is utilized by client applications worldwide. You are tasked with granting access to 10 new operations exclusively to a select group of 200 beta developers around the world. How can you enable these developers to test the new operations using the existing API URL?",
    "answer": "Header-based versioning uses custom HTTP headers to determine the version of the API to be accessed. This allows different versions of the API to be accessed through the same URL.",
    "options": [
      "Implement a revision in Azure API Management.",
      "Implement path-based versioning.",
      "Implement header-based versioning.",
      "Implement query string-based versioning.",
      "Create separate gateways."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "36cebc512c3735cfcff37709516871a40bb0dc417ddfc5eab7f3b3f104738e41",
    "question": "You are developing a solution that requires the Azure API Management (APIM) instance to authenticate to a backend service. The authentication process must be secure and aligned with best practices. The backend service supports authentication through specific methods, and you need to ensure that the APIM instance can access it without storing credentials within the APIM configuration. Which of the following policies should you apply to the APIM instance to achieve this requirement?",
    "answer": "By using a authentication-managed-identity identity, you can authenticate to services that support Entra ID authentication without credentials in your code. In this scenario, it allows the APIM instance to authenticate to the backend service securely.",
    "options": ["authentication-managed-identity", "validate-jwt", "check-header", "set-body"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "b8af13895a31820a5d120dfc0c8156ecab87bdf59be7e37536f8109399c9d6cf",
    "question": "Your organization has implemented Azure API Management (APIM) and requires a custom TLS/SSL certificate for securing communication. The certificate must be obtained from Azure Key Vault, and the process must adhere to security best practices without hardcoding any secrets or credentials in the APIM configuration. Which of the following policies should you apply to the APIM instance to fulfill this requirement?",
    "answer": "By using a authentication-managed-identity identity, the APIM instance can authenticate to Azure Key Vault and retrieve the custom TLS/SSL certificate securely without needing to store any credentials in the code or configuration.",
    "options": ["authentication-managed-identity", "validate-jwt", "check-header", "set-body"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "78f533392543fe56efdc7442f62d446ad7b5bcfc7e2a1d6551b7a215e4c67fb4",
    "question": "You have JSON endpoint that expects JSON payload. The client sends XML payload. What policy should be applied?",
    "answer": "Request is transformed in inbound section",
    "options": [
      "`xml-to-json-policy` in inbound section",
      "`xml-to-json-policy` in backend section",
      "`xml-to-json-policy` in outbound section",
      "`json-to-xml-policy` in inbound section",
      "`json-to-xml-policy` in backend section",
      "`json-to-xml-policy` in outbound section",
      "No policy should be applied"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "2d51cf23ae45549efccbf0901a0574c3d51be6c46c0554ff308bfa50ce72b0c7",
    "question": "You have JSON endpoint. The client expects XML response. What policy should be applied?",
    "answer": "Response is transformed in outbound section",
    "options": [
      "`xml-to-json-policy` in inbound section",
      "`xml-to-json-policy` in backend section",
      "`xml-to-json-policy` in outbound section",
      "`json-to-xml-policy` in inbound section",
      "`json-to-xml-policy` in backend section",
      "`json-to-xml-policy` in outbound section",
      "No policy should be applied"
    ],
    "answerIndexes": [5],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "878c71095c034c708745cfeb10bafe250f9e5bc384b7fce075dc41416ff40212",
    "question": "You receive `401 Access Denied` trying to get access to your API instance. What is happening?",
    "answer": "Pass a valid subsription key.  \nServer errors are `5XX`, quota and ip restrictions are `403`.",
    "options": [
      "Server crashed",
      "You have to pass a valid subscription key.",
      "Quota has been exceeded",
      "IP Filter policy has been activated"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "5cb431738ad408b43dc62fd7928259120995a681b33133978b304345b0d8305e",
    "question": "You receive `403 Forbidden` trying to get access to your API instance. What are possible reasons for that?",
    "answer": "Quota and ip restrictions are `403`.  \nServer errors are `5XX`, Subscription errors are `401`",
    "options": [
      "Server crashed",
      "You have to pass a valid subscription key.",
      "Quota has been exceeded",
      "IP Filter policy has been activated"
    ],
    "answerIndexes": [2, 3],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "29f0f64ae38d09bb91b184410bd83a83d83e90c82c5b5dacc51cc301e9e04cf7",
    "question": "Your client, once again, complains they receive a `403 Forbidden` error when trying to access your API. It was working just 5 minutes ago, they claim. You look into this \"urgent\" problem and see no policy changes have been made, and surprise, surprise, it works on your machine(tm). How would you make client to shut up and be happy?",
    "answer": "A `403 Forbidden` error is either an IP filter policy or an exceeded quota. Since policy rules haven't been modified, it must be a quota issue.  \nNote: Cursing your clients (in your mind) is a valid/invalid solution, depending on your morals.",
    "options": [
      "The API endpoint must be faulty, so just remove it for now and debug it later, like everything else",
      "Add an `ip-filter` policy that allows access to your client's IP, because that's never been a problem before",
      "Call them idiots (in your head, of course) and tell them to use a different subscription key, like you've told them a hundred times",
      "Modify the quota policy to be more generous, because probably they have been spamming your endpoint"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "e89966f40a5abb69b75d25bb0877934de5154c7c91d5aac29f70f4aa96b3364c",
    "question": "You are setting up an API Management instance to manage your organization's various API services. You have a REST API developed in-house that is already exposed to the public internet. What is the first step you should take before incorporating this REST API into the API Management instance?",
    "answer": "OpenAPI specification enables Azure API Management to automatically discover the endpoints and methods supported by the API.",
    "options": [
      "Establish a VPN connection between the in-house network and Azure.",
      "Generate OpenAPI specification for the REST API.",
      "Move the API to Azure Functions.",
      "Implement OAuth 2.0 authentication for the API.",
      "Set up a load balancer for the API in Azure.",
      "Set up a self-hosted gateway between the internal network and Azure."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "7097bb3fb30b84344638a319e628ad63750cfe7a3f2fde4b402cf07e9f52904e",
    "question": "In a company's API system hosted behind an Azure API Management service, you are tasked with implementing response caching. The user ID of the client must first be detected and saved. Then, the response must be cached specifically for that saved user ID. What types of policies should be used to accomplish this task?",
    "answer": "Both inbound and outbound policies\n\n- Inbound Policy: The inbound policy is used to extract and save the user ID from the incoming request. The `<set-variable>` policy is used to save the user ID, and the `<cache-lookup>` policy with a custom key is used to check if a cached response exists for that user ID.\n- Outbound Policy: The outbound policy is used to store the response in the cache. The `<cache-store>` policy with a custom key is used to cache the response specifically for the saved user ID.\n\nTherefore, both inbound and outbound policies are needed to meet the requirements.",
    "options": [
      "Inbound policy only",
      "Outbound policy only",
      "Both inbound and outbound policies",
      "Backend policy only",
      "Global policy only"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "461f77bd010119f38b27f9c3d8a3c78d55c772860b3568d7d8e3ce61f5e2b776",
    "question": "A company uses Azure API Management to publish APIs for external consultants. The API needs to forward the user ID associated with the subscription key to the back-end service. Which type of policy should be used for this requirement?",
    "answer": "Forwarding the user ID that is associated with the subscription key to the back-end service would be done in an inbound policy.\n\n```xml\n<policies>\n    <inbound>\n        <set-header name=\"x-user-id\" exists-action=\"override\">\n            <value>@(context.Subscription.Id)</value>\n        </set-header>\n    </inbound>\n</policies>\n\n```",
    "options": ["Inbound", "Outbound", "Backend", "Error"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "API Management"
  },
  {
    "id": "4a1cc5ce9348f5aba20c62f91baa000475a28c880f6c1c5ca909ba66803d1929",
    "question": "Create a new resource group in the West US region with name `MyResourceGroup`\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group create -l westus -n MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "f2ff95d70919f644eeed4a672c09058cf9d4077b57ffe54464f3867fa4f86bf4",
    "question": "Delete `MyResourceGroup` resource group.\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group delete -n MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "9b9bd9159cb39889752f65429f9d45a3817e673d9003a1f6a289d65fb93c15c5",
    "question": "Force delete all the Virtual Machines in `MyResourceGroup` resource group.\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group delete -n MyResourceGroup --force-deletion-types Microsoft.Compute/virtualMachines\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "dfab2a8c5580f1126ea0105eb06e945d0449dc0b681998ae972b2f09b89bff71",
    "question": "Check if `MyResourceGroup` exists.\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group exists -n MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "78d6217ff412505c6492bc3b20ad91722092788679dda30180969d6d724d0321",
    "question": "List all resource groups located in the West US region.\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group list --query \"[?location=='westus']\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "7aaaf57c9865596050ffe3f65f1ae09a816c8a2246b4aed920b284527e8e7b24",
    "question": "Update `MyResourceGroup` resource group. Set CostCenter tag to `{\"Dept\":\"IT\",\"Environment\":\"Test\"}`\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group update --resource-group MyResourceGroup --set tags.CostCenter='{\"Dept\":\"IT\",\"Environment\":\"Test\"}'\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "d2a3a68acc1c9c21f7e9222a528cca8f8a5c9ab34fd7a4024d89d52d95a58b42",
    "question": "Wait until `MyResourceGroup` resource group is created\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group wait --created  --resource-group MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "27ae81a3c84e277afa3729c49d09a56789013bf0583157fba18a68e3756eb6c1",
    "question": "Wait until `MyResourceGroup` resource group is deleted\n\n```ps\naz group # Options here\n```",
    "answer": "\n\n```ps\naz group wait --deleted --resource-group MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "0d6d6fd517055d4814fdc9973505acf150b3d91f8b3bad78106a1a1f80132291",
    "question": "Create a storage account `mystorageaccount` in new resource group `MyResourceGroup` in the West US region with locally redundant storage.\n\n```ps\naz # Options here\n```",
    "answer": "\n\n```ps\naz group create -l westus -n MyResourceGroup\naz storage account create -n mystorageaccount -g MyResourceGroup -l westus --sku Standard_LRS\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "dfb5638425dbb7a021bbceaefbb775bd02106e8cffa40ec00c2c15abd9f672c1",
    "question": "Create a storage account `mystorageaccount` in new resource group `MyResourceGroup` in the eastus2euap region with account-scoped encryption key enabled for Table Service.\n\n```ps\naz # Options here\n```",
    "answer": "\n\n```ps\naz group create -l eastus2euap -n MyResourceGroup\naz storage account create -n mystorageaccount -g MyResourceGroup --kind StorageV2 -l eastus2euap -t Account\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "5286a85468641c64409244bf9b83ffaaef25b71a68b17b29a33df9c219d17a1a",
    "question": "Delete `MyStorageAccount` storage account in `MySubscription` subscription in `MyResourceGroup` using a resource ID.\n\n```ps\naz storage # Options here\n```",
    "answer": "\n\n```ps\naz storage account delete --ids /subscriptions/MySubscription/resourceGroups/MyResourceGroup/providers/Microsoft.Storage/storageAccounts/MyStorageAccount\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "2b1c1f58e465aace18a4299a3f091615e901fd065ccdad6f16a195bf97ab4175",
    "question": "Delete a storage account using `MyStorageAccount` account name and `MyResourceGroup` resource group.\n\n```ps\naz storage # Options here\n```",
    "answer": "\n\n```ps\naz storage account delete -n MyStorageAccount -g MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "6256f79526014776e4438e758b5a44bdb819f656739970174a3915abc769be78",
    "question": "Failover `mystorageaccount` storage account from resource group `MyResourceGroup`.\n\n```ps\naz storage # Options here\n```",
    "answer": "\n\n```ps\naz storage account failover -n mystorageaccount -g MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "796dfaff4e69c146a35c809bdb29554e5fbc6282304b0750be4f5e389e052b68",
    "question": "Failover `mystorageaccount` storage account from resource group `MyResourceGroup` without waiting to complete.\n\n```ps\naz storage # Options here\n```",
    "answer": "\n\n```ps\naz storage account failover -n mystorageaccount -g MyResourceGroup --no-wait\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "eeff4500b1c485f4e0c788b17993d47c9d00a1fc022a3c5cd6f0983fbe69bb80",
    "question": "Generate a sas token for `MyStorageAccount` account that is valid for queue and table services on Linux.\n\n```ps\nend=`date -u -d \"30 minutes\" '+%Y-%m-%dT%H:%MZ'`\naz storage # Options here\n```",
    "answer": "\n\n```ps\nend=`date -u -d \"30 minutes\" '+%Y-%m-%dT%H:%MZ'`\naz storage account generate-sas --permissions cdlruwap --account-name MyStorageAccount --services qt --resource-types sco --expiry $end -o tsv\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "8cf218ec4b4fcd832730ca2c177d9bbf8a59e7b8966c991289f25f1e0a4a838a",
    "question": "Generate a sas token for `MyStorageAccount` account that is valid for queue and table services on MacOS.\n\n```ps\nend=`date -v+30M '+%Y-%m-%dT%H:%MZ'`\naz storage # Options here\n```",
    "answer": "\n\n```ps\nend=`date -v+30M '+%Y-%m-%dT%H:%MZ'`\naz storage account generate-sas --permissions cdlruwap --account-name MyStorageAccount --services qt --resource-types sco --expiry $end -o tsv\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "bf061a9a0f2d1f0f522c5435203dc8443aaf7be56e2fbe58787061c3b77c332f",
    "question": "Generate a shared access signature for `MyStorageAccount` account for resource types container object; permissions: add, create, update, write for all services. Only permit requests made with the HTTPS protocol.\n\n```ps\naccountKey='00000000'\nexpiry='2020-01-01'\naz storage # Options here\n```",
    "answer": "\n\n```ps\naccountKey='00000000'\nexpiry='2020-01-01'\naz storage account generate-sas --account-key $accountKey --account-name MyStorageAccount --expiry $expiry --https-only --permissions acuw --resource-types co --services bfqt\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "f2505086e3f5cdec3db27941943b157271141de5196a98d733b5ddca693d6e38",
    "question": "List all storage accounts in `MyResourceGroup` resource group.\n\n```ps\naz storage # Options here\n```",
    "answer": "\n\n```ps\naz storage account list -g MyResourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "AZ CLI"
  },
  {
    "id": "83296b26fde685bf908675c01ca00298b3c94368433d28eb7b60ad61da170d53",
    "question": "Which type of encryption does Azure App Configuration use to encrypt data at rest?",
    "answer": "Azure App Configuration encrypts sensitive information at rest using a 256-bit AES encryption key provided by Microsoft.",
    "options": ["64-bit AES", "128-bit AES", "256-bit AES"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "62b96fa6362d5451dedf3b41d94c2e952f3e9baa179cc0af956469a87438e0d0",
    "question": "Which of the following options evaluates the state of a feature flag?",
    "answer": "A filter is a rule for evaluating the state of a feature flag. A user group, a device or browser type, a geographic location, and a time window are all examples of what a filter can represent.  \nA feature manager is an application package that handles the lifecycle of all the feature flags in an application.  \nA feature flag is a variable with a binary state of on or off.",
    "options": ["Feature flag", "Feature manager", "Filter"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "1bc7c8c1f567a24116641d4bfc5c1c6e492f13d51854116829500ba665808172",
    "question": "What is the purpose of labels in Azure App Configuration?",
    "answer": "A feature manager provides extra functionality, such as caching feature flags and updating their states.  \nA feature flag is a variable with a binary state of on or off.  \nA filter is a rule for evaluating the state of a feature flag.",
    "options": [
      "Labels are used to differentiate key-values with the same key in App Configuration.",
      "Labels are used to encrypt key-values in App Configuration.",
      "Labels are used to limit the size of key-values in App Configuration.",
      "A feature manager is a rule for evaluating the state of a feature flag.",
      "A feature manager is a variable with a binary state of on or off.",
      "A feature manager is an application package that handles the lifecycle of all the feature flags in an application."
    ],
    "answerIndexes": [0, 5],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "e81669b57495d3296bd8ead143be468b3f860102b4380c2dcd5cc39dffff82db",
    "question": "You are building a web application that uses Azure App Configuration to manage feature rollouts. You want to introduce a new feature and gradually enable it for a subset of users based on specific conditions. Which components must you define in Azure App Configuration to support this feature flag behavior?",
    "answer": "\n\n- **Name** – Required to identify the feature flag.\n- **List of Filters** – Used to define conditions for feature activation (e.g., % rollout, user targeting).\n\nIncorrect options:\n\n- Keys and Values – Used for general configuration, not feature flags.\n- Labels – Optional; useful for versioning but not required for a flag.",
    "options": ["Name", "Keys and Values", "List of Filters", "Labels"],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "855d1d190c7f5f6b7686c41e2ed54a526ec37a91ee683c751e13a0befd02c326",
    "question": "What is the purpose of using customer-managed keys in Azure App Configuration?",
    "answer": "Customer-managed keys are used to encrypt sensitive information in key-value pairs at rest.  \nWhile a managed identity is used for authentication, it's not the primary purpose of customer-managed keys.  \nThe unwrapped encryption key is cached within App Configuration for one hour, not stored permanently.",
    "options": [
      "To enable authentication with Microsoft Entra ID",
      "To permanently store the unwrapped encryption key",
      "To encrypt sensitive information at rest"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "2d824cc86f074ef91474309da65ffad47f178ab951a4e83aae589c51dd9a86ea",
    "question": "What is the primary difference between Azure App Configuration and Azure Key Vault in terms of encryption?",
    "answer": "Azure Key Vault offers hardware-level encryption (available in Premium tier), while Azure App Configuration does not.",
    "options": [
      "App Configuration does not support encryption at rest.",
      "App Configuration does not support encryption at transit.",
      "App Configuration does not support hardware-level encryption."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "99278e27f72771938f4c6da3ddc2e788e197eb3b52d941680a3b66a276ec3714",
    "question": "Which of the following features is NOT provided by Azure App Configuration?",
    "answer": "Azure App Configuration does not offer granular access policies, unlike Key Vault.",
    "options": [
      "Real-time control of feature availability.",
      "Dynamic application settings adjustments without redeployment or restart.",
      "Granular access policies.",
      "Hierarchical configuration data management.",
      "Capability to import and export configuration information between Azure App Configuration and separate files."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "906f03c4c7612f1546268fb84fb7de0a54159dc46ec3894f707157be53730ec9",
    "question": "In Azure App Configuration, when loading configuration and using multiple `.Select()` calls, how does the order of these calls affect the values that are used if a key with the same name exists in both labels? Consider the following code snippet used to load configuration from Azure App Configuration:\n\n```csharp\nbuilder.Configuration.AddAzureAppConfiguration(options =>\n{\n    options.Connect(connectionString)\n           // Select a subset of the configuration keys\n           .Select(\"TestApp:*\", LabelFilter.Null)\n           .Select(\"TestApp:*\", \"dev\");\n});\n```",
    "answer": "Merge and override previous values",
    "options": [
      "Only the last `.Select()` is considered. Values with keys starting with \"TestApp:\" and the label \"dev\" will be used; all values with no label will be discarded.",
      "Only the first `.Select()` is considered. Values with keys starting with \"TestApp:\" and the label \"dev\" will not be loaded.",
      "Both labeled and unlabeled values will be merged; if the same key exists in both, only the value from the first `.Select()` (no label) will be considered, and \"dev\" will not override them.",
      "Both labeled and unlabeled values will be merged; if the same key exists in both, values from the last `Select()` (\"dev\" label) will override the existing (no label).",
      "- An exception will be thrown"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "ce347055eb4efd964ed96cd85e7cd078439852850883279a4695fd416837a4e5",
    "question": "In Azure App Configuration, how can you explicitly reference a key-value that does not have a label?",
    "answer": "To explicitly reference a key-value without a label, use `\\0` (URL encoded as `%00`). This acts as a placeholder to indicate that the key-value in question is unlabeled, allowing you to differentiate it from key-values that might have been assigned specific labels.",
    "options": [
      "Use the label \"unlabeled\"",
      "Use the label \"\\0\"",
      "Use the label \"%00\"",
      "Use the label \"null\"",
      "Leave the label field blank"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "adf5997c5ea9031750057b00717f1cd8d056b9f87bad1bef187cdadf793b38cf",
    "question": "You are setting up resources in Azure App Configuration and have the following entries:\n\n- Key = `AppName:Region1:DbEndpoint`\n- Key = `AppName:region1:dbendpoint`\n- Key = `AppName:Service1:ApiEndpoint`\n- Key = `AppName:Service1:ApiEndpoint` with Label = `\\0`\n- Key = `AppName:Service1:ApiEndpoint` with Label = `Test`\n\nWhat is the total count of distinct keys that will be saved in Azure App Configuration?",
    "answer": "\n\n- `AppName:Region1:DbEndpoint` and `AppName:region1:dbendpoint` are considered two unique keys because they differ in case.\n- `AppName:Service1:ApiEndpoint` appears three times with different label variations:\n  - No label (default label)\n  - Label `\\0` (acts as no label, but explicitly specified, and is also considered distinct)\n  - Label `Test`\n\nSince different labels create different versions of the same key, these are considered distinct entries.\n\nTherefore, the total number of unique keys stored in Azure App Configuration is: **5**.",
    "options": ["2", "3", "4", "5"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "49c3dce73cb8e14d503135ff08db0050af38918e4995d07aaa05b573eb6403fc",
    "question": "A financial services company is deploying a compliance-sensitive application to Azure. The app is hosted in a virtual network (VNet) and retrieves its runtime settings from Azure App Configuration.\nTo meet internal security requirements, the company mandates that all configuration traffic must stay entirely within the Azure backbone network and avoid exposure to the public internet.\nWhich solution meets this requirement?",
    "answer": "Private endpoint assigns a VNet IP to the App Configuration store, ensuring all traffic flows over the Azure backbone via a private link—never crossing the public internet.",
    "options": [
      "Use a system-assigned managed identity",
      "Use a user-assigned managed identity",
      "Configure a private endpoint for the App Configuration store",
      "Use a customer-managed key for encryption"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "9feeb84fb35801d8e4aa3275f9a81e71350ee77b0aae15a56fc3d4f621c45e66",
    "question": "You are configuring an Azure App Configuration standard tier to use a customer-managed key from Azure Key Vault. What are the essential actions you must perform to ensure secure key access and compliance? Choose all that apply.",
    "answer": "Virtual network connection is not required for customer-managed key integration. The focus is on permissions and identity, not network settings.  \nPublic network access is unrelated to key integration and could pose security risks. Key integration relies on identity and access control, not public accessibility.",
    "options": [
      "Enable purge protection on the Azure Key Vault.",
      "Connect the Azure App Configuration to a virtual network.",
      "Assign a managed identity to the Azure App Configuration instance.",
      "Grant the managed identity appropriate permissions on the Azure Key Vault.",
      "Enable public network access for the Azure App Configuration."
    ],
    "answerIndexes": [0, 2, 3],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "50425cd67facfe36ed2ba158ce06178c716f3783e2520944104a8bb97cd3f53f",
    "question": "Which Azure CLI command option would correctly configure key access permissions for a managed identity in your Key Vault?",
    "answer": "This command correctly sets the necessary permissions: `GET`, `WRAP`, and `UNWRAP`.",
    "options": [
      "`az keyvault set-policy --vault-name 'MyVault' --object-id 'userObjectId' --key-permissions get wrapKey unwrapKey`",
      "`az keyvault policy-update --vault 'MyVault' --object-id 'userObjectId' --permissions keys read write`",
      "`az keyvault set-policy --name 'MyVault' --identity-id 'userObjectId' --key-access get list`",
      "`az keyvault update-policy --vault 'MyVault' --object-id 'userObjectId' --permissions keys get list`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Configuration"
  },
  {
    "id": "01a9e29ae664b2979c3070147c1fa6f03ccefed28d3ee54fb093dad3322207e6",
    "question": "Which of the following networking features of App Service can be used to control outbound network traffic?",
    "answer": "Hybrid Connections are an outbound network feature.",
    "options": ["App-assigned address", "Hybrid Connections", "Service endpoints"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "ef3090e6e02d1bd6e1c0f82b5ce18fa3e7600de319cb6008b3133a79a051173a",
    "question": "Which of the following networking features of App Service are outbound?",
    "answer": "What's listed.",
    "options": [
      "App-assigned address",
      "Hybrid Connections",
      "Service endpoints",
      "Virtual network integration",
      "Gateway-required virtual network integration",
      "Access restrictions",
      "Private endpoints"
    ],
    "answerIndexes": [1, 3, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "37bb1f3b28497b4872333df1eb3586b592a419eac7a6b1248e95783073969a9f",
    "question": "Which of the following networking features of App Service are inbound?",
    "answer": "What's listed.",
    "options": [
      "App-assigned address",
      "Hybrid Connections",
      "Service endpoints",
      "Virtual network integration",
      "Gateway-required virtual network integration",
      "Access restrictions",
      "Private endpoints"
    ],
    "answerIndexes": [0, 2, 5, 6],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "59802688c67e07141c635746c096da5a78aaf247dfc25f4385916c52c5bfe687",
    "question": "When would you use the App-assigned address feature?",
    "answer": "To support IP-based SSL for your app, To support a dedicated inbound address for your app.",
    "options": [
      "To access an on-premises database server.",
      "To support IP-based SSL for your app.",
      "To access resources in your Azure virtual network.",
      "To restrict access to your app from a set of well-defined IP addresses.",
      "To support a dedicated inbound address for your app."
    ],
    "answerIndexes": [1, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "7c8fe7f6ead2119803b9f456169823c3a7aacea2db04bbb9eae62340afa51c4b",
    "question": "When would you use Access restrictions feature?",
    "answer": "To restrict access to your app from a set of well-defined IP addresses.",
    "options": [
      "To restrict access to your app from a set of well-defined IP addresses.",
      "To access an on-premises system securely.",
      "To access resources in your Azure virtual network.",
      "To support IP-based SSL for your app.",
      "To restrict access to your Azure Service Resources to only your virtual network."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "c0c70823d927e1119fbef4f66370eb7487d942338f58fbafa81dd8f29da6bba1",
    "question": "When would you use Service endpoints/Private endpoints feature?",
    "answer": "To restrict access to your Azure Service Resources to only your virtual network.",
    "options": [
      "To restrict access to your Azure Service Resources to only your virtual network.",
      "To support IP-based SSL for your app.",
      "To access an on-premises system securely.",
      "To access resources in your Azure virtual network or on-premises network over ExpressRoute or site-to-site VPN.",
      "To restrict access to your app from a set of well-defined IP addresses."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "340be0cecb3ad6dd796baa8131cc2d34b429adc0c93f4fd73c1bdd73ce13ee70",
    "question": "When would you use Private endpoints features?",
    "answer": "To restrict access to your Azure Service Resources to only your virtual network; to expose your app on a private IP in your virtual network.",
    "options": [
      "To restrict access to your Azure Service Resources to only your virtual network.",
      "To expose your app on a private IP in your virtual network",
      "To support IP-based SSL for your app.",
      "To access an on-premises system securely.",
      "To access resources in your Azure virtual network or on-premises network over ExpressRoute or site-to-site VPN.",
      "To restrict access to your app from a set of well-defined IP addresses."
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "6b347e154919a3e43f5728d64bef4e9b3942d119a13c46bca786e5b5e33266da",
    "question": "When would you use Hybrid Connections feature?",
    "answer": "To access an on-premises system or service securely.",
    "options": [
      "To access an on-premises system or service securely.",
      "To restrict access to your app from a set of well-defined IP addresses.",
      "To support IP-based SSL for your app.",
      "To restrict access to your Azure Service Resources to only your virtual network.",
      "To access resources in your Azure virtual network."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "b88411cb36c65af9c9e63170d15c8414380641a63f75bc6eb571aeb976ed282c",
    "question": "When would you use Gateway-required virtual network integration feature?",
    "answer": "To access resources in your Azure virtual network or on-premises network over ExpressRoute or site-to-site VPN.",
    "options": [
      "To access resources in your Azure virtual network or on-premises network over ExpressRoute or site-to-site VPN.",
      "To restrict access to your Azure Service Resources to only your virtual network.",
      "To support IP-based SSL for your app.",
      "To access an on-premises system securely.",
      "To support a dedicated inbound address for your app."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "7dc7b5ebbe8e2e6628f258794804f9958dc9b7e46c62f9bec86a520845edf3f9",
    "question": "When would you use Virtual network integration feature?",
    "answer": "To access resources in your Azure virtual network.",
    "options": [
      "To access resources in your Azure virtual network.",
      "To restrict access to your app from a set of well-defined IP addresses.",
      "To access an on-premises system securely.",
      "To support IP-based SSL for your app.",
      "To restrict access to your Azure Service Resources to only your virtual network."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "00e4a8c5641c458c38f9b79f1a5d8a1d17d8bb55188cfe7279d13af68a2f7cfb",
    "question": "In which of the following app configuration settings categories would you set the language and SDK version?",
    "answer": "General Settings is used to configure stack, platform, debugging, and incoming client certificate settings.",
    "options": ["Application settings", "Path mappings", "General settings"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "9e72011f2c320ad24d3474d15b6d0a29eae0a3cd6b05b4ac3fef65d088c2512b",
    "question": "By default, all client requests to the app's production URL (`http://<app_name>.azurewebsites.net`) are routed to the production slot. One can automatically route a portion of the traffic to another slot. What is the default routing rule applied to new deployment slots?",
    "answer": "By default, new slots are given a routing rule of `0%`",
    "options": ["0%", "10%", "20%", "50%"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "b3fec3db99a77f9d945496fa6df797c2a3f958f92d018d49ae04f18600cb08b4",
    "question": "Some configuration elements follow the content across a swap (not slot specific), whereas other configuration elements stay in the same slot after a swap (slot specific). Which of the following settings are swapped?",
    "answer": "WebJobs content is swapped.",
    "options": ["Publishing endpoints", "WebJobs content", "WebJobs schedulers"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "b0b9d0b10e4136a8962e2e95e63d4630ffbf006e159ec61ed98d2b3979c46cc4",
    "question": "You are a developer tasked with deploying a new .NET web application using on Microsoft Azure. Your first task is to create a new resource group `myResourceGroup` located in the South Central US region. Following that, you need to establish a deployment user for the web application. You are then required to create `myAppServicePlan` App Service plan within a Linux environment that is cost-efficient. The web application should be created within this resource group and App Service plan, using .NET as its runtime. You are also required to configure the application settings to set the deployment branch to `main`. Lastly, you are provided with a sample application from GitHub: `https://github.com/Azure-Samples/App-Service-Troubleshoot-Azure-Monitor`. You need to clone this application, rename the default branch to `main`, add the Azure remote repository using the URL from the webapp create command, and push the code to the Azure repository. How would you accomplish these tasks using Azure CLI commands?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Create a new resource group in the South Central US region\naz group create --name myResourceGroup --location \"South Central US\"\n\n# Set up a deployment user for the web application\naz webapp deployment user set --user-name <username> --password <password>\n\n# Create an App Service plan with a Linux environment. The SKU B1 is chosen for its cost efficiency.\naz appservice plan create --name myAppServicePlan --resource-group myResourceGroup --sku B1 --is-linux\n\n# Create the web application within the resource group and the App Service plan, using .NET as its runtime\naz webapp create --resource-group myResourceGroup --plan myAppServicePlan --name MyApp --runtime \"DOTNET|6.0\" --deployment-local-git\n\n# Capture the Git URL from the output of the previous command\ngit_url=$(az webapp show --name MyApp --resource-group myResourceGroup --query gitUrl --output tsv)\n\n# Configure the application settings to set the deployment branch to 'main'\naz webapp config appsettings set --name MyApp --resource-group myResourceGroup --settings DEPLOYMENT_BRANCH='main'\n\n# Clone the sample application from GitHub\ngit clone https://github.com/Azure-Samples/App-Service-Troubleshoot-Azure-Monitor\n\n# Navigate into the cloned repository\ncd App-Service-Troubleshoot-Azure-Monitor\n\n# Rename the default branch to 'main'\ngit branch -m main\n\n# Add the Azure remote repository using the URL from the webapp create command\ngit remote add azure $git_url\n\n# Push the code to the Azure repository\ngit push azure main\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "b6bebc470b94c94b74956ac25bfa5cf885a867b6afb94e82e3bc34a57b8006a3",
    "question": "You're an Azure developer with the task of deploying a PHP web app from a GitHub repo to Azure App Services. As a part of your workflow, you want to test the new features of your app in a safe, isolated environment before making it live.\n\nYour task is to script the process to deploy the app to an Azure App Service, using a staging deployment slot to validate updates before they go live. This involves creating all necessary resources in Azure, such as a resource group and an App Service plan. After verifying the new version of the app works as expected, swaps the staging slot into production. It should be optimized for cost efficiency.\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nlocation=\"East US\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\ntag=\"deploy-deployment-slot.sh\"\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\ngitrepo=https://github.com/Azure-Samples/php-docs-hello-world\n\n# Code here\n```",
    "answer": "\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nlocation=\"East US\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\ntag=\"deploy-deployment-slot.sh\"\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\ngitrepo=https://github.com/Azure-Samples/php-docs-hello-world\n\n# Create a resource group.\naz group create --name $resourceGroup --location \"$location\" --tag $tag\n\n# Create an App Service plan in STANDARD tier (minimum required by deployment slots).\necho \"Creating $appServicePlan\"\naz appservice plan create --name $appServicePlan --resource-group $resourceGroup --location \"$location\" \\\n--sku S1\n\n# Create a web app\naz webapp create --name $webapp --plan $appServicePlan --resource-group $resourceGroup\n\n# Create a deployment slot with the name \"staging\".\naz webapp deployment slot create --name $webapp --resource-group $resourceGroup --slot staging\n\n# Deploy sample code to \"staging\" slot from GitHub.\naz webapp deployment source config --name $webapp --resource-group $resourceGroup --slot staging --repo-url $gitrepo --branch master --manual-integration\n\n# Swap the verified/warmed up staging slot into production.\naz webapp deployment slot swap --name $webapp --resource-group $resourceGroup \\\n    --slot staging\n```\n\nNote: If there was no deployment slots requirenments, `az webapp deployment slot create` and `az webapp deployment slot swap` (and `--slot staging` in `az webapp deployment source config`) could be dropped, and app service plan can go as low as FREE.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "865fdcc722e3e48dc3e0acaf25507be239c527940cef0cc8d06b7aba4fb952dc",
    "question": "You're an Azure developer working on a hobby project, deploying a PHP web app from a GitHub repo to Azure App Services. As part of your hands-on learning, you aim to automate the entire deployment process using a script.\n\nYour mission is to script the deployment of the app to an Azure App Service. This process involves spinning up all the necessary resources in Azure, such as a resource group and an App Service plan. The goal is to have your application live and running on the Azure App Service at the end of the script execution, avoiding unnecessary costs.\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nlocation=\"East US\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\ntag=\"deploy-github.sh\"\ngitrepo=https://github.com/Azure-Samples/php-docs-hello-world\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\n\n# Code here\n```",
    "answer": "\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nlocation=\"East US\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\ntag=\"deploy-github.sh\"\ngitrepo=https://github.com/Azure-Samples/php-docs-hello-world\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\n\n# Create a resource group.\naz group create --name $resourceGroup --location \"$location\" --tag $tag\n\n# Create an App Service plan in `FREE` tier.\naz appservice plan create --name $appServicePlan --resource-group $resourceGroup --sku FREE\n\n# Create a web app.\naz webapp create --name $webapp --resource-group $resourceGroup --plan $appServicePlan\n\n# Deploy code from a public GitHub repository.\naz webapp deployment source config --name $webapp --resource-group $resourceGroup \\\n    --repo-url $gitrepo --branch master --manual-integration\n```\n\nNote: If there is deployment slots requirenment, then `az webapp deployment slot create` and `az webapp deployment slot swap` (and `--slot staging` in `az webapp deployment source config`) should be added. Also the app service plan should be bumped to STANDARD.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "1c6300b320d60ff48ba5a18e20016d8cf4455b501869c899fb1851331cec9a57",
    "question": "You're building an ASP.NET Core Linux application utilizing Azure App Service Web App for Containers. You aim to deploy the application in a region in which your company's subscription does not currently have a resource group in there. What are the necessary Azure command-line interface (CLI) commands to deploy the application in that appropriate region?\n\n```ps\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\nappServicePlan=\"WEuropePlan\"\nappName=\"AspApp\"\ncontainerImage=\"mcr.microsoft.com/dotnet/samples:aspnetapp\"\n\n# Code here\n```",
    "answer": "\n\n```ps\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\nappServicePlan=\"WEuropePlan\"\nappName=\"AspApp\"\ncontainerImage=\"mcr.microsoft.com/dotnet/samples:aspnetapp\"\n\n# Create a new resource group in the West Europe region\naz group create --name $resourceGroup --location \"$location\"\n\n# Create an App Service plan in the West Europe region\naz appservice plan create --name $appServicePlan --resource-group $resourceGroup --location \"$location\" --is-linux\n\n# Create a web app in the new App Service Plan\naz webapp create --name $appName --resource-group $resourceGroup --plan $appServicePlan --deployment-container-image-name $containerImage\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "c47c3d51648512497295301163ef05a327e5f70a040c503e0d562c55e7ae1dcc",
    "question": "Imagine you are tasked with setting up a WordPress application for your company's blog. The application should run in a Docker multi-container environment, with the configuration provided in the `docker-compose-wordpress.yml` file. The WordPress application should connect to a new MySQL database, both to be hosted on Microsoft Azure. After the initial setup, you need to accommodate changes in your WordPress configuration, which will be done by modifying the Docker compose file and applying the changes. How would you handle this?\n\n```ps\n\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\nappServicePlan=\"WEuropePlan\"\nappName=\"WpApp\"\nwordpressDbServer=\"wp-database\"\nwordpressDb=\"wordpress\"\nwordpressAdmin=\"adminuser\"\nwordpressPassword=\"My5up3rStr0ngPaSw0rd!\"\n\n# Code here\n```",
    "answer": "\n\n```ps\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\nappServicePlan=\"WEuropePlan\"\nappName=\"WpApp\"\nwordpressDbServer=\"wp-database\"\nwordpressDb=\"wordpress\"\nwordpressAdmin=\"adminuser\"\nwordpressPassword=\"My5up3rStr0ngPaSw0rd!\"\n\n# Create a new resource group\naz group create --name $resourceGroup --location \"$location\"\n\n# Create a new App Service plan\naz appservice plan create --name $appServicePlan --resource-group $resourceGroup --location \"$location\" --is-linux\n\n# Create a new MySQL server\naz mysql server create --resource-group $resourceGroup --name $wordpressDbServer --location \"$location\" --admin-user $wordpressAdmin --admin-password wordpressPassword\naz mysql db create --resource-group $resourceGroup --server-name $wordpressDbServer --name $wordpressDb\n\n# Create a web app\naz webapp create --resource-group $resourceGroup --plan $appServicePlan --name $appName --multicontainer-config-type compose --multicontainer-config-file docker-compose-wordpress.yml\n# Set environment variables\naz webapp config appsettings set --resource-group $resourceGroup --name $appName --settings WORDPRESS_DB_HOST=\"$wordpressDbServer.mysql.database.azure.com\" WORDPRESS_DB_USER=\"$wordpressAdmin\" WORDPRESS_DB_PASSWORD=\"wordpressPassword\" WORDPRESS_DB_NAME=\"$wordpressDb\"\n\n# Update app after editing docker-compose-wordpress.yml\naz webapp config container set --resource-group $resourceGroup --name $appName --multicontainer-config-type compose --multicontainer-config-file docker-compose-wordpress.yml\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "8bd985fe0e1cc5019e736eb940d74f461c02a45731217106c1612f982a836707",
    "question": "You're responsible for migrating your company's online store application to a Kubernetes environment hosted on Azure (AKS). This application has a set of microservices, each defined in `serviceA.yaml`, `serviceB.yaml`, and `serviceC.yaml` YAML files. You need to ensure the successful deployment and management of these microservices. Also, let's say you are required to monitor the performance and health of your application, for which you decide to integrate Azure Monitor for containers. How would you accomplish this task?\n\n```ps\n# Variables\nresourceGroup=\"MyResourceGroup\"\nAKSClusterName=\"MyAKSCluster\"\nlocation=\"West Europe\"\nmonitorWorkspace=\"MyMonitorWorkspace\"\n```",
    "answer": "\n\n```ps\n# Variables\nresourceGroup=\"MyResourceGroup\"\nAKSClusterName=\"MyAKSCluster\"\nlocation=\"West Europe\"\nmonitorWorkspace=\"MyMonitorWorkspace\"\n\n# Create a new resource group in the specified location\naz group create --name $resourceGroup --location \"$location\"\n\n# Create a new AKS cluster in the resource group\naz aks create --resource-group $resourceGroup --name $AKSClusterName --generate-ssh-keys\n\n# Get the credentials for the AKS cluster\naz aks get-credentials --resource-group $resourceGroup --name $AKSClusterName\n\n# Deploy the microservices to the AKS cluster using their respective YAML files\nkubectl apply -f serviceA.yaml\nkubectl apply -f serviceB.yaml\nkubectl apply -f serviceC.yaml\n\n# Create Log Analytics workspace\naz monitor log-analytics workspace create --resource-group $resourceGroup --workspace-name $monitorWorkspace --location $location\n\n# Enable Azure Monitor for containers\naz aks enable-addons --resource-group $resourceGroup --name $AKSClusterName --addons monitoring --workspace-resource-id $monitorWorkspace\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "b067e2caf6fc1c6b9e6a36d19ab7e52f0dd27cf2c1fae08824526093c2198e1b",
    "question": "Imagine you are tasked with deploying a web application on Azure App Service. As part of the task, you are also expected to set up Application Insights for monitoring the performance and usage data of the application, located in resource group `$resourceGroup` and app service plan `$appServicePlan` in location `$location`. All of these activities should be performed using the Azure CLI. How would you go about accomplishing this?\n\n```ps\nappName=\"MyWebApp\"\nappInsightsName=\"MyAppInsights\"\n```",
    "answer": "\n\n```ps\nappName=\"MyWebApp\"\nappInsightsName=\"MyAppInsights\"\n\n# Create a web app\naz webapp create --name $appName --resource-group $resourceGroup --plan $appServicePlan\n\n# Create a new Application Insights resource in the resource group\naz monitor app-insights component create --app $appInsightsName --location \"$location\" --resource-group $resourceGroup\n\n# Enable Application Insights for the web app\naz webapp config appsettings set --name $appName --resource-group $resourceGroup --settings APPINSIGHTS_INSTRUMENTATIONKEY=\"$(az monitor app-insights component show --app $appInsightsName --resource-group $resourceGroup --query instrumentationKey --output tsv)\"\n```\n\nIn the last command, we are using the Azure CLI to get the Instrumentation Key of the newly created Application Insights resource. The `--query` parameter allows us to specify the data to extract, and `--output tsv` is used to format the output as Tab-separated values, which gives us a clean output to use in setting the `APPINSIGHTS_INSTRUMENTATIONKEY` setting.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "e88d02ed6456813cb86ad6630d6c884ef2b0a1f866f744ed62c15afd4341ee2a",
    "question": "As an Azure Cloud Engineer, your company needs you to streamline the deployment process of their web applications. To help achieve this, they've asked you to automate the process using a bash script.\n\nHere's your mission: the script needs to set up an environment in Azure that pulls a Docker container named `companywebapp:latest` from your company's private registry and deploys it to an Azure App Service in the East US region.\n\nThe resources that need to be created include a resource group (tagged with `deploy-linux-docker-app-only.sh`), an app service plan, and a web app. Can you write a script that fulfills these requirements?\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\nregistryUrl=\"https://yourCompanyRegistry.azurecr.io\"\n\n# Code here\n```",
    "answer": "\n\n```ps\nlet \"randomIdentifier=$RANDOM*$RANDOM\"\nresourceGroup=\"msdocs-app-service-rg-$randomIdentifier\"\nappServicePlan=\"msdocs-app-service-plan-$randomIdentifier\"\nwebapp=\"msdocs-web-app-$randomIdentifier\"\nregistryUrl=\"https://yourCompanyRegistry.azurecr.io\"\n\nlocation=\"East US\"\ntag=\"deploy-linux-docker-app-only.sh\"\ndockerHubContainerPath=\"simplewebapp:latest\"\n\n# Create a resource group.\naz group create --name $resourceGroup --location \"$location\" --tag $tag\n\n# Create an App Service plan\naz appservice plan create --name $appServicePlan --resource-group $resourceGroup\n\n# Create a web app.\naz webapp create --name $webapp --resource-group $resourceGroup --plan $appServicePlan  --runtime \"NODE|14-lts\"\n\n# Configure the web app to use the Docker container image from your company's custom registry\naz webapp config container set --name $webapp --resource-group $resourceGroup --docker-custom-image-name $dockerHubContainerPath --docker-registry-server-url $registryUrl\n```\n\nNote: `--docker-registry-server-url` is not needed if the image is on DockerHub",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "baad1f41d0f54eb7839d71e1ebfc4f48788812fe526f1e92e82af708da69dfac",
    "question": "How to access client certificate for Asp.Net app?",
    "answer": "For ASP.NET, the client certificate is available through the `HttpRequest.ClientCertificate` property.  \nFor other application stacks (Node.js, PHP, etc.), the client cert is available in your app through a base64 encoded value in the `X-ARR-ClientCert` request header.",
    "options": [
      "`HttpRequest.ClientCertificate` property",
      "Through the HTTPS request header",
      "Through a URL query string",
      "Through the client cookie"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "d3ec26ee3cc86a9533174e74600a621ebf418b96df6099315a9e6998dc90134b",
    "question": "How to access client certificate for Node.js app?",
    "answer": "For Node.js, the client cert is available in your app through a base64 encoded value in the `X-ARR-ClientCert` request header.\nFor ASP.NET, the client certificate is available through the `HttpRequest.ClientCertificate` property.",
    "options": [
      "`HttpRequest.ClientCertificate` property",
      "Through the HTTPS request header",
      "Through a URL query string",
      "Through the client cookie"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "ad14ff049ebd97c52205b526ba4519fa97d6a4a3521fd816c8ff02b0f1e98f67",
    "question": "How to enable client certificates through interface?",
    "answer": "`Configuration > General Settings`. Set `Client certificate mode` to **Require**",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "eac5ced8debd5314810ef11c817ceaef27ded0ae0816981fe609ef9f78e55950",
    "question": "How to require client certificates for a webapp?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz webapp update --set clientCertEnabled=true --name <app-name> --resource-group <group-name>\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "4017875d803d200fe0d8b16e80e486c9868546eb8716402bd316b561a7eec702",
    "question": "As an Azure Cloud Engineer, you received a complaint from a client who is running tests on your web application from `http://localhost:5000`. They reported an error stating: `Access to XMLHttpRequest at 'http://myWebApp.azurewebsites.net' from origin 'http://localhost:5000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.` Fix that!\n\nThe web application resides in a resource group named `myResourceGroup`.\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz webapp cors add --resource-group myResourceGroup --name myWebApp --allowed-origins 'http://localhost:5000'\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "e45311518ffb93922739ac0ca30154c37d226223f13abb3b3d4ab0601c0f78de",
    "question": "As an Azure Cloud Engineer, you received an email from a client who's trying to access data in one of your Blob Storage accounts from their local machine at `http://localhost:5000`. They've been encountering an error message that says: `Access to XMLHttpRequest at 'https://myStorageAccount.blob.core.windows.net' from origin 'http://localhost:5000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.`. Fix that!\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz storage cors add --services blob --methods GET POST --origins 'http://localhost:5000' --allowed-headers '*' --exposed-headers '*' --max-age 200 --account-name myStorageAccount\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "f5c43a127f50804e88f3f5a9f806c55205a114e7e966d75c13bef32d4d221762",
    "question": "Several users have reported receiving HTTP 500 errors when attempting to connect to the web app you have developed using Azure App Service. To allow developers to see the connection error details in real-time, what action is required on your part?",
    "answer": "You must enable Web Server Logging in Azure App Service when handling raw HTTP requests data. The logs contain details like HTTP method, resource URI, client IP and port, user agent, response code, etc. You can save these logs in storage or a file system, and specify the number of days for retaining them.",
    "options": [
      "Enable the Web Server Logging feature.",
      "Create a security playbook.",
      "Enable the Application Logging feature.",
      "Create resource health alerts."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "7c6a1d13e6752805d90a56940ee67924a1a83ec11615f4ef8b88a473d07c253b",
    "question": "Which of the following types of application logging is supported on the Linux platform?",
    "answer": "Deployment and Application logging are supported on the Linux platform.",
    "options": [
      "Web server logging",
      "Failed request tracing",
      "Detailed Error Messages",
      "Deployment logging",
      "Application logging"
    ],
    "answerIndexes": [3, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "d5640f47ddcff1707c4c3eb8652c4e4becc286d82526035ef5b7ded633dc7db9",
    "question": "Which of the following types of application logging supporteds blob storage?",
    "answer": "Web server and Application logging are support blob storage.",
    "options": [
      "Web server logging",
      "Failed request tracing",
      "Detailed Error Messages",
      "Deployment logging",
      "Application logging"
    ],
    "answerIndexes": [0, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "6e2cb4cb8ef1592624729ad8a0c517eb9fefc5be5f5e61822974e63d7a1d47b7",
    "question": "Imagine you are a developer who needs to set up monitoring for a .NET web application named `myApp` deployed on Microsoft Azure. Your task is to leverage Azure Monitor by creating a Log Analytics workspace within an existing resource group named `myResourceGroup`. After creating the workspace, you need to retrieve the resource ID of the web application and the workspace ID of the newly created Log Analytics workspace. With these IDs, you are then tasked with creating diagnostic settings for the web application to enable Azure Monitor to collect console logs and HTTP logs. How would you perform these tasks using Azure CLI commands?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Create a Log Analytics workspace within the existing resource group named 'myResourceGroup'\naz monitor log-analytics workspace create --resource-group myResourceGroup --workspace-name myMonitorWorkspace\n\n# Retrieve the resource ID of the web application named 'myApp'\nresourceID=$(az webapp show -g myResourceGroup -n myApp --query id --output tsv)\n\n# Retrieve the workspace ID of the newly created Log Analytics workspace\nworkspaceID=$(az monitor log-analytics workspace show -g myResourceGroup --workspace-name myMonitorWorkspace --query id --output tsv)\n\n# Create diagnostic settings for the web application to enable Azure Monitor to collect console logs and HTTP logs\naz monitor diagnostic-settings create --resource $resourceID \\\n --workspace $workspaceID \\\n -n myMonitorLogs \\\n --logs '[{\"category\": \"AppServiceConsoleLogs\", \"enabled\": true},\n  {\"category\": \"AppServiceHTTPLogs\", \"enabled\": true}]'\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "22904b0a7770682a99b3a937ff94cbadfb16d7c378bb8655b919f4893d9e944e",
    "question": "How to stream HTTP logs for `myApp` in `myResourceGroup` resource group, using Cloud Shell?\n\n```ps\n# Code here\n```",
    "answer": "\n\nTo filter specific log types, such as HTTP, use the `--provider` parameter\n\n```ps\naz webapp log tail --name myApp --resource-group myResourceGroup --provider http\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "e74976fdb617ebb67214beb679e83f402d46d6d9d6e0efcc7373682ab3f0d3ff",
    "question": "How to stream errors from `myApp` in `myResourceGroup` resource group, using Cloud Shell?\n\n```ps\n# Code here\n```",
    "answer": "\n\nTo filter logs by errror, such as HTTP, use the `--filter Error` parameter\n\n```ps\naz webapp log tail --name myApp --resource-group myResourceGroup --filter Error\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "ab60020326c84cde8e2a182d5e2df1d76c7b5b64240fcbff1ef489fac806c83f",
    "question": "Which of the following App Service plan categories provides the maximum scale-out capabilities?",
    "answer": "The Isolated category provides network and compute isolation, and has the maximum scale-out capability.",
    "options": ["Dedicated compute", "Isolated", "Shared compute"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "a2a8951c21b3a31fe5252a99def8707ba779a946a2d14ef63cb941b36cc4ac43",
    "question": "Which of these statements best describes autoscaling?",
    "answer": "The system can scale out when specified resource metrics indicate increasing usage, and scale in when these metrics drop.",
    "options": [
      "Autoscaling requires an administrator to actively monitor the workload on a system.",
      "Autoscaling is a scale out/scale in solution.",
      "Scaling up/scale down provides better availability than autoscaling."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "de808af6572fd3264d44cb981af99ce5222c42d3a721119d2ac568ec47f40626",
    "question": "Which of these scenarios is a suitable candidate for autoscaling?",
    "answer": "Changes in application load that are predictable are good candidates for autoscaling.  \nOrganization running a promotion: Manual scaling is a better option here since this is a one-off event with a known duration.  \nSudden influx of requests: The increasing burst of activity could be caused by a Denial of Service attack that is attempting to overwhelm your system. Autoscaling wouldn't solve the problem.",
    "options": [
      "The number of users requiring access to an application varies according to a regular schedule. For example, more users use the system on a Friday than other days of the week.",
      "The system is subject to a sudden influx of requests that grinds your system to a halt.",
      "Your organization is running a promotion and expects to see increased traffic to their web site for the next couple of weeks."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "a0f6afe48090564696cb57390abb2138f418f0d9e24edac711a69d3ffbcf5cc1",
    "question": "Create an App Service web app `MyAppService` and its prerequisites. The service will be located in West US and must scale if usage is increased. Should be outimized for cost.\n\n```ps\n# Code here\n```",
    "answer": "`Standard`, `Premium`, `PremiumV2`, and `PremiumV3` all support autoscale, but `Standard` is most cost effective in this scenario.\n\n```ps\naz group create --name MyResourceGroup --location \"West US\"\n\naz appservice plan create --name MyAppServicePlan --resource-group MyResourceGroup --sku S1\n\naz webapp create --name MyAppService --resource-group MyResourceGroup --plan MyAppServicePlan\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "df27ce1767cc18688274e3c9abf28bce05ce87467cd05b8291059d13589a44b1",
    "question": "Create an App Service web app `MyAppService` and its prerequisites. The service will be located in West US and would scale occasionally when some promotion is running. Should be outimized for cost.\n\n```ps\n# Code here\n```",
    "answer": "All plans support manual scaling, but `Basic` is most cost effective in this scenario.\n\n```ps\naz group create --name MyResourceGroup --location \"West US\"\n\naz appservice plan create --name MyAppServicePlan --resource-group MyResourceGroup --sku B1\n\naz webapp create --name MyAppService --resource-group MyResourceGroup --plan MyAppServicePlan\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "5cceae7e7c0a9e32f513cd95b63797f880ebe57747cd8abc57fbaffb786c210f",
    "question": "Which of the following properties of an App Service plan can be set via Azure CLI:",
    "answer": "[You can't change the maximum scale limit in Azure CLI](https://learn.microsoft.com/en-us/azure/app-service/manage-automatic-scaling?tabs=azure-cli#set-maximum-number-of-web-app-instances), you must instead use the Azure portal.\n\n```ps\naz webapp update --minimum-elastic-instance-count X --prewarmed-instance-count Y\n```",
    "options": [
      "Set minimum number of web app instances",
      "Set maximum number of web app instances",
      "Number of preWarmed instances."
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "b1020b8c47cada705d18e6171e6d5a546bc0d75d145b237dbe71eae5ebaa71fa",
    "question": "Your company has developed a web application that experiences intermittent high traffic volumes. However, the company has noticed that during the sudden traffic spikes, the performance of their application temporarily degrades before it starts to stabilize. The degradation is especially apparent when the application hasn't received any significant traffic for a while. How can you mitigate this performance degradation issue?",
    "answer": "The problem your company experiences is called _cold boot_. In order to mitigate it, you need Automatic Scaling with pre-warmed/always ready instances. This requires at least `PremiumV2` plan.  \nAzure Manual Scaling lacks real-time adjustment capabilities, making it inefficient for sudden traffic spikes.  \nAlthough Azure Autoscale scales based on traffic, it can still experience \"cold starts\" during sudden traffic spikes.  \nAzure Application Insights Live Metrics: This is a monitoring tool, not a scaling solution.",
    "options": [
      "Manually adjust the number of instances when traffic increases.",
      "Set up custom scaling rules based on traffic using Azure Autoscale.",
      "Implement Azure Automatic Scaling with prewarming of instances.",
      "Monitor performance using Azure Application Insights Live Metrics.",
      "Bonus question: You are running on a `Standard` plan, do you need to chenge it in order to implement your solution?"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "20843834b321926e81323274aab429f4298c1ed7556f550cb43a248351964f3e",
    "question": "Which of the following App Service plans supports pre-warmed instances:",
    "answer": "Pre-warmed instances is a feature of Automatic scaling, which is supported only on `PremiumV2` and `PremiumV3` plans.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "28746fd4ef0da335abb2c84166678d27d666ecd41e7a03a2e21b570e063a2158",
    "question": "Which of the following App Service plans you can have always ready instances?",
    "answer": "\n\n- `PremiumV2` and `PremiumV3`: yes (default 1)\n- `Standard` and `Premium`: No, your web app runs on other instances available during the scale out operation, based on threshold defined for autoscale rules.\n- `Basic`: No, your web app runs on the number of manually scaled instances.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "PremiumV2", "Isolated"],
    "answerIndexes": [5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "ddbc0a4f8d20b844b5443eae152b163f4e27150b2fd0372b36b0c39f367d3530",
    "question": "Which of the following App Service plans supports schedule based scaling:",
    "answer": "Only `Standard` to `Premium` support schedule based scaling.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [3, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "c182d934d1f0b4592dae57f71db446935d1d2604a450276cf6d2574c7c909aae",
    "question": "Which of the following App Service plans does not supports scaling out:",
    "answer": "`Free` and `Shared` tiers run apps on a common Azure VM, shared with other users' apps.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "995ac4559d990d2993ec379bcc854a1da2c6beeff612a492b4e1bc3d4b77fe94",
    "question": "How apps are charged in Shared App Service plan:",
    "answer": "CPU quota",
    "options": [
      "Each app is charged for CPU quota",
      "Each VM instance in the App Service plan is charged",
      "The number of isolated workers that run your apps"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "83cc4b837ce218735de8156b83286dbb7b3495cfea4831480e1d50e16be222b6",
    "question": "How apps are charged in Dedicated compute App Service plan:",
    "answer": "VM instances",
    "options": [
      "Each app is charged for CPU quota",
      "Each VM instance in the App Service plan is charged",
      "The number of isolated workers that run your apps"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "365bedbd3f8042ead280ac4728250d57c1e320256ca1717bdfafebd351de7b83",
    "question": "How apps are charged in Isolated App Service plan:",
    "answer": "The number of isolated workers that run your apps",
    "options": [
      "Each app is charged for CPU quota",
      "Each VM instance in the App Service plan is charged",
      "The number of isolated workers that run your apps"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "dbc4e9c4e7e4680fa6585ca446ff6079dbbb1cc83746bd0accffbb6894e18ba9",
    "question": "Which App Service plans support custom DNS name:",
    "answer": "You cannot have custom DNS name on the `Free` tier.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [1, 2, 3, 4, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "7547fe6318aa0d2fe555de8a40c1c51393e6d00ef8371840f06c0db2209672eb",
    "question": "You want a Linux App Service web app on an App Service plan that supports custom DNS name. Which App Service plans satisfy these requirenments?",
    "answer": "Although `Shared` supports custom DNS name, it does not support Linux. Lowest is `Basic`.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [2, 3, 4, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "29cad983c16db864bf52b53a28adcca64791d9d182f672e92e87452c62303b6d",
    "question": "Which App Service plans support custom TLS bindings:",
    "answer": "You cannot have custom TLS bindings on the `Free` and `Shared` tiers.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [2, 3, 4, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "8f640084768e7586852d351831790e6cfad1e024ad1c178a2062521181aec469",
    "question": "Which App Service plans support Always On:",
    "answer": "You cannot have Always On on the `Free` and `Shared` tiers.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [2, 3, 4, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "2ffc3aa4d8dc3355dba46035a5a312599c5e3871abe2855bfda0d48214f83103",
    "question": "A corporation has an Azure-based web application that triggers an email alert during specific events. Users have noticed that alerts for irregular activities are sometimes delayed by several minutes. What should be done to address this delay?",
    "answer": "The problem arises because the Azure Web App goes into an idle state when not in use, causing delays in sending anomaly detection emails. To prevent this, the Always On feature should be enabled. This ensures that the Web App is always running, thereby eliminating the delay.",
    "options": [
      "Configure the Azure Function to operate on an App Service plan.",
      "Disable the Always On feature.",
      "Switch the Azure Function to a consumption-based plan.",
      "Enable the Always On feature."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "3db1106b8c5e47bd93f921abe2a5d04628dbca0b9fa55d8e0ed2e4bbf386ed42",
    "question": "Which App Service plans support staging environments (deployment slots):",
    "answer": "You cannot have staging environments on the `Free`, `Shared`, and `Basic` tiers.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [3, 4, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "570e87ab54b8d95134d34ebbc752ce06c4378645ccd49c91afcd31a37597fede",
    "question": "You have existing App Service web app `MyAppService` in West US running on `Basic` plan. You want to add support for staging environments and move it to North Central US region region. What steps you need to take?\n\n```ps\n# Code here\n```",
    "answer": "You cannot move web app from one region to other. Also `Basic` plan does not support staging environments. The app needs to be cloned into a new region.\n\n```ps\n# Create new resource group with location 'North Central US'\nNew-AzResourceGroup -Name DestinationAzureResourceGroup -Location \"North Central US\"\n\n# Create new 'Standard' App Service plan for that group (and set staging environments leter)\nNew-AzAppServicePlan -Location \"North Central US\" -ResourceGroupName DestinationAzureResourceGroup -Name DestinationAppServicePlan -Tier Standard\n\n# Clone `MyAppService` into new web app and place it in the new App Service plan\n$srcapp = Get-AzWebApp -ResourceGroupName SourceAzureResourceGroup -Name MyAppService\n$destapp = New-AzWebApp -ResourceGroupName DestinationAzureResourceGroup -Name MyAppService2 -Location \"North Central US\" -AppServicePlan DestinationAppServicePlan -SourceWebApp $srcapp\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "9661a5bc6eb76f5d1ac2a9f4f35279bee4f88ad99b194c8358658515119a506e",
    "question": "Where the setting for cloning existing App Service web app is located?",
    "answer": "General Settings is used to configure stack, platform, debugging, and incoming client certificate settings.",
    "options": ["Application settings", "Development Tools", "General settings"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "83b775f9fdbb5f209a3fb9909282f7634a3e44611632a2ea17956cee5406ddab",
    "question": "You have `MyAppServicePlan` App Service plan that have no apps associated with it. Which of the following statements is true:",
    "answer": "App Service plans that have no apps associated with them still incur charges because they continue to reserve the configured VM instances.",
    "options": [
      "MyAppServicePlan still incurs charges even if unused",
      "MyAppServicePlan will not incur any charges if it is not used"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "c850795ae40766ddf8c12d034f67f529248a775a9bfcad08aebed5f1f65a31b2",
    "question": "You want an App Service web app which runs on dedicated Azure Virtual Networks. Which App Service plans satisfies this requirenment?",
    "answer": "The `Isolated` and `IsolatedV2` tiers run dedicated Azure VMs on dedicated Azure Virtual Networks.",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "a2d22db84555b61c51428ca23c85e15c600abe93cada0e0bc627eff632a7d4d2",
    "question": "When should you isolate your app into a new App Service plan?",
    "answer": "\n\n- The app is resource-intensive.\n- You want to scale the app independently from the other apps in the existing plan.\n- The app needs resource in a different geographical region.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "dde711802f16bbfee03223fbbacb75c74c432c4ecc8ba9899a9c91689d4dc4a1",
    "question": "Which of the following statements are true:",
    "answer": "App Service is set of VMs that run one or group of applications and their services together in the same VM. Scaling out simply adds another VM with the same applications and services.",
    "options": [
      "You can independently scale apps placed in the same App Service plan",
      "An app runs on all the VM instances configured in the App Service plan.",
      "If multiple apps are in the same App Service plan, they all share the same VM instances.",
      "All deployment slots also run on the same VM instances as the app in given App Service plan",
      "Diagnostic logs, backups, and WebJobs don't use CPU cycles and memory from VM instances on an App Service plan",
      "You can improve app's performance if you put it in a new, empty App Service plan"
    ],
    "answerIndexes": [1, 2, 3, 5],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "3959348f085e26a4d88e2dc23c6557ce819f4ff261bd1caf9ad3229a68f3f957",
    "question": "What App Service plan the following command will create:\n\n```powershell\naz appservice plan create --name $planName --resource-group $resourceGroupName --location $location\n```",
    "answer": "Default SKU is `B1` (Basic)",
    "options": ["Free", "Shared", "Basic", "Standard", "Premium", "Isolated"],
    "answerIndexes": [2],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "ac380946fa1d82680ac1bf6d0aa482632419f6fb72b076404ce7774a1fa0f04e",
    "question": "When you execute the following command `az webapp create --name MyWebApp --plan D1 --resource-group MyResourceGroup` you notice that page load times are longer during peak traffic hours. You aim to automate scaling when the CPU load surpasses 80 percent, ensuring minimal costs. Which az cli command should you execute first?",
    "answer": "The app is on Shared plan, Standard is the minimum for autoscale (`--sku S1`).",
    "options": [
      "`az monitor autoscale create --resource MyWebApp --condition \"Percentage CPU > 80 avg 5m`",
      "`az appservice plan update --name <YourPlanName> --resource-group MyResourceGroup --sku P1v2`",
      "`az appservice plan update --name <YourPlanName> --resource-group MyResourceGroup --sku S1`",
      "None of the listed. You need to switch to Consumption plan first."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "33b8b1064bfb7978b2838926cebe3a18b62e23727cec6c380933c5105a6b207e",
    "question": "When you set `WEBSITE_OVERRIDE_PRESERVE_DEFAULT_STICKY_SLOT_SETTINGS=false` for an app in Azure App Service, what happens to managed identities during the swap process?",
    "answer": "They are never swapped, regardless of the setting",
    "options": [
      "They are always swapped, regardless of this setting",
      "They are not swapped, because this setting needs to be true",
      "They are swapped because the setting is set to false",
      "They are never swapped",
      "They are swapped only in the production slot"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "24e6ded6d48800aa3e1ccb016d4fb1553725f4e985e572d16bf7ae1a0e7c054a",
    "question": "Where will this link `<webappname>.azurewebsites.net/?x-ms-routing-name=self` will take you if you set routing rules to `10%`?",
    "answer": "`?x-ms-routing-name=self` will send you to production deployment slot regardless of the routing rules.",
    "options": [
      "Same deployment slot",
      "To `self` deployment slot",
      "To staging deployment slot",
      "To production deployment slot",
      "Unable to determine, because default routing rules will switch you to random deployment slot"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "87d47a97e625fdffce69b92b12652d3e533c9ddb5684430ffc2bb635fd2ef91d",
    "question": "Where will this link `<webappname>.azurewebsites.net/` will take you if you haven't set any routing rules?",
    "answer": "Default routing rules give 0% chance for random slot, and by default you are directed to production slot.",
    "options": [
      "To staging deployment slot",
      "To production deployment slot",
      "Unable to determine, because default routing rules will switch you to random deployment slot"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "7d8232737c0a33f682c79fdb3e7cc956406686236f02a46a8e5353bdc5c33d4d",
    "question": "What is the effect on the `/home/LogFiles` directory when enabling application logging with the File System option?",
    "answer": "The `/home/LogFiles` directory always persists upon app restarts if application logging is enabled with the File System option, independently of the persistent storage being enabled or disabled.",
    "options": [
      "It will persist only if `WEBSITES_ENABLE_APP_SERVICE_STORAGE=true`.",
      "It will not persist across app restarts.",
      "It will always persist upon app restarts, regardless of the persistent storage setting.",
      "It will be overwritten by existing files on persistent storage."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "9d561e1c434246939c1aa979fb235e44ca543654c8638c96ca15ed8b32edce3b",
    "question": "What action is required if you initiate a storage failover when the storage account is mounted to the app?",
    "answer": "Storage failover requires app restart or remounting of Azure Storage.",
    "options": [
      "The app must be restarted or the storage must be remounted.",
      "`WEBSITES_ENABLE_APP_SERVICE_STORAGE` must be set to `false`.",
      "No action is required. The app will automatically reconnect to the storage mount."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "e22f93198017d493fc3ee37c23d711dc37f60ee376ffdad43ef344b7f291eee3",
    "question": "Finnish this command to enable automatic scaling with max burst of 5:\n\n```ps\n  az appservice plan update --name $appServicePlanName --resource-group $resourceGroup # Code here\n```",
    "answer": "\n\n```ps\n  az appservice plan update --name $appServicePlanName --resource-group $resourceGroup --elastic-scale true --max-elastic-worker-count 5\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "App Service"
  },
  {
    "id": "ec470745cfe2f6fe05ad0b8f1636e785bc6a6650e5f55b35184eaa0a9c598709",
    "question": "When does scale out occur?",
    "answer": "When any of the conditions is met",
    "options": [
      "When all of the conditions are met",
      "When at least one of the conditions is met",
      "Only scale in can occur"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "27f6fd11a9bec8403d515bc214dffb339d01793cd0fba8d1d4d5d82c21dec743",
    "question": "Which of the following certificates can be exported?",
    "answer": "Free Managed certificated - No wildcard certificates or private DNS, can't be exported.",
    "options": ["Free Managed Certificate", "App Service Certificate", "Certificates stored in Key Vault"],
    "answerIndexes": [1, 2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "c0ce7cac5e5f595285502ceefaee8a3a060d70339819aeebb1a374fd9fccdd5e",
    "question": "You have multiple web apps in Standard plan. Which of the following statements is true?",
    "answer": "If you have a Standard App Service plan, all the apps in that plan run on the same worker.",
    "options": [
      "All apps in theat plan use shared workers",
      "Each app has its own worker",
      "All apps in that plan run on the same worker"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "adeae91353c313753a5c4c754dc10ffe71e55c008d9fbf728e7a97b17d115b34",
    "question": "What are outbound addresses in the context of Azure App Service Web Apps?",
    "answer": "Outbound addresses in Azure App Service Web Apps are the IP addresses used by the application to make calls to external services. They are shared among all applications running on the same type of worker VM.",
    "options": [
      "IP addresses used to identify inbound traffic to the application.",
      "IP addresses used by the application to make calls to external services.",
      "A set of firewall rules to control inbound and outbound traffic.",
      "A set of IP addresses that application can make calls to",
      "A feature to map custom domain names to IP addresses."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "a6c58166e4ba443ac0a82b6b2f09cb07f5de858dafd62e8967511c7905271b58",
    "question": "For logs stored in the App Service file system of Windows apps, the easiest way is to download the ZIP file in the browser at:",
    "answer": "For Windows is in `/api/dump`, for Linux is `/api/logs/docker/zip`",
    "options": [
      "`https://<app-name>.scm.azurewebsites.net/api/logs/docker/zip`",
      "`https://<app-name>.scm.azurewebsites.net/api/dump`",
      "`https://<app-name>.scm.azurewebsites.net/api/logs`",
      "`https://<app-name>.scm.azurewebsites.net/logs`",
      "`https://<app-name>.scm.azurewebsites.net/api/home/LogFiles`",
      "`https://<app-name>.scm.azurewebsites.net/home/LogFiles`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "227f38223d6e4108a13c1184edaf70c5d4b5f1b7c90babcba9050388fcea77fb",
    "question": "In Azure App Service, the metrics such as CPU Percentage, Memory Percentage, Data In, and Data Out are used to monitor:",
    "answer": "These metrics are used across all instances of the plan, not for monitoring a single app within the plan.",
    "options": [
      "A single app within the plan.",
      "All instances of the plan, not a single app.",
      "Individual virtual machines within the app service plan.",
      "The performance of the underlying infrastructure supporting the app service plan."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "4642ba8c047d8227ed6e25bd3351faf573dafc4eab36f96edc2f35b01ab1f891",
    "question": "In Azure App Service running on Basic plan, the Health Check feature pings a specified path every minute. What actions are taken if an instance fails to respond with a valid status code?",
    "answer": "An instance is marked unhealthy and removed from the load balancer if it fails to respond with a valid status code after 10 requests. If it stays unhealthy for an hour, it's replaced (only for Basic+).",
    "options": [
      "It's marked unhealthy and immediately replaced.",
      "It's marked unhealthy and removed from the load balancer, and if it stays unhealthy for an hour, it's replaced.",
      "It's marked unhealthy, and has to be manually fixed.",
      "It's marked unhealthy, and the x-ms-auth-internal-token request header is checked against the WEBSITE_AUTH_ENCRYPTION_KEY environment variable to confirm status."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "b1dccaadcf22015592ca414977bd0238bff6edf95b6a22e671452047b052bb6e",
    "question": "When mounting Azure Storage as a local share in Azure App Service, what is something you should NOT do?",
    "answer": "Avoid using Azure Storage for local databases or apps dependent on file handles and locks, refrain from using it for read/write actions in Linux containers, and minimize frequent regeneration of your access key.",
    "options": [
      "Place app and storage in the same Azure region.",
      "Use Azure Storage for local databases or apps relying on file handles and locks.",
      "Use Azure Files for read/write operations in Linux containers.",
      "Use Azure Files for read/write operations in Windows apps.",
      "Use Azure Storage for read/write operations in Linux containers.",
      "Use Azure Storage for read/write operations in Windows apps.",
      "Regenerate your access key often."
    ],
    "answerIndexes": [1, 4, 6],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "ce77ae5e611e699b806491d771047fa699c3ab6972cd708e9daae14f8d3937bb",
    "question": "Which of the following scenarios is best suited for using a Free Managed Certificate in Azure App Service?",
    "answer": "Free Managed Certificates are auto-renewed but don't support wildcard certificates, can't be exported, and are not supported in ASE.",
    "options": [
      "Securing a custom domain with wildcard certificates",
      "Storing in Azure App Service Environment (ASE)",
      "Securing a public website with auto-renewal every 6 months",
      "Exporting the certificate for use in another environment"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "e5bab7de54c12b8530626847af94a44a107b2a621228bac1ec1b6324b45f1834",
    "question": "Which of the following is NOT a feature of the App Service Certificate in Azure?",
    "answer": "This requirement is related to uploading a private certificate, not the App Service Certificate.",
    "options": [
      "Automated certificate management",
      "Renewal and export options",
      "Requires a password-protected PFX file",
      "Managed by Azure as a private certificate"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "41edd6e7fc376d515603b34ec0d62a4fa5eff8d27e5e23e2fd2ad43f5eb0bee5",
    "question": "When uploading a private certificate to Azure App Service, what is a mandatory requirement?",
    "answer": "This is a specific requirement for uploading a private certificate.",
    "options": [
      "Can be used for accessing remote resources",
      "Supports private DNS in Free Managed Certificate",
      "Requires a password-protected PFX file encrypted with triple DES",
      "Auto-renewed every 6 months"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "fbf4d8afae8b2ad1552f811e8dbf8d9e1ab4d7fa617078397ee953741fcb8624",
    "question": "You want to secure a public domain in Azure App Service without needing wildcard certificates or private DNS, and you want the certificate to be auto-renewed. Which option should you choose?",
    "answer": "Azure App Service's Free Managed Certificate provides a free TLS/SSL certificate for custom domains. It is automatically renewed every 6 months by Azure, works with standard (non-wildcard) public domains, and does not require private DNS or Azure DNS configuration. The App Service Certificate (B) must be purchased through Azure and managed separately. Uploading a Private Certificate (C) or Public Certificate (D) requires you to handle procurement and manual renewal yourself. The Free Managed Certificate is therefore the only option that satisfies auto-renewal without needing wildcard certificates or private DNS.",
    "options": [
      "Free Managed Certificate",
      "App Service Certificate",
      "Upload a Private Certificate",
      "Upload a Public Certificate"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "ecd3a302eff9a8f429df0d58c7217909801225ad11f8d6c9216a31a14f354f13",
    "question": "You need to manage a private certificate in Azure with automated renewal that can also be exported. Which option should you choose?",
    "answer": "App Service Certificate provides automated certificate management, renewal, and export options for private certificates.",
    "options": [
      "Free Managed Certificate",
      "App Service Certificate",
      "Upload a Private Certificate",
      "Upload a Public Certificate"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "677de7595ca0a1aa7dd233bf09765e667143e62eb90a145af0bb8a524d5cbc5f",
    "question": "You need to access remote resources in Azure App Service. Which certificate option should you choose?",
    "answer": "Public Certificate is used for accessing remote resources.",
    "options": ["Use Free Managed Certificate", "App Service Certificate", "Private Certificate", "Public Certificate"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "d2246acd032bb1e53bc7d18b6e8a1c701bd59dfec4975f723972d8ce855f25f7",
    "question": "Which of the following steps is necessary to secure a custom domain using a certificate?",
    "answer": "You must upload a private certificate that satisfies all the requirements for a private certificate.",
    "options": [
      "Upload a private certificate to the App Service, ensuring that it meets all the requirements for private certificates.",
      "Upload a private certificate to the App Service, ensuring that it meets all the requirements for public certificates.",
      "Upload a public certificate to the App Service, ensuring that it meets all the requirements for private certificates.",
      "Upload a public certificate to the App Service, ensuring that it meets all the requirements for public certificates."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "41b04031813b17224df9943b50b2e580fa7b6a1278780d72afe7827fc480f59e",
    "question": "What is Transient Fault Handling in the context of cloud applications?",
    "answer": "A technique to handle temporary service interruptions by implementing smart retry/back-off logic, possibly using circuit breakers.",
    "options": [
      "A permanent error handling mechanism that requires manual intervention to resolve issues.",
      "A technique to handle temporary service interruptions by implementing smart retry/back-off logic, possibly using circuit breakers.",
      "A security protocol to handle unauthorized access to cloud services.",
      "A method for handling user interface glitches in cloud-based applications."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "d2138f6b3debaaa9568c21d286fc5804170be79176819b0785f2d2ee8595c857",
    "question": "You are managing a web application hosted on an Azure Web App. The application is expected to experience a significant increase in traffic soon. You are tasked with configuring auto-scaling for the web app. The requirement is to scale out when the CPU usage goes above 85% for a period of 10 minutes and to scale in when the CPU usage drops below 70%. Which two Azure CLI commands would you use to set up the required auto-scaling rules?",
    "answer": "`az monitor autoscale rule create`, not `az webapp autoscale rule create`",
    "options": [
      "`az monitor autoscale rule create -g ResourceGroup --resource WebAppName --autoscale-name AutoScaleName --scale out 1 --condition \"Percentage CPU > 85 avg 10m\"`",
      "`az webapp autoscale rule create -g ResourceGroup --resource WebAppName --autoscale-name AutoScaleName --scale out 1 --condition \"Percentage CPU > 85 avg 10m\"`",
      "`az monitor autoscale rule create -g ResourceGroup --resource WebAppName --autoscale-name AutoScaleName --scale in 1 --condition \"Percentage CPU < 70 avg 10m\"`",
      "`az webapp autoscale rule create -g ResourceGroup --resource WebAppName --autoscale-name AutoScaleName --scale in 1 --condition \"Percentage CPU < 70 avg 10m\"`"
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "6c6f48a0ee37de5aa6fee33b6ac4cebf5298e160194f555c2649b708d4997454",
    "question": "To utilize Application Insights Profiler in an Azure Web App, which configuration must be enabled?",
    "answer": "The \"Always On\" setting is required to enable Application Insights Profiler for an Azure Web App. This setting keeps the app loaded even when there's no incoming traffic, allowing the profiler to collect data effectively.",
    "options": [
      "Cross-Origin Resource Sharing (CORS)",
      "Always On Feature",
      "Identity Activation",
      "Custom Domain Configuration"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "8cfd49f85226030186408424f8fa9d7a3310ae51cda795313c117b3d3bd2a3e1",
    "question": "Your company is evaluating Azure App Services Basic plan and needs to have some continuous WebJobs and WebJobs triggered by a CRON expression. What steps must be taken, while staying cost effective?",
    "answer": "To run Continuous WebJobs or WebJobs triggered by a CRON expression in Azure App Service, you must enable the \"Always On\" feature and have at least a \"Basic Plan\". The \"Always On\" setting ensures that the WebJobs continue running even when there is no incoming traffic, and the \"Basic Plan\" or higher is required to support these operations.",
    "options": [
      "Activate Deployment Slots",
      "Turn on \"Always On\" Feature",
      "Disable IP Restrictions",
      "Switch to Standard Plan",
      "Switch to Premium Plan",
      "Configure a free managed certificate"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "212dcfe3114867da4afa4c7d25488ad6fb2fa2be3dc69e4f2bf87e19414ce74f",
    "question": "Your organization has a web app deployed on Azure using the D1 App Service Plan. You are tasked with setting up the infrastructure to automatically scale when CPU utilization hits 85%, while also keeping costs low. Which of the following actions should you take to meet these objectives?",
    "answer": "The D1 App Service Plan is a Shared Service Plan, which doesn't support autoscaling. Therefore, you would need to switch the web app to the Standard App Service Plan.",
    "options": [
      "Activate autoscaling for the Web App",
      "Set up a scaling condition",
      "Switch the web app to the Standard App Service Plan",
      "Upgrade the web app to the Premium App Service Plan",
      "Create a scaling rule"
    ],
    "answerIndexes": [0, 1, 2, 4],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "d59657198688bf7ac7c9bfb37693e3609320b3bce59e602845e43cac504c074b",
    "question": "You are an Azure developer responsible for maintaining a web application hosted on Azure App Service. Users are reporting unexpected behavior within the application, and you need to diagnose the issue by capturing errors and trace information from your application code. What should you do?",
    "answer": "Enabling the Application Logging feature allows you to capture application-level events and errors, which is useful for debugging issues within your application code.  \nAzure Monitor is more for monitoring performance metrics and not specifically tailored for application-level logging.  \nWeb Server Logging captures HTTP-level information, not application-level issues.  \nResource health alerts are for monitoring the health of Azure resources, not for application-level debugging.",
    "options": [
      "Configure Azure Monitor to capture telemetry data.",
      "Enable the Web Server Logging feature.",
      "Enable the Application Logging feature.",
      "Create resource health alerts."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "e771f72c5bf7cf0affa336da6b076783689f81d0f7920964d135ea0107251b88",
    "question": "Your Azure App Service-hosted web application named `webapp-prod` is experiencing issues. Users are complaining about receiving HTTP `404` errors when trying to access certain pages. You need to diagnose these issues by capturing detailed information about each HTTP request. What should you do?",
    "answer": "Enabling the Web Server Logging feature captures all HTTP requests and responses, which is useful for diagnosing issues like HTTP 404 errors.  \nAzure Application Insights is more focused on application performance monitoring and not specifically tailored for capturing all HTTP requests and responses.  \nApplication Logging is for capturing application-level events and errors, not HTTP-level information.  \nAzure Network Watcher is for monitoring, diagnosing, and viewing metrics for Azure networking resources, not specifically for HTTP-level logging.",
    "options": [
      "Enable Azure Application Insights.",
      "Enable the Application Logging feature.",
      "Enable the Web Server Logging feature.",
      "Set up Azure Network Watcher."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "fca8fe913e0cc83e5f32c4fa461f44808a9a0068c2c554d0695e167f4a1420b2",
    "question": "Which of the following won't trigger a restart?",
    "answer": "Storage failover requires app restart or remounting of Azure Storage.",
    "options": [
      "Changes in App Settings",
      "Changes in Connection strings",
      "Changes in Storage Mounts",
      "Storage failover"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "App Service"
  },
  {
    "id": "bf40bc68196ccd36c0f4935b2c688e5fc6f993f220158e4e8bb64b7fdc0b449d",
    "question": "Which of the following availability tests is recommended for authentication tests?",
    "answer": "Custom TrackAvailability test is the long term supported solution for multi request or authentication test scenarios.  \nThe URL ping test is used to test endpoint availability.  \nStandard test is similar to the URL ping test, but it includes additional information.",
    "options": ["URL ping", "Standard", "Custom TrackAvailability"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "6e4590232d2eca0b7a99832659931cc92bee8db8c04d61cdb6e57389dd79ec92",
    "question": "Which of the following metric collection types provides near real-time querying and alerting on dimensions of metrics, and more responsive dashboards?",
    "answer": "Pre-aggregated metrics are stored as a time series and only with key dimensions, which enable near real-time alerting on dimensions of metrics, more responsive dashboards.  \nAzure Service Bus is message queueing service.  \nLog-based metrics are aggregated at query time and require more processing to produce results.",
    "options": ["Log-based", "Pre-aggregated", "Azure Service Bus"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "5cc48aa818c3592d56547a360ea2076c04e780cbec641b5848cea0a828411c63",
    "question": "Which of the following metrics are pre-aggregated?",
    "answer": "Standard metrics are pre-aggregated.",
    "options": [
      "Log based metrics",
      "Standard metrics",
      "All metrics",
      "Metrics cannot be pre-aggregated",
      "Only metrics enabled by the developer"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "75b75371ab8b8dbb92ce765c3a9898d05fc445c1e5f5da6fd5a11d7409f6d5da",
    "question": "Where is the exception logged?\n\n```csharp\ntry { Save(); }\ncatch(Exception ex)\n{\n     var client = new EventSource();\n     client.Write(ex.Message);\n}\n```",
    "answer": "The exception gets logged to Event Log. The Write method of the EventSource class allows you to log data to the Event Log.",
    "options": ["Diagnostic trace listener", "The Trace.aspx page", "Application Insights", "Event Log"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "033bc063c533372d3c6723e850b1a907243cfcbfa4268222b9594e5da3debb6e",
    "question": "Where is the exception logged?\n\n```csharp\ntry { Save(); }\ncatch(Exception ex)\n{\n     Trace.Error(ex.Message);\n}\n```",
    "answer": "The exception gets logged to a diagnostic trace listener. The Trace.Error method allows you to log exceptions to a diagnostic trace listener. Alternatively, you would need to configure tracing for the application.",
    "options": ["Diagnostic trace listener", "The Trace.aspx page", "Application Insights", "Event Log"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "cc0be0062aff4e099bbc31beb01da54b2760e4decbcf1898a01be0045717ce8c",
    "question": "Where is the exception logged?\n\n```csharp\ntry { Save(); }\ncatch(Exception ex)\n{\n     var client = new TelemetryClient();\n     client.TrackException(ex);\n}\n```",
    "answer": "The exception gets logged to Application Insights.",
    "options": ["Diagnostic trace listener", "The Trace.aspx page", "Application Insights", "Event Log"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "ac2faf68cf9ba4e2f8a5c78666e2e50896ec500ac650b5ed2756a631e3298d3c",
    "question": "Where in Application Insights does the following code send data to?\n\n```csharp\ntelemetry.TrackEvent(\"WinGame\");\n```",
    "answer": "The `TrackEvent` method is used to log custom events, which can be used to track user interactions or other significant occurrences within the application.",
    "options": ["Custom Metrics", "Page Views", "Custom Events", "Dependency Tracking"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "587ba8b1ccab15a69b7701e287c718c6f03d667b4471b505e992f1b51bdbeb67",
    "question": "You have the following code:\n\n```csharp\ntelemetry.TrackTrace(\"Monitor\");\n```\n\nCan you use `az monitor activity-log` to view it?",
    "answer": "`az monitor activity-log` _cannot_ display data from Application Insight telemetry.",
    "options": ["Yes", "No"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "8bb2b88fe24633cd35ffae761ec6cc37e791ca3186a51c08937d5084676670b8",
    "question": "Where in Application Insights does the following code send data to?\n\n```csharp\ntelemetry.TrackDependency(\"Database\", \"Query\", startTime, timer.Elapsed, success);\n```",
    "answer": "The `TrackDependency` method is used to log calls to external dependencies, such as databases, services, or other external resources. This information can be used to monitor the performance and success rate of these calls, helping to identify potential bottlenecks or failures in the system.",
    "options": ["Custom Metrics", "Page Views", "Custom Events", "Dependency Tracking"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "59090e87795bc589b93d9edb5831886c9e91be664aa6382b2d8fd2de58786032",
    "question": "In your organization, you are working on an ASP.NET web application that is hosted on a development server within the company's internal network. Before transitioning the application to the production server, you must gather performance data from various global locations. What action should you take?",
    "answer": "Application Insights provides the ability to monitor, detect, and diagnose performance issues, including those related to geographic locations. A URL ping test specifically allows you to test the responsiveness of a web application from different regions, aligning with the requirement to capture performance metrics from multiple geographies.  \nTraffic Manager sets up close endpoints to users to boost performance. Endpoint monitoring checks if these endpoints are working well with a probing agent.",
    "options": [
      "Implement a URL ping test using Application Insights.",
      "Set up HTTP forwarding through an Application Gateway.",
      "Establish a VPN connection with a Global Network.",
      "Utilize a Load Balancer across diverse availability regions.",
      "Enable monitoring of endpoints in a Traffic Manager profile."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "460e7a814e5ef9cfefc73cbe3d35d26306ada01142f7219905f9a9967f3286ed",
    "question": "In Application Insights, what does the Users tool specifically count?",
    "answer": "The Users tool in Application Insights counts how many people have used the app and its features.",
    "options": ["Page views", "Button clicks", "People using the app", "User sessions"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "672f1bc0fa4ae02b3cb84b5732a5366ef4373e36fc4d3d97557f97b5cb22fe44",
    "question": "When session resets?",
    "answer": "In Application Insights, a session is reset after half an hour of user inactivity, or 24 hours of continuous usage.",
    "options": ["After 30 minutes of inactivity", "After 24 hours of continuous use", "On application error", "Never"],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "2d6b889b6f8668e1548e11e4d0a38d60541d7b39f2f5ced22092d20133752c85",
    "question": "Which of the following can be represented as a custom event in Application Insights?",
    "answer": "Custom events in Application Insights often represent specific occurrences like user interactions such as button selections or task completions.",
    "options": ["A page view", "A user interaction like a button selection", "A session reset", "Both A and B"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "a2f337b5cf662c460128a8db77c56ba7bcacc2d0e1fb16735738d8ee1f210d1b",
    "question": "In Application Insights, if a single person uses different browsers or machines, how are they counted in the Users tool?",
    "answer": "In Application Insights, a single person using different browsers or machines will be counted as more than one user by the Users tool.",
    "options": ["As one user", "As more than one user", "As an inactive user", "Not counted"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "efe258362e0b0b51ec56f33f302a4b5e739c9f2c76c72668ca356b0d6d860beb",
    "question": "What tool should you use in Application Insights to monitor page views?",
    "answer": "The Events tool in Application Insights is used to measure how often pages and features are used, including counting when a browser loads a page from your app.",
    "options": ["Users tool", "Sessions tool", "Events tool", "Custom Events tool"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "e7df29573e35ce0b06e9f1aec57b052ed4a6b3fee5fc056ff12a0bd107072fa8",
    "question": "You're a product manager for an e-commerce site and want to understand the customer journey from browsing products to completing a purchase. Which Azure Insights feature would you use to identify the stages where customers are dropping off?",
    "answer": "Funnels are used to track how users move through different stages of a web application. They help identify where users may stop or leave the app, providing insights into effective areas and where improvements are needed.",
    "options": ["Users", "Sessions", "Events", "Funnels", "Cohorts", "Impact", "Retention"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "69670c28f18bd653aca9657af7bf83cf8eb1dc9f0d84f748410cf9d1d3127600",
    "question": "What is the primary purpose of the connection string in Application Insights?",
    "answer": "The connection string in Application Insights is used to define where the telemetry data is sent.  \nIt doesn't handle authentication, appearance, or encryption of the data.",
    "options": [
      "Authenticates the Application Insights SDK",
      "Controls the visual appearance of telemetry data",
      "Controls where telemetry is sent",
      "Encrypts telemetry data for secure transmission"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "01fcca4dbb782ad1a9e712e5ea40786f862ccfc330fc90ed7746580d0b3a798f",
    "question": "As an Azure Developer, you are tasked with integrating Azure Application Insights into a mobile application using the Azure Mobile Apps SDK. Your goal is to collect telemetry data for analyzing user behavior. Which of the following telemetry types should you avoid manually capturing for this purpose?",
    "answer": "Application Insights already takes care of this for you. Manually capturing it would be redundant and could interfere with the automatically generated Session Ids.",
    "options": ["Exception", "Events", "Session Id", "Trace"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "2696854979974eb8e4f9c1ccd8d0926a220d74a0e46e992733df09053e4a8c83",
    "question": "Your web application must be able to scale on demand. Which Azure Application Insights data model should you use?",
    "answer": "You can use Application Insights metrics to scale Web Apps.",
    "options": [
      "An Application Insights metric",
      "An Application Insights dependency",
      "An Application Insights trace",
      "An Application Insights event"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "fde321bf682d6395c4a3652d802d6cb9a41772e66d71372f6de4e1edcbb8df66",
    "question": "Your ASP.NET application is generating a high volume of telemetry data, causing you to exceed your data quota. You want to reduce the telemetry traffic while maintaining a statistically correct analysis. Which sampling method would you use?",
    "answer": "Adaptive sampling automatically adjusts the volume of telemetry sent from the SDK in your ASP.NET application. It is particularly useful for high-volume applications to avoid exceeding data quotas.",
    "options": ["Adaptive Sampling", "Fixed-rate Sampling", "Ingestion Sampling"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "0c600759ed5b2904900f46e53092be4f169453990b443acacb491784679e4453",
    "question": "You have a Java application and you need fine-grained control to apply different sampling rates to selected dependencies, requests, and health checks. Which sampling method would you use?",
    "answer": "Fixed-rate sampling is available for Java applications and allows you to set the rate yourself. It also supports sampling overrides for fine-grained control over telemetry.",
    "options": ["Adaptive Sampling", "Fixed-rate Sampling", "Ingestion Sampling"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "e5c20aa1b51885b407ebb9f566e144d80ad44669388159489ea469489ee5f8c7",
    "question": "You have a running application and you want to control the rate of data ingestion without redeploying the application. Which sampling method would you use?",
    "answer": "Ingestion sampling happens at the Application Insights service endpoint and allows you to set the sampling rate without redeploying your app. It helps you keep within your monthly quota.",
    "options": ["Adaptive Sampling", "Fixed-rate Sampling", "Ingestion Sampling"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "ce1c865d2de3c9d1654ac0d59d2aa430795124b349db58d8e7bf17cd86674a5d",
    "question": "Which method should be used here instead of `XXXXXXXXXX` to track response time:\n\n```cs\nvar startTime = DateTime.UtcNow;\nvar timer = System.Diagnostics.Stopwatch.StartNew();\n\ntry {\n  await sendRequest();\n}\nfinally {\n  timer.Stop();\n  telemetry.XXXXXXXXXX(\"type\", \"name\", \"myTracedRequest\", startTime, timer.Elapsed, success);\n}\n```",
    "answer": "`TrackDependency`: for those dependencies not automatically collected by SDK, like response times.",
    "options": ["`TrackEvent`", "`TrackException`", "`TrackDependency`", "`TrackRequest`", "`TrackTrace`"],
    "answerIndexes": [2],
    "hasCode": true,
    "topic": "Application Insights"
  },
  {
    "id": "e1e242a7cebc14c76ee1f20560319d29dd13e262b1852d87280af0d406f2e990",
    "question": "You’re designing a distributed architecture deployed in containers across AKS and App Services. You want to visualize the full topology in Application Map, including dependencies between components.\n\nWhich actions you should take?",
    "answer": "\n\n- Using the same Application Insights instrumentation key across all services ensures telemetry goes to the same Application Insights resource.\n- Defining `service.name` and `service.namespace` ensures proper role naming and grouping on the map via OpenTelemetry.\n- Running each service in the same Azure subscription is irrelevant.\n- `cloud_RoleInstance` is indirectly set via `service.name` and `service.namespace`.\n- `cloud_RoleInstance` defines instance-level granularity, not grouping.",
    "options": [
      "Use the same Application Insights instrumentation key across all services.",
      "Ensure that each service runs in the same Azure subscription.",
      "Set `cloud_RoleName` uniquely for each component.",
      "Define `service.name` and `service.namespace` attributes in each service's telemetry configuration.",
      "Use `cloud_RoleInstance` to ensure component grouping."
    ],
    "answerIndexes": [0, 3],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "80f7d956e8167c7979ef54cdb8b67d11e407c2da691c8c5a536841cd60dc15ef",
    "question": "You are containerizing a microservice called `inventory-service`, part of the `storefront` platform. You want it to appear under the correct logical grouping in the Application Map.\n\nWhich telemetry attributes should you configure?",
    "answer": "\n\n- `service.namespace` defines grouping\n- `service.name` defines logical service\n- `cloud_RoleName` is derived internally from these.\n- `service.name = storefront` is incorrect because it flips the names.\n- `cloud_RoleInstance = instance-01` is optional and relates to instances, not grouping.",
    "options": [
      "`service.name = storefront`",
      "`cloud_RoleName = storefront.inventory-service`",
      "`service.namespace = storefront`",
      "`cloud_RoleInstance = instance-01`",
      "`service.name = inventory-service`"
    ],
    "answerIndexes": [2, 4],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "d9782c867bdc7e4cc873a445ac8db8f181afad017b46299ffba616a09cc3eebd",
    "question": "You instrument an ASP.NET Core app using OpenTelemetry. You want to track incoming HTTP requests and outgoing calls to a payment gateway.\n\nWhich span kinds should you explicitly use?",
    "answer": "\n\n- ✅ `Server` for incoming HTTP requests\n- ✅ `Client` for outbound calls (e.g. to the payment gateway)\n- ❌ `Producer` is for message sending\n- ❌ `Internal` is for in-process, not I/O\n- ❌ `Consumer` is for receiving from queues",
    "options": ["`Server`", "`Client`", "`Producer`", "`Internal`", "`Consumer`"],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "13ded3d8be5e8193208e12eff68ea611d1b26ea757f3cfa8219c71c10eb06279",
    "question": "You're debugging a performance issue in a multi-service architecture. Application Map shows incomplete dependency lines between services.\n\nWhat are the likely causes?",
    "answer": "The likely causes for incomplete dependency lines between services in a multi-service architecture, as indicated by the Application Map, are:\n\n- Services send telemetry to separate Application Insights resources. This breaks the map since Application Insights doesn’t correlate across resources.\n- `cloud_RoleName` is not uniquely configured per service. This prevents unique nodes from appearing in the Application Map.\n\nOther options are less likely or irrelevant:\n\n- The `service.namespace` being missing from some components could cause grouping issues but not necessarily map breakage.\n- Services not exposing their metrics via Prometheus exporters is irrelevant to Application Insights.\n- HTTP calls between services being done using raw TCP sockets is a problem because Application Map expects HTTP dependencies, but this is not one of the primary causes listed.",
    "options": [
      "Services send telemetry to separate Application Insights resources.",
      "Services don’t expose their metrics via Prometheus exporters.",
      "`cloud_RoleName` is not uniquely configured per service.",
      "The `service.namespace` is missing from some components.",
      "The HTTP calls between services are done using raw TCP sockets."
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "441abe2d22345360a0121258a40b9148faef7a7003db9f9ef45ff15f44bef2e4",
    "question": "You’re sending custom metrics with OpenTelemetry from a .NET backend. You want to capture detailed latency info, including percentiles, across key endpoints.\n\nWhich OpenTelemetry instrument should you use?",
    "answer": "Histogram enables min/max/avg/count → ideal for latency.\n\n- Counter only aggregates sums.\n- UpDownCounter tracks increments/decrements.\n- ObservableGauge is for instantaneous measurements (like memory usage).",
    "options": ["Counter", "UpDownCounter", "Histogram", "ObservableGauge"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "93e510f60f30cedda5f2eaca0742c063c7bf203c525eedc701c0fc7729f0eb3c",
    "question": "You’re deploying a worker service that pulls messages from Azure Service Bus and logs operations to Application Insights via OpenTelemetry. You want telemetry to appear as “Requests” in Application Insights.\n\nWhat should you do?",
    "answer": "Consumer spans are mapped to Requests in Application Insights.\n\n- Server is for HTTP handling.\n- Client is for outgoing calls.\n- Histogram is not relevant to classification.",
    "options": [
      "Set the span kind to Consumer when processing messages.",
      "Set the span kind to Server when dequeuing.",
      "Define the span as Client to match dependency tracing.",
      "Use a Histogram to record the message processing latency."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "d8ef46531d26abaa596f95d59fd8b287ab657520836794fa3f7596c5c1e34c3a",
    "question": "You want to reduce noise in your telemetry and exclude internal spans and health checks from being sent to Application Insights.\n\nWhich approaches should you implement?",
    "answer": "Custom processor and instrumentation filtering are correct ways to prevent span creation/export.\n\n- Filters after ingestion—cost is still incurred.\n- Disabling telemetry disables all collection.\n- Using firewall is a networking workaround—not a trace filtering solution.",
    "options": [
      "Use a custom span processor to drop internal spans.",
      "Filter spans in Azure Monitor Analytics query language (KQL).",
      "Disable telemetry auto-collection in the Application Insights config.",
      "Use instrumentation-level filtering before export.",
      "Configure firewall rules to block span export from local hosts."
    ],
    "answerIndexes": [0, 3],
    "hasCode": false,
    "topic": "Application Insights"
  },
  {
    "id": "25d1866283c8a8601b27db75144a9eb91f19ed8593c5b5ec76931dd3d8f0007e",
    "question": "Where can you discover the autoscale settings in Azure subscription?",
    "answer": "The correct procedure to discover the autoscale settings in your subscription is to open the Azure portal, search for and select Azure Monitor, and then select Autoscale to view all the resources for which autoscale is applicable.",
    "options": [
      "Azure Portal > Azure Storage > Autoscale",
      "Azure Portal > Azure Monitor > Autoscale",
      "Azure Portal > Resource Groups > Autoscale",
      "Azure Portal > Virtual Machines > Autoscale",
      "Azure Portal > Azure Functions > Autoscale"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Azure"
  },
  {
    "id": "2178f1a390110d4106bb67fddfb1072e7ab7a2d29932604cfa4e874735bf7d9b",
    "question": "In Azure, when an application is scaled out to multiple instances, which service can be configured to conduct health checks on these instances and direct traffic only to the healthy ones?",
    "answer": "Azure App Service can perform health checks on instances, routing traffic only to healthy ones.",
    "options": ["Azure Service Health", "Load Balancer", "App Service", "Azure Traffic Manager"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Azure"
  },
  {
    "id": "1c341674b0b4f4ba0cf55a5bb48c961e11d9d8aa60b976516ac9d69fa9a89ed7",
    "question": "Which Azure service enables you to create subscriptions to events raised by third-party resources?",
    "answer": "Event Grid is designed to create subscriptions to events raised by Azure services, third-party resources, and custom topics.  \nEvent Hub is used for big data streaming and event ingestion, not for subscribing to third-party events.  \nService Bus is a message broker service, not designed for subscribing to third-party events.  \nNotification Hubs are used for sending mobile push notifications, not for subscribing to third-party events.",
    "options": ["Azure Event Hub", "Azure Service Bus", "Azure Event Grid", "Azure Notification Hubs"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Azure"
  },
  {
    "id": "26eaa30f5e917cbe9b0e4e9ca941d7937c0f680cc464f481d83a2b7f58bb4cf3",
    "question": "You need to configure an application performance management (APM) service to collect and monitor the application log data. Which Azure service should you configure?",
    "answer": "Application Insights is a feature of Azure Monitor that provides extensible application performance management (APM) and monitoring for live web applications. Azure Monitor helps you maximize the availability and performance of applications and services.",
    "options": ["Azure Monitor", "Log Analytics", "Application Insights", "Azure Advisor"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Azure"
  },
  {
    "id": "752cb082cdbcde7235f6dc1a55ff4b6e3389c65d552a39530bcbf7677f488e56",
    "question": "Which of the following types of blobs are used to store virtual hard drive files?",
    "answer": "Page blobs store random access files up to 8 TB in size, and are used to store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.  \nAppend blobs are optimized for data append operations.  \nBlock blobs are made up of blocks of data that can be managed individually.",
    "options": ["Block blobs", "Append blobs", "Page blobs"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "71f0ab17f0b413be64f5eeabdbeb5afe3c02620ffb7dc725fcc4003633ec486c",
    "question": "Which of the following types of storage accounts is recommended for most scenarios using Azure Storage?",
    "answer": "General-purpose v2 supports blobs, files, queues, and tables. It's recommended for most scenarios using Azure Storage.  \nFileStorage: This is recommended for enterprise or high-performance scale applications and won't cover most scenarios.  \nGeneral-purpose-v1 is a legacy account type.",
    "options": ["General-purpose v2", "General-purpose v1", "FileStorage"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "275192f40637b388b5e59b45ae3a31c5a58544b3237060797ccd3785588b9ea9",
    "question": "What is the maximum size of data that a block blob in Azure Blob storage can store?",
    "answer": "Block blobs in Azure Blob storage can store up to about 190.7-TiB of data.",
    "options": ["8-TB", "Unlimited", "190.7-TiB"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "960bf81f8aeaa526863aaf67479104fb47e3a0c43d86039a86b3e7015913c676",
    "question": "You are developing a service that organizes uploaded documents into containers in Azure Blob Storage. Your application assigns metadata to each container, including user names and titles that may contain international characters (e.g., Vietnamese or Bulgarian diacritics). You use the `SetMetadata` method on a `BlobContainerClient` to store this metadata. During testing, you discover that the metadata appears corrupted or incomplete when retrieved. You need to ensure the metadata is stored and retrieved correctly without losing any special characters.\n\nWhat should you do?",
    "answer": "Azure Storage metadata only supports ASCII characters. If you need to store non-ASCII characters (e.g., Unicode), you must encode them, and Base64 is the standard workaround.",
    "options": [
      "Convert the metadata values to Base64 before storing them.",
      "Re-save metadata again to ensure values are stored properly.",
      "Encode the values in UTF-8 and pass them directly to `SetMetadata`.",
      "Change the storage account's encoding settings to allow Unicode metadata.",
      "`SetMetadata` is not a valid method on a `BlobContainerClient` object."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "b6eff3366e99ba8256ef715b35b5185cf84650fb70b59e2a068a9117670401b4",
    "question": "What are the two versions of client-side encryption available in the Azure Blob Storage and Queue Storage client libraries?",
    "answer": "Version 1 uses CBC mode with AES, while Version 2 uses GCM mode with AES.",
    "options": [
      "Version 1 uses Cipher Block Chaining (CBC) mode with AES and Version 2 uses Galois/Counter Mode (GCM) mode with AES",
      "Version 1 uses Advanced Encryption Standard (AES) and Version 2 uses Federal Information Processing Standards (FIPS)",
      "Version 1 uses Galois/Counter Mode (GCM) mode with AES and Version 2 uses Cipher Block Chaining (CBC) mode with AES"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "1e04c069074f66f3197b9079593c8b27c5f1ee985db672e96b1b22cef48b606d",
    "question": "Which access tier is considered to be offline and can't be read or modified?",
    "answer": "Blobs in the archive tier must be rehydrated to either the hot or cool tier before it can be read or modified.",
    "options": ["Cool", "Archive", "Hot"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "d9bb1ee389eaf79baf078c1b05b8dbbfbad23a5eda2b7b14be10f617ccb55e48",
    "question": "Which of the following storage account types supports lifecycle policies?",
    "answer": "Azure Blob storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and Blob storage accounts.  \nGeneral Purpose v1 accounts need to be upgraded to v2 before lifecycle policies are supported.",
    "options": ["General Purpose v1", "General Purpose v2", "FileStorage"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "d92f5c832516fa4f2d88ad30602855b012db7f41cc5c17aaa09c04607dd18c69",
    "question": "Which of the following standard HTTP headers are supported for both containers and blobs when setting properties by using REST?",
    "answer": "Last-Modified and ETag are supported on both containers and blobs.  \nContent-Length and Cache-Control are only supported on blobs.",
    "options": ["Last-Modified", "Content-Length", "Origin", "Cache-Control", "ETag"],
    "answerIndexes": [0, 4],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "8441a07867f9ce1edfb14023294b3cf834d52357a164a10f40fda96feb7def61",
    "question": "Which of the following classes of the Azure Storage client library for .NET allows you to manipulate both Azure Storage containers and their blobs?",
    "answer": "The BlobContainerClient can be used to manipulate both containers and blobs.  \nThe BlobUriBuilder provides a way to modify the contents of a Uri instance to point to different Azure Storage resources like an account, container, or blob.  \nThe BlobClient class is limited to manipulating blobs.",
    "options": ["BlobClient", "BlobContainerClient", "BlobUriBuilder"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "01e1cf4417298c41771ecf46a4a7f8e74cd3ac4fa1e99339972c9fffe0e79a5b",
    "question": "As an Azure Developer working for a company named Contoso, your task involves managing the company's Azure storage account. The storage account contains numerous block blobs, all of which are tagged with specific metadata indicating the project they are associated with. For instance, some blobs are tagged as \"Project: Contoso\".\n\nHowever, due to new privacy regulations, Contoso has decided to delete all blobs tagged with \"Project: Contoso\" as soon as possible after they have been uploaded to the storage. This is to ensure that sensitive data is not retained longer than necessary.\n\nYour task is to create an Azure Storage lifecycle policy `DeleteContosoData` (as a JSON definition) to automate this process.\n\n```jsonc\n// Code here\n```",
    "answer": "\n\n```json\n{\n  \"rules\": [\n    {\n      \"enabled\": true,\n      \"name\": \"DeleteContosoData\",\n      \"type\": \"Lifecycle\",\n      \"definition\": {\n        \"actions\": {\n          \"baseBlob\": {\n            \"delete\": {\n              \"daysAfterModificationGreaterThan\": 0\n            }\n          }\n        },\n        \"filters\": {\n          \"blobIndexMatch\": [\n            {\n              \"name\": \"Project\",\n              \"op\": \"==\",\n              \"value\": \"Contoso\"\n            }\n          ],\n          \"blobTypes\": [\"blockBlob\"]\n        }\n      }\n    }\n  ]\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Blob Storage"
  },
  {
    "id": "867f5034a87f30aacd0354d68647be47684f92b53751735202b501f99ee8bbee",
    "question": "How frequently are Azure Storage lifecycle management policy rules evaluated?",
    "answer": "Azure Storage lifecycle management policies are evaluated once a day. In practice, this means that the policies are not necessarily applied immediately after a blob satisfies the policy rule.",
    "options": [
      "Every time a new blob is added or an existing blob is modified.",
      "Once every hour.",
      "Once a day",
      "Once a week",
      "They are manually triggered by the user (by default)"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "8b59efbee02f1c8802a65f8db5f78bcdba458276985847b74f0723e8fdd014cc",
    "question": "As a cloud solutions developer, you're working with an existing codebase that uses Azure's legacy Storage SDK. During a service disruption at the primary data center, you need to configure the Azure Storage client to retry any failed requests on the secondary location using RA-GRS storage.\n\nWhich LocationMode option from the `Microsoft.Azure.Storage.RetryPolicies.LocationMode` class should you use?",
    "answer": "`LocationMode.PrimaryThenSecondary` allows requests to first try the primary location and then retry at the secondary location if necessary.  \nThis option is part of the legacy Azure Storage SDK. In the newer Azure SDKs (like Azure.Storage.Blobs), the handling of retries and failovers has been reworked and made more efficient.",
    "options": [
      "LocationMode.PrimaryOnly",
      "LocationMode.SecondaryOnly",
      "LocationMode.PrimaryThenSecondary",
      "LocationMode.SecondaryThenPrimary"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "9b815fb0da09534c67215bd91acd82c3aa1ccb3561d81daf8e231f537b1ab105",
    "question": "In the Azure Blob Storage .NET SDK, you want to ensure that you only update a blob if it has not been modified since you last fetched it. Here is the initial code you have:\n\n```cs\nBlobServiceClient blobServiceClient = new BlobServiceClient(connectionString);\nBlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(\"mycontainer\");\nBlobClient blobClient = containerClient.GetBlobClient(\"myblob\");\n\nBlobProperties properties = await blobClient.GetPropertiesAsync();;\n\n// perform the update\nBlobUploadOptions options = new BlobUploadOptions\n{\n    Metadata = new Dictionary<string, string>\n    {\n        { \"key\", \"value\" }\n    },\n    // Fill in this part\n};\n\nawait blobClient.UploadAsync(BinaryData.FromString(\"data\"), options);\n```\n\nWhat should you put in the `// Fill in this part` section to ensure the blob is only updated if it has not been modified since you last fetched it?",
    "answer": "The missing code should be as follows:\n\n```cs\nConditions = new BlobRequestConditions { IfMatch = properties.Value.ETag }\n```\n\nThe `IfMatch` property of the `BlobRequestConditions` object can be set to the ETag of the blob. The `ETag` is a version identifier for the blob, and it changes whenever the blob is modified. By setting `IfMatch` to the `ETag` you got when you fetched the blob's properties, you're specifying that the update should only occur if the blob has not been modified since then. If the blob has been modified, its `ETag` will have changed, the IfMatch condition will not be met, and the update operation will fail with a `412 Precondition Failed` error. This approach is used to implement optimistic concurrency control, preventing unexpected overwrites due to concurrent modifications.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Blob Storage"
  },
  {
    "id": "ff9a19b588b0ca3b1b21868045ba84be2d2a065dc19e566d4f1d9aa3edeab93b",
    "question": "GreenTech Energy Solutions specializes in providing real-time energy consumption data to its clients. They store sensor data in Azure Blob Storage and have decided to maintain two copies of each data file in the 'North Europe' and 'South Africa North' Azure regions. New sensor data files are initially uploaded to the 'North Europe' blob storage. What Azure tool and command would you use to ensure both blob containers have completely identical data?",
    "answer": "`azcopy sync --recursive` is the correct choice because it not only copies the blobs but also ensures that both blob containers are in sync. The `--recursive` flag ensures that all subdirectories and their files are also synced.  \n`azcopy copy --recursive` will copy the entire content of source container to destination container, which is not an optimal approach.",
    "options": [
      "`Copy-AzStorageBlob`",
      "`az storage blob copy start-batch`",
      "`azcopy sync --recursive`",
      "`azcopy copy --recursive`"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "e0bf70619b5b32792b01301980500288e0d37d1874a64d6ffa3a0cbe06dab77b",
    "question": "Which of the following methods is the most optimal way to copy a blob from one container to another in Azure Blob Storage?",
    "answer": "`await targetBlob.StartCopyFromUriAsync(sourceBlob.Uri);`  \n`await sourceBlob.DownloadToAsync(localStream); await targetBlob.UploadAsync(localStream);` is inefficient because it requires downloading the blob to local storage and then uploading it, consuming more time and resources.",
    "options": [
      "`await targetBlob.StartCopyFromUriAsync(sourceBlob.Uri);`",
      "`await sourceBlob.StartCopyFromUriAsync(targetBlob.Uri);`",
      "`await sourceBlob.DownloadToAsync(localStream); await targetBlob.UploadAsync(localStream);`",
      "`await targetBlob.DownloadToAsync(localStream); await sourceBlob.UploadAsync(localStream);`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "d24610a60bf837c97243c6e0da3cc0dfaaf64ce3afc197039074538a6d54e93b",
    "question": "A healthcare organization is considering Azure Blob Storage for storing patient records that are rarely accessed but need to be retained for compliance reasons. They are particularly interested in minimizing storage costs. If they need to access a file stored in Azure Blob Archive storage, which of the following are a valid first step they can take?",
    "answer": "Archive blobs cannot be accessed directly. To read the data of an archived blob, you must first either:\n\n- Change its tier to Hot or Cool (this process is known as rehydration).\n- Copy the blob to a Hot or Cool tier to access its data without affecting the original blob in Archive storage.",
    "options": [
      "Reconfigure the storage account",
      "Change the tier of the blob",
      "Rotate the storage account keys",
      "Copy the blob to another tier",
      "Change the access permissions",
      "Change the account kind"
    ],
    "answerIndexes": [1, 3],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "a4f98fe929e4811e536e1c4a9504a06a8396f0eaeb16c11fe860dd6d40db585b",
    "question": "The organization DataGenix has developed a machine-learning model for the predictive maintenance of industrial machinery. The model runs periodically and stores the predictive results in Blob storage. Additionally, it stores sensor data from the machinery in a separate container. The predictive results are frequently accessed for immediate action and need to be available within minutes. The sensor data is primarily used for compliance and can be accessed less frequently. What would be the optimal access tier for storing Predictive Results and Sensor Data?\n\nOptions:",
    "answer": "\n\n- **Predictive Results**: Both Hot and Cool tiers are instantly available (within minutes), but the Hot tier is optimized for frequently accessed data. Anything needing less than an hour that doesn’t need frequent access could be put in Cool.\n- **Sensor Data**: Both Cool and Archive are suitable for less frequently accessed data. Since the question mentions no requirement for instant access and asks for optimal (cost-effective), Archive tier is more suitable.",
    "options": [
      "Predictive Results – Archive access tier, Sensor Data – Cool access tier",
      "Predictive Results – Hot access tier, Sensor Data – Archive access tier",
      "Predictive Results – Cool access tier, Sensor Data – Archive access tier",
      "Predictive Results – Hot access tier, Sensor Data – Cool access tier"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "064634b9a6f1097b0e92764ea4b010e1a0367e04d50d3718621c768022f8f9e9",
    "question": "A video streaming company stores large media files, typically 8-9 GB in size, in Azure Blob Storage. These files are infrequently accessed but are preferably needed within an hour when requested for editing. What would be the most cost-effective Access tier for storing these media files, considering the one-hour access time is a preference but not a strict requirement?",
    "answer": "Files up to 10 GB can be rehydrated within an hour on High Priority, but it's not guaranteed.",
    "options": ["Hot access tier", "Cool access tier", "Cold access tier", "Archive access tier"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "9b4f39003ed1c09f5b2c484bd1f1fcd107671f7f0b49ff2521c53b80b89b58cc",
    "question": "Which of the following methods can be used to move blobs from one container to another?",
    "answer": "You can use any of these",
    "options": ["Powershell", "AzCopy", "AZ CLI", ".Net SDK", "Azure Portal"],
    "answerIndexes": [0, 1, 2, 3, 4],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "7109611052ef7ae71f752548c074c0a75d5b201a1c66b1611a35dd0cd8c7a433",
    "question": "You need to retrieve and update the metadata of blobs in an Azure storage account using a .Net library. Which functions would you use:",
    "answer": "Beautiful! [BlobClient](https://learn.microsoft.com/en-us/dotnet/api/azure.storage.blobs.blobclient?view=azure-dotnet) doesn't support `GetMetadataAsync`, but it has `SetMetadataAsync`.",
    "options": [
      "`GetMetadataAsync` and `SetMetadataAsync`",
      "`GetPropertiesAsync` and `SetMetadataAsync`",
      "`GetMetadataAsync` and `SetPropertiesAsync`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "2c4dfa6887fdf327e5fd91ef79b6a95830f92f273cd095eb03650e01bcfadff5",
    "question": "An intern inadvertently misconfigures an application, bypassing the necessary safety checks, which leads to the deletion of a crucial file. Soft delete is enabled on the Azure storage account where this file is stored. Before the incident, two snapshots — Snapshot A and Snapshot B — had been created for the file. Snapshot A was deleted for optimization before the mishap occurred. As a result of the intern's error, the crucial blob and all its remaining snapshots are deleted. Is it possible to restore Snapshot B?",
    "answer": "Azcopy supports both SAS and OAuth authentication for data transfer between two Azure Blobs.",
    "options": ["Yes", "No", "OAuth", "SAS", "Operation is not permitted"],
    "answerIndexes": [0, 2, 3],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "5abe83b06b310ae59f8aac2b60bc0806182902c0556ca91fdd69c702ff8f951c",
    "question": "Which of the following accounts are eligable for using ZRS?",
    "answer": "Only General purpose v2 accounts are eligable for using ZRS",
    "options": [
      "General purpose v2 account at Standard performace tier",
      "General purpose v2 account at Premium performace tier",
      "General purpose v1 account at Standard performace tier",
      "General purpose v1 account at Premium performace tier",
      "Blob storage account at Standard performace tier",
      "Blob storage account at Premium performace tier"
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "23bc338f944b4d4d9e930499e4d3d03454ed2f80b0e0ffde70540c96ead09fba",
    "question": "Which of the following accounts are eligable for using GRS?",
    "answer": "Only General purpose v2 accounts at standard tier are eligable for using GRS",
    "options": [
      "General purpose v2 account at Standard performace tier",
      "General purpose v2 account at Premium performace tier",
      "General purpose v1 account at Standard performace tier",
      "General purpose v1 account at Premium performace tier",
      "Blob storage account at Standard performace tier",
      "Blob storage account at Premium performace tier"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Blob Storage"
  },
  {
    "id": "55276f8c8cbc01eb382814596f0462001bae7f39fa0f0b017064a2a33d70fad9",
    "question": "A startup is developing a complex application with multiple microservices. They want to focus on coding without worrying about the underlying infrastructure. They also need the ability to scale based on traffic and handle background tasks. Which Azure service would be the most suitable for this scenario?",
    "answer": "Azure Container Apps is designed to handle microservices architecture, providing a serverless experience and enabling scaling based on traffic. It's suitable for startups looking to build complex applications without managing the underlying infrastructure.",
    "options": [
      "Azure App Service",
      "Azure Container Instances",
      "Azure Kubernetes Service",
      "Azure Functions",
      "Azure Container Apps"
    ],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "d1a5e0b2b3725e3027739b6a637ff91daa03e5412f0dd2b3a1db131432f35f16",
    "question": "You need to deploy a containerized application to Azure and require autoscaling based on custom metrics. Which Azure service would you use?",
    "answer": "Both Azure App Service and Azure Container Apps support autoscaling and allows you to scale based on custom metrics. ACI doesn't offer autoscaling, and Azure Functions and Logic Apps are not primarily container services.",
    "options": [
      "Azure Container Instances (ACI)",
      "Azure App Service",
      "Azure Container Apps",
      "Azure Functions",
      "Azure Logic Apps"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "d8e21715d1eb257b903662c27a231a7c09ec9cb2bfbcb480ca44c91121afad25",
    "question": "An e-commerce company is looking to host its web platform, which includes a customer-facing website and internal web APIs. They want a service that integrates well with other Azure services and is tailored for web applications. Which Azure service would you choose?",
    "answer": "Azure App Service is optimized for hosting web applications, including websites and web APIs. Its integration with other Azure services makes it a suitable choice for an e-commerce platform.",
    "options": [
      "Azure App Service",
      "Azure Container Instances",
      "Azure Kubernetes Service",
      "Azure Functions",
      "Azure Container Apps"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "9242b88b2bf8341419383d86432005af3a3df0cdb7d286d09235a206504e07e3",
    "question": "A research team needs to run customized containers for a short-term project. They require a basic and flexible solution without the need for load balancing or automatic scaling. Which Azure service would be the most suitable for this scenario?",
    "answer": "Azure Container Instances provides a straightforward way to run customized containers without additional features like load balancing or automatic scaling. It's ideal for a research project that requires flexibility and simplicity.",
    "options": [
      "Azure App Service",
      "Azure Container Apps",
      "Azure Kubernetes Service",
      "Azure Functions",
      "Azure Container Instances"
    ],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "a277db6a8077ca5222fdf2e99beead43f59c3ac163f8bd141402b6f9829b464e",
    "question": "A large enterprise is migrating its applications to containers and requires a robust solution that allows full control over the Kubernetes cluster, including direct access to the Kubernetes API. Which Azure service would you choose?",
    "answer": "Azure Kubernetes Service (AKS) offers a fully managed Kubernetes experience with direct access to the Kubernetes API. It's suitable for large enterprises that need robust container orchestration and full control over the cluster.",
    "options": [
      "Azure App Service",
      "Azure Container Instances",
      "Azure Kubernetes Service",
      "Azure Functions",
      "Azure Container Apps"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "fa5446a59d50d81926c72d32153e789012571c2dd43a92a07c393c822d8d3032",
    "question": "An IoT company is building a data processing system that triggers specific functions based on incoming events from various sensors. They need a solution that can efficiently handle event-driven architecture and execute functions in response to specific triggers. Which Azure service would be the most suitable for this scenario?",
    "answer": "Azure Functions is designed to handle event-driven applications, allowing functions to be triggered by specific events. It's an ideal choice for an IoT company that needs to process data based on incoming sensor events.",
    "options": [
      "Azure App Service",
      "Azure Container Instances",
      "Azure Kubernetes Service",
      "Azure Functions",
      "Azure Container Apps"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Compute Solutions"
  },
  {
    "id": "7c25b6f97114d28bf3af89f33897bb8483fc61e20352d282447517ff1fff18a6",
    "question": "Which of the following methods is recommended when deploying a multi-container group that includes only containers?",
    "answer": "Due to the YAML format's more concise nature, a YAML file is recommended when your deployment includes only container instances.  \n`az container creates` isn't specific to this scenario.",
    "options": ["Azure Resource Management template", "YAML file", "`az container create` command"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "1e8e02135a0e17de184b661e4afc8ef383bfe25dd697a44fcd6581f29f518b57",
    "question": "What is the purpose of a restart policy in Azure Container Instances?",
    "answer": "The restart policy allows you to specify when and how containers should be restarted, based on the desired behavior. This can help optimize resource usage and ensure that tasks are completed successfully.",
    "options": [
      "To charge customers more for compute resources used while the container is running.",
      "To ensure that containers are never restarted, even if the process fails.",
      "To specify when and how containers should be restarted, based on the desired behavior."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2d470bcd30ef94d622b6ab47ef65c285dbdef09462bc1a9c22cef48aa8197dc4",
    "question": "If you want to mount multiple volumes, what options are at your disposal for deployment?",
    "answer": "To mount multiple volumes in a container instance, you must deploy using an Azure Resource Manager template or a YAML file.  \nPowerShell alone doesn't directly support volume mounting.",
    "options": [
      "YAML file only",
      "Azure Resource Manager template and YAML file",
      "Azure Resource Manager template and PowerShell"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "173503852b1171243b1bbc28b72271d33d9a15b5220d7fe3f8f0a5d74df8b0bc",
    "question": "Which of the following methods is recommended when deploying a multi-container group that includes containers and additional Azure service resources (for example, an Azure Files share)?",
    "answer": "Container Groups share a lifecycle, resources, local network, and storage volumes.",
    "options": [
      "Azure Resource Management template",
      "YAML file",
      "`az container create` command",
      "Deployment manifest (YAML)",
      "Container group",
      "Virtual network integration",
      "Runtime environment variables"
    ],
    "answerIndexes": [0, 4],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "c34defee8bd0dbb0550ed309a7e18f2dd41f2fdcbaf54ac5426694d2ebc30168",
    "question": "Which of the following options is true about the built-in authentication feature in Azure Container Apps?",
    "answer": "Azure Container Apps provides built-in authentication and authorization features to secure your external ingress-enabled container app with minimal or no code.  \nThe built-in authentication feature can be configured to handle authenticated and non-authenticated users.",
    "options": [
      "It can only be configured to restrict access to authenticated users.",
      "It allows for out-of-the-box authentication with federated identity providers.",
      "It requires the use of a specific SDK."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "5489b81b5e6d006350c9d02bb9ae9d673c2bd3386be6d852fa80581fbda879b7",
    "question": "What is a revision in Azure Container Apps?",
    "answer": "A revision is an immutable snapshot of a container app version.  \nThe security container running the authentication and authorization module doesn't run in-process, so no direct integration with a specific language is possible.",
    "options": [
      "A dynamic snapshot of a container app version.",
      "A version of a container app that is actively being used.",
      "An immutable snapshot of a container app version."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "3bece104a12ce7291cb467b72b010c7666afc05acdf9b860f29ef2be8faa22d8",
    "question": "You are tasked with deploying a legacy application written in .NET Framework on Azure. This application's container image is stored in an Azure Container Registry with the address `myAcrRegistry.azurecr.io/myNetApp:latest`.\n\nYou need to ensure that the application is properly isolated and manageable within Azure's infrastructure. To do this, write the necessary PowerShell commands to host this application in a container group named `\"myContainerGroup\"` using Azure Container Instances (ACI).\n\n```ps\n# Code here\n```",
    "answer": "Because the application needs to be isolated, you need to create a new resource group.\n\nBecause .Net Framework is specific to Windows only, you'll need `-OsType \"Windows\"`\n\n```ps\n# Create a new resource group \"myResourceGroup\"\nNew-AzResourceGroup -Name \"myResourceGroup\" -Location \"West US\"\n\n# Create a new container group \"myContainerGroup\"\nNew-AzContainerGroup -ResourceGroupName \"myResourceGroup\" -Name \"myContainerGroup\" -Image \"myAcrRegistry.azurecr.io/myNetApp:latest\" -OsType \"Windows\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "62a84b6b5da6f2effea257fe50e789f5e1844e3671fd01e7fc26448f8695d966",
    "question": "You are planning to deploy a legacy application built using .NET Framework as a containerized solution on Azure. Which operating system type should be specified when creating the Azure Container Group for this application?",
    "answer": "The .NET Framework is Windows-specific, so when creating a container for a .NET Framework application, -OsType \"Windows\" must be used.",
    "options": ["Linux only", "Windows only", "Linux or Windows"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "c78cd2c1d1f37e548787af4e2e9f9fa3be10c8f96fdf589f713acbc64e61f92a",
    "question": "You are planning to containerize a .NET Core application for deployment on Azure. When creating the Azure Container Group to host this application, which operating system types are viable options?",
    "answer": "As .NET Core is a cross-platform framework, it is capable of running on multiple operating systems, including both Linux and Windows. Therefore, both of these options could be potentially specified when creating the Azure Container Group.",
    "options": ["Linux only", "Windows only", "Linux or Windows"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "f8d5632d78394b49497f7e60a9fed7559f214487c9357e6db39d22f54ef33815",
    "question": "You are tasked with deploying a .NET Core application on Azure, and you plan to use Azure Container Instances for this purpose. Which PowerShell command should you use to create the necessary resources for hosting this application?",
    "answer": "Azure Container Instances (ACI) are created and managed using the `New-AzContainerGroup` cmdlet. The `New-AzContainerService` cmdlet is used for creating an Azure Kubernetes Service (AKS), which is a different service from ACI.",
    "options": [
      "`New-AzContainerService`",
      "`New-AzContainerGroup`",
      "First `New-AzContainerGroup`, then `New-AzContainerService`",
      "Either `New-AzContainerGroup` or `New-AzContainerService`",
      "None of the mentioned"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "8c3cb9b3fec7a6bf22976f2d3d47859352c7cdbe8c6e068885ad26df7b65d966",
    "question": "Which command is used for creating a container image?",
    "answer": "`az acr build` is used to create container image  \n`az acr create` is used to create Azure Container registry",
    "options": ["`az acr create`", "`az acr build`", "`az acr build` then `az acr create`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "08fd9437d1c73c0bc5c3a95ab772980e57b1c403352dfa9b88c11c4d1481fa7e",
    "question": "You are working with a resource group, `MultiContainerGroup1`, which contains several services such as Azure Functions, a CosmosDB instance, and multiple container instances. You need to export this resource group for future deployment. Write the Azure CLI command to export the template for this resource group.\n\n```ps\n# Code here\n```",
    "answer": "You should use a Resource Manager (ARM) template because this format allows the inclusion of multiple Azure services along with the container instances.\n\n```ps\naz group export --name MultiContainerGroup1 --output-template-file \"./MultiContainerGroup1.json\"\naz deployment group create --resource-group MultiContainerGroup1 --template-file \"./MultiContainerGroup1.json\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "3e9d3c8429d00bc13318536e6e6b2a7877a12f68a5f34a7e02df42c485d4166d",
    "question": "You have just deployed several Azure resources within the `DemoResourceGroup` resource group and you want to capture the template that Azure Resource Manager used for the deployment for future use. How can you accomplish this efficiently?",
    "answer": "The `az group deployment export` command is used to export the template that was used for a specific deployment. This command allows you to capture the exact template used by Azure Resource Manager for that particular deployment.  \nUsing the Azure portal to manually copy the JSON of the deployment template also is valid way to do it, but is a manual process and inefficient.  \n`az group export --name DemoResourceGroup` and `Export-AzResourceGroup -Name DemoResourceGroup` export the resource group (which may include many deployments)  \nis a manual process that doesn't provide the convenience or automation of a CLI command",
    "options": [
      "`az group export --name DemoResourceGroup`",
      "`az group deployment export --name DemoResourceGroup --deployment-name Deployment1`",
      "Use the Azure portal to manually inspect and copy the JSON of the deployment template.",
      "`Export-AzResourceGroup -Name DemoResourceGroup`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2acd7a5ca1b387d5284c4be197a88892e1ba91ab7fa37dfe1f5df0dab69e0e28",
    "question": "What is needed to enable authentication on your Azure Container App?",
    "answer": "Auth works only with HTTPS, requires any identity provider and specified provider within app settings. Authough Entra ID is a valid option, it's incorrect to state it's required.",
    "options": [
      "A configured ingress rule with `allowInsecure` set to disabled",
      "Any Identity provider",
      "A specified Authentication / Authorization provider within the app settings",
      "A secret key to be embedded in the app's code",
      "A Premium Azure service tier subscription",
      "An Azure Container App certificate issued by Microsoft",
      "Microsoft Entra ID is required as an Identity provider"
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "70b29173c2aada27e8b9d769eb6ec3d2435ae12ced45175b4a5c3f18a066004f",
    "question": "You want to store and manage private Docker images that your application will use. Which Azure CLI command would be most appropriate to achieve this?",
    "answer": "`az acr create` - The Azure Container Registry (ACR) service stores and manages private Docker container images. Using the `az acr create` command creates a new ACR, allowing you to handle and manage your Docker images.",
    "options": ["`az containerapp create`", "`az acr create`", "`az container create`", "`az containerapp up`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "099f3eabe79aca49466f3c2adcef35d2573bbbfa12d8f884aa46d7fc914c0714",
    "question": "You have a single-container application that doesn't require advanced orchestration features like scaling or networking with other containers. Which Azure CLI command would be the most suitable for this purpose?",
    "answer": "`az container create` - Azure Container Instances (ACI) is a service that allows you to run containers directly without the need for any orchestration service. The `az container create` command is used to create these instances, which are ideal for single, isolated workloads.",
    "options": [
      "`az acr create`",
      "`az container create`",
      "`az containerapp create`",
      "`az acr build`",
      "`az containerapp up`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "02b1c073a298807cf8a00115ca194ea6b59efe39e7e228ad910da188cfa78523",
    "question": "Your team has developed a new microservices-based application, and you need to deploy these services on Azure. Which command allows you to deploy these applications with scaling and orchestration features?",
    "answer": "`az containerapp create` - Azure Container Apps is a serverless container service that provides advanced features such as scaling and orchestration. The `az containerapp create` command is used to create a new Azure Container App, which is ideal for deploying microservices.",
    "options": ["`az container create`", "`az acr create`", "`az acr build`", "`az containerapp create`"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "3c757490c3933beb8e01a6748567d1d8b5b24dcbc1200d8f76ed0171a792e48f",
    "question": "In the context of Dapr, what is the purpose of a Dapr sidecar and how does it interact with a container app?",
    "answer": "A Dapr sidecar exposes Dapr's APIs to your application, enabling features like service invocation and state management over HTTP or gRPC.  \nWhile Dapr does provide state management capabilities, it's not the sidecar's responsibility to store the state of the container app and periodically sync it.",
    "options": [
      "The Dapr sidecar is used to manage the lifecycle of the container app and has no direct interaction with the app itself.",
      "The Dapr sidecar is used to expose Dapr APIs to the container app, which can be invoked via HTTP or gRPC.",
      "The Dapr sidecar is used to provide a user interface for managing the container app and can be accessed via a web browser.",
      "The Dapr sidecar is used to store the state of the container app and periodically syncs this state with the app."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "340ebdce9e53ac6d2dc9a330b9be74937ae48e92905d3b3a0963425ac2da1b4a",
    "question": "What is the default behavior of Dapr-enabled container apps regarding the loading of Dapr components?",
    "answer": "They load the full set of deployed components.",
    "options": [
      "They load no components by default.",
      "They load only the components specified in the application's configuration.",
      "They load the full set of deployed components.",
      "They load components based on the runtime context."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "1a80e9f465479fe5ab91725e861dceae4e00c9672a766d7ca96d0d5837e0ef2e",
    "question": "What is the primary function of the \"Observability\" feature in Dapr?",
    "answer": "It sends tracing information to an Application Insights backend.",
    "options": [
      "It provides a user interface for monitoring the state of your application.",
      "It sends tracing information to an Application Insights backend.",
      "It allows you to observe the behavior of other services in your application.",
      "It provides a dashboard for visualizing the performance of your application."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2e58486203684170a7d2ac3168e65f3839e7de9d48d7fb71fe6e2d33c6eb5504",
    "question": "What is the recommended solution if you need a stable public IP address for your container group, especially considering potential container group restarts?",
    "answer": "To address the potential IP changes when a container group restarts, it's advisable to use Application Gateway. Application Gateway provides a stable public IP address that remains consistent even if the container group's IP changes due to restarts or other factors.",
    "options": [
      "Use a hardcoded IP address in your container group configuration.",
      "Configure a different subnet for your container group.",
      "Utilize Azure Load Balancer to manage IP address changes.",
      "Use Application Gateway to ensure a static public IP address."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "837d08028e91beed76cd3c1a3ab59573c8957172cd8a5c61cbd7a2e2833f429f",
    "question": "If a container group restarts, what will happen to its IP address?",
    "answer": "When a container group restarts, there's a possibility that its IP address might change. This uncertainty is due to the dynamic nature of container group deployments. It's important not to rely on hardcoded IP addresses in such scenarios.",
    "options": [
      "The IP address will always remain the same.",
      "The IP address will change to a different subnet.",
      "The IP address might change.",
      "The IP address will change only if a new image is deployed."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "da09ab0ebae783e205ef9a23fb754421ccf3656f836807ea30f1b4801a2f1398",
    "question": "What happens when you update an application secret in Azure Container App?",
    "answer": "Adding, removing, or changing secrets doesn't generate new revisions. Apps need to be restarted to reflect updates.",
    "options": [
      "A new revision is created",
      "The application restarts to reflect the updated value",
      "Nothing happens"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "378cb5b245eb4c6fa2d05375568e96616e63765291e7cae999284b1de3d83125",
    "question": "You have deployed a container using the following YAML configuration:\n\n```yaml\napiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n    - name: mycontainer\n      properties:\n        environmentVariables:\n          - name: 'EXPOSED'\n            value: 'my-exposed-value'\n          - name: 'SECRET'\n            secureValue: 'my-secret-value'\n  osType: Linux\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups\n```\n\nYou want to retrieve the value of the environment variable \"SECRET\" using the Azure CLI and execute the following command:\n\n```bash\naz container show --resource-group myResourceGroup --name securetest --query \"properties.containers[0].properties.environmentVariables[?name=='SECRET']\"\n```\n\nWhat should you expect from this command?",
    "answer": "The given YAML configuration demonstrates how to set a secure environment variable named \"SECRET\" with the `secureValue` property. When viewing container properties through the Azure portal or Azure CLI, only the secure variable's name is displayed, not its value. Therefore, executing the given command will show the name of the secure variable \"SECRET\", but not its actual value.",
    "options": [
      "The value \"my-secret-value\" will be displayed.",
      "An error will be thrown since the secret value cannot be accessed.",
      "Only the variable's name \"SECRET\" will be displayed, not its value.",
      "The entire container's properties will be displayed."
    ],
    "answerIndexes": [2],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "1033e1fb2b0b9e83d0333ee8e35f43e57b779ef0f3e8e8eff9862c6deaf6ea99",
    "question": "You have deployed a container using the following YAML configuration:\n\n```yaml\napiVersion: 2018-10-01\nlocation: eastus\nname: securetest\nproperties:\n  containers:\n    - name: mycontainer\n    properties:\n      image: mcr.microsoft.com/azuredocs/hello-world\n      ports:\n      - port: 80\n      resources:\n        requests:\n          cpu: 1.0\n          memoryInGB: 1.5\n  osType: ¯\\_(ツ)_/¯\n  restartPolicy: Always\ntags: null\ntype: Microsoft.ContainerInstance/containerGroups\n```\n\nWhat should you use for `osType`?",
    "answer": "Since this is a single container instance, both Windows and Linux would work. Two or more is for multi-containers, thus Linux only.",
    "options": ["AMD64", "Linux", "Windows", "Both Linux and Windows will work", "Neither option will work"],
    "answerIndexes": [3],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "c322fe2a1605fdceb3e121f0738705dc1dac54af1047b62f3bb539981d743796",
    "question": "You have declared a connection string to a queue storage account in the `--secrets` parameter of a container app. Now you need to reference this secret in an environment variable when creating a new revision in your container app.\n\nWhich of the following commands correctly references the secret `queue-connection-string` in an environment variable in the Azure CLI?",
    "answer": "The correct way to reference a secret in an environment variable in the Azure CLI is to set its value to `secretref:`, followed by the name of the secret.",
    "options": [
      "`--env-vars \"ConnectionString=secretref:queue-connection-string\"`",
      "`--env-vars \"ConnectionString=queue-connection-string\"`",
      "`--env-vars \"ConnectionString=$CONNECTION_STRING\"`",
      "`--env-vars \"ConnectionString=$queue-connection-string\"`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "b2f3dd42dd5b55bcd9a203d2fe85b3a4138e64dba47e8ca82e3202f06a1bd3b2",
    "question": "You are using Azure Container Registry with the Standard service tier to manage images for a rapidly growing project. Recently, your team has noticed inconsistencies in overall performance during deployment workflows. Some members have reported delays when accessing certain images, while others claim operations occasionally fail under high load.\n\nWhich of the following actions could most effectively address the performance issues without incurring additional costs?",
    "answer": "Performance degradation is likely due to the overhead of managing excessive metadata (unused repositories and tags) in the registry. Cleaning up these resources reduces lookup times and improves efficiency without incurring additional costs.\n\nWhile upgrading to Premium would resolve performance issues by increasing throughput and capacity, the scenario specifies budget constraints, making this option impractical.",
    "options": [
      "Upgrade to a more powerful local machine.",
      "Upgrade to the Premium plan.",
      "Delete unused repositories and tags.",
      "Increase the bandwidth of your internet connection.",
      "Call Azure support."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "e0d5221ec36020c2d1d46b326ef2f18549c69f822d8ee12f834f09aaee90eff6",
    "question": "Which container registry tier has the highest throughput?",
    "answer": "Premium has the highest throughput.",
    "options": ["Basic", "Standard", "Premium"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "4684c68b627ec73be3da08e2531bc42c74eb916a743120f3efaa43bac75d59b0",
    "question": "Given a Dockerfile in your current directory with the following content:\n\n```Dockerfile\nFROM mcr.microsoft.com/hello-world\n```\n\nYou are asked to run an image in Azure from a Container Registry named `myContainerRegistry008`. The image should be tagged as `sample/hello-world:v1`.\n\nAssume that the Azure subscription and Azure CLI have already been configured on your local system. However, the necessary resources for running the image, such as the resource group and container registry, have not yet been created.\n\nConsidering all these requirements, write down the sequence of Azure CLI commands necessary to successfully run the image from the specified container registry.",
    "answer": "\n\n```Dockerfile\n# Create a resource group named 'myResourceGroup' in 'eastus' location\naz group create --name myResourceGroup --location eastus\n\n# Create an Azure container registry named 'myContainerRegistry008' within the 'myResourceGroup'\naz acr create --resource-group myResourceGroup --name myContainerRegistry008 --sku Basic\n\n# Authenticate Docker client to the registry\naz acr login --name myContainerRegistry008\n\n# Build the Docker image from the Dockerfile in the current directory, tag it as 'sample/hello-world:v1',\n# and push it to the 'myContainerRegistry008' registry\naz acr build --registry myContainerRegistry008 --image sample/hello-world:v1 .\n\n# Execute the image from the registry\naz acr run --registry myContainerRegistry008 --cmd '$Registry/sample/hello-world:v1' /dev/null\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "1d67e2d148ba09a1529da6162a9905b99099f507809d5cc0706a37330f44d1aa",
    "question": "You are managing an Azure Container Registry named `myregistry`. You have a task to publish the most recent `windows/servercore` container image from the Microsoft Container Registry into your registry. After importing, you want the image to be tagged as `servercore:ltsc2019` in your registry. Write the Azure CLI command that would be needed to accomplish this.\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz acr import \\\n--name myregistry \\ # specifies the name of your Azure Container Registry\n--source mcr.microsoft.com/windows/servercore:latest \\ # the fully qualified source image reference\n--image servercore:ltsc2019 # the new tag you want the image to have in your registry\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "ccd79c072fab64bef04ea9f3c7271a06875d96da19cfdc9f28f082b99d786916",
    "question": "Which of the following Azure Container Registry options support geo-replication to manage a single registry across multiple regions?",
    "answer": "The premium tier adds geo-replication as a feature.",
    "options": ["Basic", "Standard", "Premium"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "16cb60a3c62b6ae6197ffeba96605fa05ed86e363ee4a61d270a24e55eb8050a",
    "question": "Which Azure container registry tiers benefit from encryption-at-rest?",
    "answer": "Encryption-at-rest is supported in all three tiers.",
    "options": ["Basic", "Standard", "Premium"],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "ef5429131cd6fae37ae0148dc4eb4c966acfbb522e17bfa862111a09b1afa3b4",
    "question": "You are working with an Azure Container Registry and you exceed the request limits for your service tier. What is the expected behavior when this happens, and what strategies should you implement to mitigate the impact on container operations?",
    "answer": "When you exceed the request limits in Azure Container Registry, [throttling](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#throttling) occurs, resulting in an `HTTP 429 error`. The recommended solution is implementing retry logic with exponential backoff to handle the temporary throttling and resume operations smoothly.\n\nUpgrading the tier isn’t necessary to resolve throttling.",
    "options": [
      "The registry will throttle all pull and push operations, resulting in an `HTTP 429 error`, and you will need to upgrade your service tier to continue operations.",
      "The registry will begin returning `HTTP 503 errors`, and you must reconfigure your container deployment pipeline to reduce the frequency of image pulls and pushes.",
      "The registry will temporarily throttle pull and push operations, returning an `HTTP 429 error`, and you should implement retry logic with exponential backoff to mitigate the issue.",
      "The registry will continue to operate normally, but container operations will slow down as a result of throttling, requiring no further intervention."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2d99362244a07bfc1c3d2e9a6403d6c5765e4840e497c9ec40d90e208c1425d5",
    "question": "If you're developing a Linux-based ASP.NET Core application that is planned to be deployed via Azure Container Instances, and you need to launch it in a geographic region where your company doesn't have an existing resource group, what sequence of Azure CLI commands would you utilize to correctly deploy your application in that target region?\n\n```ps\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\ncontainerGroup=\"WEuropeGroup\"\ncontainerName=\"AspContainer\"\ncontainerImage=\"mcr.microsoft.com/azuredocs/aci-helloworld\"\n\n# Code here\n```",
    "answer": "\n\n```ps\nlocation=\"West Europe\"\nresourceGroup=\"WEurope\"\ncontainerGroup=\"WEuropeGroup\"\ncontainerName=\"AspContainer\"\ncontainerImage=\"mcr.microsoft.com/azuredocs/aci-helloworld\"\n\n# Create a new resource group in the West Europe region\naz group create --name $resourceGroup --location \"$location\"\n\n# Create a container group (ACI) in the new resource group\naz container create --name $containerName --resource-group $resourceGroup --image $containerImage --dns-name-label $containerGroup --location $location\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "213aed26647571222127a8f743585c1f94974771e422d3af687b15d1e1445b30",
    "question": "You have an ASP.NET Core application running within an Azure Container Instance (ACI) and your monitoring team has a unique container image loaded with their monitoring tools. To ensure compliance, you have been tasked with attaching a \"sidecar\" container (an auxiliary container that works alongside the main application container) from the same host. However, you must take into account that the solution should be cost-effective and require minimal changes to the current application, keeping the setup simple. What Azure service would you employ to realize this objective?",
    "answer": "The most suitable service to accomplish this is Azure Container Instances (ACI) Container Groups. This service allows you to run multiple containers, maintained by different teams, deployed together as a group on the same host. Each deployed container instance will share the resources of the host and are able to communicate which each other (in this case: your application and the monitoring sidecar). This offers an economical solution without the need for significant changes to your existing application, and it aligns with the sidecar container model.  \nAzure Kubernetes Service (AKS) allows running multiple containers on the same host, which supports the sidecar pattern. However, it introduces additional complexity and potential cost increases due to the need for managing clusters and implementing scaling features.  \nAzure App Services provides a platform for hosting web applications and RESTful APIs, including those in containers. The downside is that it does not natively support the sidecar pattern and can lead to higher costs as the scale of operations increases.  \nAzure Container Registry serves as a storage and management solution for container images. It is primarily a storage service and does not provide the functionalities to deploy containers or implement the sidecar pattern.  \nAzure Service Fabric is a platform that provides orchestration of microservices and containers, and could technically support the sidecar pattern. However, it requires a deeper understanding of the microservices architecture and might not be the most cost-effective or simple solution for running a single application with a monitoring sidecar.",
    "options": [
      "ACI Container Groups",
      "Azure Kubernetes Service (AKS)",
      "Azure App Services",
      "Azure Container Registry",
      "Azure Service Fabric"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "c62fcf88ebefcd7894fd1f11576ef5b3dd2199cfbdd1b990924a0adfc279458f",
    "question": "As a developer in a startup, you're helping your team transition to Microsoft Azure. Your task is to deploy a containerized API service, `MyAPI`, on Azure from an older Linux workstation. The source code for `MyAPI` is stored locally in the `./src` directory and is also tracked on the GitHub repository `myuser/myrepo`.\n\nBefore deploying, you'll need to create a production environment named `prod`. The Azure CLI on your workstation should be up-to-date, but given its age and the type of service you're deploying, you're unsure if all necessary tools and extensions are available.\n\n`MyAPI` utilizes services that allow for hosting of RESTful APIs and collection and analysis of telemetry data. So, ensure to configure your Azure account accordingly before deployment. Once your local setup is prepared, deploy `MyAPI` to a resource group named `MyResourceGroup` in the `eastus` region using the `prod` environment.\n\nCan you draft an Azure CLI script to achieve these tasks?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Check the current Azure CLI version on the old Linux workstation, and upgrade if needed\naz upgrade\n\n# The nature of the application (a containerized service) hints at the need for the containerapp extension. So, add and upgrade it.\naz extension add --name containerapp --upgrade\n\n# It's time to connect to Azure once all the local tasks are completed.\n# However, you could technically log in at any time before this.\naz login\n\n# Register the necessary providers as the application uses services for hosting APIs and telemetry analysis\naz provider register --namespace Microsoft.App\naz provider register --namespace Microsoft.OperationalInsights\n\n# Create the 'prod' environment\naz containerapp env create --resource-group MyResourceGroup --name prod\n\n# Deploy the API service\naz containerapp up \\\n  --name MyAPI \\\n  --resource-group MyResourceGroup \\\n  --location eastus \\\n  --environment prod \\\n  --context-path ./src \\\n  --repo myuser/myrepo\n```\n\nIn the context of deploying containerized applications on Azure, the command `az extension add --name containerapp --upgrade` is essential to interact with Azure Container Apps service. The `--upgrade` flag is used to ensure that you have the latest version of the extension. This is especially critical when your workstation is older, and there's uncertainty regarding the presence and version of the necessary tools and extensions.\n\nThe `az login` command logs you into your Azure account. Although we performed all the local tasks like upgrading the Azure CLI and adding the necessary extension before logging into Azure, technically you could log into Azure at any time. The reason why we log in after performing local tasks is just to make sure that we've done everything we can do locally before initiating a connection to Azure. This could help avoid unnecessary delays or connection timeouts, especially if you're on a slow or unreliable network.\n\n- `Microsoft.App`: This namespace is typically used when your application leverages Azure App Services. Azure App Services provide a platform to host web apps, mobile app back ends, RESTful APIs, or automated business processes. If your application doesn't use any of the services provided by Azure App Service, you may not need to register this provider.\n\n- `Microsoft.OperationalInsights`: This namespace is associated with Azure Log Analytics. If your application uses Azure Monitor Log Analytics to collect and analyze telemetry and other data, you need to register this provider. Log Analytics can provide insights about your applications, infrastructure, and network. If you don't use these services, you might not need this provider.\n\n  Note: Explicit provider registration is not typically necessary for Azure services. It's often handled automatically when you create a resource that belongs to a provider, although there can be exceptions.\n\nThe command `az containerapp env create` is used to create an environment in Azure Container Apps. This command creates an environment under a specific resource group with a given name. An environment is a space where you can deploy container apps. You can have different environments for different stages of your app like development, staging, and production. Each environment can have its own configuration like compute resources, networking settings, etc. For your use case, we've assumed that the prod environment has been already configured as per your production specifications.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "8bfa043a5ec322d1f1586c7e36a7f16fb935617c84959826700820478821f01b",
    "question": "As a DevOps engineer at ABC Industries, you are in charge of deploying a Node.js web service that communicates with a MongoDB database to Microsoft Azure. This service relies on environment variables `DB_URL` and `SECRET_TOKEN` for database connection and secure interactions, respectively. The service's Docker image is hosted on Docker Hub under `abcindustries/ai-service-app`.\n\nFor deployment, an Azure Container App should be created under the name `ai-service-app`, located in the `westus2` region within the `abc-industries` resource group. The app will run on port 8000 and needs DB_URL and SECRET_TOKEN set to `mongodb://username:password@dbhost:27017/dbname` and `sometoken` respectively. The container instance should have suitable CPU and memory specifications.\n\nConsidering these requirements, how would you employ Azure CLI to set up this Azure Container App?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Log into the Azure account\naz login\n\n# Upgrade the Azure CLI to the latest version\naz upgrade\n\n# Add and upgrade the containerapp extension\naz extension add --name containerapp --upgrade\n\n# Register the necessary providers as the microservice uses services for hosting APIs\naz provider register --namespace Microsoft.App\n\n# Create the 'dev' environment\naz containerapp env create --resource-group MyResourceGroup --name dev\n\n# Create the container application\naz containerapp create \\\n  --name ai-service-app \\\n  --resource-group abc-industries \\\n  --environment-variables DB_URL=mongodb://username:password@dbhost:27017/dbname SECRET_TOKEN=sometoken \\\n  --docker-image abcindustries/ai-service-app \\\n  --region westus2 \\\n  --target-port 8000 \\\n  --cpu <CPU_CORES> \\\n  --memory <MEMORY_GB>\n```\n\n`az containerapp create` gives you more control over the configuration and is suitable for setting up a new application or when making changes to an existing application in a production environment.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "dbb71d64a1ec495eec7e3904166f6ba42b86652304c1a2dd5901e3399ea7d2f6",
    "question": "You are designing a microservices-based application using Azure Container Apps. The architecture will include eight separate container apps. You are evaluating whether to deploy them in a single Container Apps environment or across multiple environments.\n\nWhich two requirements would justify deploying multiple environments?",
    "answer": "\n\n- VNet isolation is bound to the Container Apps Environment. If you need multiple VNets, you need multiple environments.\n- Log Analytics workspace is set at the environment level. Different destinations require separate environments.\n- AAD auth is configurable per app via identity provider config; environments don’t constrain this.\n- ACA supports individual autoscaling configs per app (via KEDA rules), even in the same environment.\n- Container registry configuration is at the app level (you can pull from different registries within one environment).",
    "options": [
      "Each app must authenticate against a different Azure Active Directory tenant.",
      "Each app must scale based on different CPU and memory usage thresholds.",
      "Each app must be isolated to a different virtual network.",
      "Each app must send logs to a separate Log Analytics workspace for auditing purposes.",
      "Each app uses a different container registry for pulling images."
    ],
    "answerIndexes": [2, 3],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "a91c94fe1cb6297a9511a5d6507976bf4fc563f4fa8bf4e65cf924fa4771e33e",
    "question": "What is the recommended strategy in the event of a full region outage?",
    "answer": "All actions are recommended strategies.",
    "options": [
      "Wait for the region to recover and then manually redeploy all environments and apps.",
      "Manually deploy to a new region",
      "Deploy container apps in advance to multiple regions and use Azure Front Door or Azure Traffic Manager to handle incoming requests.",
      "Do nothing and hope for the best."
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "0ac2ebe9a1bd21f7feadbd4620a87ea8ddca55502029c9f4a4b6086f9b971608",
    "question": "What is the requirement for enabling zone redundancy in your Container Apps environment?",
    "answer": "The environment must include a virtual network (VNET) with an available subnet.",
    "options": [
      "The environment must include a virtual network (VNET) with an available subnet.",
      "The environment must have at least 10 replicas.",
      "The environment must be located in a specific region.",
      "The environment must have a specific number of applications running."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "1c95a26953b8d2d91a1a2b49ff6b96030ecf155c541d3cd582aaafb8c26b2b4e",
    "question": "How can you maintain the availability of a crucial website hosted on Azure Container Apps, even if a single Azure datacenter goes down, while keeping the solution simple and using the least number of Azure services?",
    "answer": "The optimal solution is to activate zone redundancy in the Container Apps setting. This distributes Azure Container Apps replicas across multiple availability zones, ensuring the website stays operational even if a datacenter fails. Using multiple regions and Azure Front Door or setting up automatic Azure DevOps deployment pipelines would either involve additional services or cause temporary downtime, making them less ideal.",
    "options": [
      "Activate zone redundancy in the Container Apps setting.",
      "Set up automatic Azure DevOps deployment pipelines to shift to a new region if the primary datacenter fails.",
      "Use multiple regions and route requests via Azure Front Door."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "cdac215cd4e4010b1a4d039c9ccb59748cab517754b0768549d6dcd76151bb59",
    "question": "You want to use managed identities in the scaling rules for your container app. Which ones can you use?",
    "answer": "Using managed identities in scale rules isn't supported.",
    "options": ["System-assigned identities only", "User-assigned identities only", "Both", "None"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "6e5fd1079fd5ed872b5c3ce8ea398f938b7f72c52cc0798992b17fe9c2538f28",
    "question": "What is ACR Tasks?",
    "answer": "ACR Tasks is a suite of features within Azure Container Registry that provides cloud-based container image building and can automate OS and framework patching for Docker containers.",
    "options": [
      "A tool for managing virtual machines in Azure",
      "A suite of features within Azure Container Registry for container image building and patching",
      "A service for managing Kubernetes clusters",
      "A tool for network monitoring in Azure"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "1749d1a3b3e2270f3cd67bddbd019477d9eaa2b5c82aeb0734b18148e62d5578",
    "question": "What is the default platform for building images with ACR Tasks?",
    "answer": "By default, ACR Tasks builds images for the Linux OS and the amd64 architecture.",
    "options": ["Windows/amd64", "Linux/arm64", "Linux/amd64", "Linux/arm"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "319c02a53dcf4f5aacbc6056191ea38d52f7bf0d6e706d36ebbbef6fe57aa54e",
    "question": "The az acr build command in Azure Container Registry is used to build and push a container image to ACR. To which of the following Docker commands is this Azure command equivalent? (Choose two)",
    "answer": "The az acr build command is equivalent to the combination of docker build, which builds the Docker image, and docker push, which pushes the image to a registry.",
    "options": ["docker run", "docker build", "docker push", "docker pull", "docker compose"],
    "answerIndexes": [1, 2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "c6dadd8b00097dee47d0c36471d73e118a4c1f7d93d9bf18512a5cfe59335d08",
    "question": "What is the default restart policy in Azure Containers?",
    "answer": "Always restart",
    "options": ["Always", "On failure", "Never"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "74eae9f5afde9644ebecdd7052464a1c583e0af79919d97da07417aea1ec704c",
    "question": "In Azure Container Instances, which restart policy should you choose if you want the containers in the container group to execute only once and not restart?",
    "answer": "`Never` policy ensures that the containers in the container group will not be restarted. It aligns with the requirement of running the containers at most once.",
    "options": ["Never", "OnFailure", "Always"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "3b448321f00f85b533ff3e121a32acfd92a17906dccd1ee4ead179f34bc07069",
    "question": "Which command will set environment variable `MinLength` to `8`?",
    "answer": "`az container create --environment-variables 'MinLength'='8' 'NumWords'='5' ...`",
    "options": [
      "`az container create --environment-variables 'MinLength'='8'`",
      "`az container create --environment-variables 'MinLength=8'`",
      "`az container create --environment-variables MinLength=8`",
      "`az container create --environment-variables {'MinLength':8}`",
      "`az container create --environment-variable-name 'MinLength' --environment-variable-value 8`"
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "14f46d069ecc938a4547920b6190f1da3745a4e1309c0357d6c14d3195a0d927",
    "question": "You are working on a project that requires deploying a containerized application in Azure. The application has two key requirements: It needs to run a process that requires root access, and it must be hosted on a Windows-based operating system. You are considering Azure Container Apps as a hosting option. Which of the following statements is correct regarding the feasibility of using Azure Container Apps for this project?",
    "answer": "Azure Container Apps can't run privileged containers, and if a process requires root access, it will cause a runtime error. This rules out fulfilling the first requirement. It also only support Linux-based (linux/amd64) container images, which rules out hosting on a Windows-based operating system.",
    "options": [
      "Azure Container Apps can fulfill both requirements.",
      "Azure Container Apps can only fulfill the requirement of running a process that requires root access but not the Windows-based operating system requirement.",
      "Azure Container Apps can only fulfill the Windows-based operating system requirement but not the requirement of running a process that requires root access.",
      "Azure Container Apps cannot fulfill either of the requirements for this project."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "475fa03b293a62e693ab3df879da27b2817131b5088d8f6740c45089059ec3cf",
    "question": "What will happen if you change `template.scale.maxReplicas` from 3 to 5?\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"resources\": [\n    {\n      \"properties\": {\n        \"template\": {\n          \"scale\": {\n            \"minReplicas\": 1,\n            \"maxReplicas\": 3\n          }\n        }\n      }\n    }\n  ]\n}\n```",
    "answer": "Changes made to the `template` section are revision-scope changes, which triggers a new revision. The changes are limited to the revision in which they're deployed, and don't affect other revisions.",
    "options": [
      "All existing revisions will have max 5 replicas now.",
      "A new revision is created. All revisions now have 5 max replicas now.",
      "A new revision is created with 5 max replicas. All existing revisions remain unchanged."
    ],
    "answerIndexes": [2],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "3524e02655911f624c0d4befe108d3b87586e9fd3a927e5c3c31b6b471595cee",
    "question": "What will happen if you change `configuration.ingress.allowInsecure` from `false` to `true`?\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"resources\": [\n    {\n      \"properties\": {\n        \"configuration\": {\n          \"ingress\": {\n            \"external\": true,\n            \"targetPort\": 80,\n            \"allowInsecure\": false\n          }\n        }\n      }\n    }\n  ]\n}\n```",
    "answer": "Changes made to the `configuration` section are application-scope changes, which does not triggers a new revision, but affects all existing revisions.",
    "options": [
      "All existing revisions will now allow insecure traffic.",
      "A new revision is created. All revisions will now allow insecure traffic.",
      "A new revision is created that allows insecure traffic. All existing revisions remain unchanged."
    ],
    "answerIndexes": [0],
    "hasCode": true,
    "topic": "Containers"
  },
  {
    "id": "9777ceeac72f134f57ea7a43a97cf61945417d432d7847cfb7a5a00e3134aba1",
    "question": "What command should you execute to verify if your image has been successfully pushed to Azure Container Registry?",
    "answer": "`az acr repository list` lists the repositories in the specified Azure Container Registry, allowing you to verify if your image is present.  \n`docker images` lists the images available on your local machine, not in the Azure Container Registry.  \n`az acr show` retrieves detailed information about the specified container registry, but does not list the images or repositories.  \n`az acr task list` lists all the tasks for a specified container registry, which are used for automated container image building.",
    "options": ["`az acr repository list`", "`docker images`", "`az acr show`", "`az acr login`", "`az acr task list`"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "6f4b5689cc22edfb6069cdd10f8c545a623b3d472b1f512864c7a1ac99da1d87",
    "question": "Which command is used to deploy an image in Azure Container Instances (ACI)?",
    "answer": "`az container create` is the correct command to deploy a container in Azure Container Instances (ACI)  \n`az container push` is not a valid Azure CLI command.  \n`az container export` exports a container group in yaml format.  \n`docker build` builds a Docker image from a Dockerfile, not related to deploying a container.",
    "options": ["`az container push`", "`az container create`", "`az container export`", "`docker build`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "c3f43ba37950d3244ba2ac786001b29f3e65f8fe42946947ce9dd99fd5ba7e57",
    "question": "You are planning to use Azure Container Registry for your application. Which identity type should you use to ensure both headless authentication and role-based access control (RBAC)?",
    "answer": "Service principals are designed for headless authentication and can be assigned specific Azure roles, making them ideal for both requirements.  \nIndividual Entra ID Identity and Admin User are used for interactive push/pull operations.  \nManaged Identity for Azure Resources: While it supports unattended operations, it's limited to select Azure services and may not offer the full range of RBAC options.",
    "options": [
      "Individual Entra ID Identity",
      "Managed Identity for Azure Resources",
      "Entra ID Service Principal",
      "Admin User"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "54780eef8df0e34c56d2406512dac8b9bb7f5c5fd66d07215638bc3b03cad125",
    "question": "You need to attach the standard output and standard error streams of a running container in Azure to your terminal. Which Azure CLI command should you use?",
    "answer": "`az container attach` is used to attach the standard output and standard error streams of a running container to your terminal.  \n`az container logs` is used to fetch the logs for a container in a container group.",
    "options": ["`az container logs`", "`az container exec`", "`az container attach`", "`az container start`"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "bfd512ec591fa6439807cbd8cf618d3ddfa109b39e59a5748aca4facc04b6867",
    "question": "You need to mount Azure Files in `/aci/logs/`. Under which property in the YAML file `mountPath: /mnt/secrets/` will go?",
    "answer": "`volumesMounts` - Where to mount.",
    "options": ["volumes", "volumesMounts"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2f051e075a8dffe38dd4b783ca3954dcdad95737078333d4c33e2bd4c67085c4",
    "question": "You need to mount Azure Files in `/aci/logs/`. Under which property in the YAML file `azureFile:` will go?",
    "answer": "`volumes` - What to mount.",
    "options": ["volumes", "volumesMounts"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "a1a0612114c2ee1471c7dced2168027e3bf6c41640f8d7a4fc1e38ccb86d2ffa",
    "question": "\n\nYour organization utilizes an Azure container registry. What is the most restrictive role you should assign to developers so they can upload/publish images to the registry without granting excessive permissions?",
    "answer": "The `AcrPush` role allows developers to push images to the Azure container registry while adhering to the principle of least privilege.",
    "options": ["`Owner`", "`Contributor`", "`AcrPush`", "`AcrPull`"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "2696837ed06850ebfc07f7c031c12c4a025cd089cedddb53959140c31b1f81bb",
    "question": "You are using Azure Container Instances (ACI) to run a container that requires access to an Azure File Share. Which of the following is required to mount the Azure File Share to the ACI?",
    "answer": "To mount an Azure File Share to an Azure Container Instance, you need the Storage Account Key. SAS Tokens, OAuth Tokens, and Entra ID Credentials are not used for this specific operation.",
    "options": ["Storage Account Key", "Shared Access Signature (SAS) Token", "OAuth Token", "Entra ID Credentials"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Containers"
  },
  {
    "id": "fc7f7a0d6bc2e4e07df3b689a70fbdcf14b4256fa79b394d86507a9a0819f58c",
    "question": "Which of the following consistency levels offers the greatest throughput?",
    "answer": "The eventual consistency level offers the greatest throughput at the cost of weaker consistency.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "bf3a9458915b9b2132334389ab60fc7b7e1329129fc43de4efc544417b25acbe",
    "question": "What are request units (RUs) in Azure Cosmos DB?",
    "answer": "RUs represent the normalized cost of all database operations in Azure Cosmos DB, including writes, point reads, and queries.",
    "options": [
      "A unit of measurement used to express the cost of all database operations in Azure Cosmos DB.",
      "A unit of time used to measure the duration of database operations.",
      "A unit of storage used to measure the amount of data stored in Azure Cosmos DB."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "a3a4a4b48f8473a873fc06fe8bd344abfb2dca8c347c18593988611f16551862",
    "question": "When defining a stored procedure in the Azure portal input parameters are always sent as what type to the stored procedure?",
    "answer": "When defining a stored procedure in Azure portal, input parameters are always sent as a string to the stored procedure.",
    "options": ["String", "Integer", "Boolean"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "ea301ee58bf7803e7f68bdcddd515b7314fa88584f755776a6fa311fbc2bc83f",
    "question": "Which of the following would one use to validate properties of an item being created?",
    "answer": "Pretriggers can be used to conform data before it's added to the container.  \nPost-triggers run after an item is created.  \nUser-defined functions run on existing data.",
    "options": ["Pretrigger", "Post-trigger", "User-defined function"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "1c587bf05bff788974510b9eed5fcfe946225033fcdbc71d474e2051376c9466",
    "question": "What is the purpose of the context object in a stored procedure in Azure Cosmos DB?",
    "answer": "The context object in a stored procedure provides access to all operations that can be performed in Azure Cosmos DB, and access to the request and response objects.",
    "options": [
      "It provides access to the database schema and metadata.",
      "It allows for the creation of new collections within the database.",
      "It provides access to all operations that can be performed in Azure Cosmos DB, and access to the request and response objects."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "0630920998328518d723d651ffda4938b586f5b835239f3507f4842e191f30e5",
    "question": "What is the role of pretriggers in Azure Cosmos DB?",
    "answer": "Pretriggers in Azure Cosmos DB are executed before modifying a database item and they need to be specified for each database operation where they should execute.",
    "options": [
      "Pretriggers are automatically executed for each database operation.",
      "Pretriggers are executed before modifying a database item and must be specified for each database operation where you want them to execute.",
      "Pretriggers are used to execute operations after modifying a database item."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "b2bee7eb9e6cc9b312c529ef8d276092c905541a955d99504cf896d1876451cf",
    "question": "What is the purpose of the lease container in the Azure Cosmos DB change feed processor?",
    "answer": "The lease container acts as a state storage and coordinates processing the change feed across multiple workers.",
    "options": [
      "It stores the data from which the change feed is generated.",
      "It processes the change feed across multiple workers.",
      "It acts as a state storage and coordinates processing the change feed across multiple workers."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "8fafc546846b2d9045cf908753f314c6dde0d4170740530d1d30460533536b23",
    "question": "Your company is developing a new online multiplayer game. The game includes a feature that shows a global leaderboard to players. The leaderboard doesn't have to be up-to-the-minute for each player, but any lag should be within a defined limit. What consistency level would be most suitable for this scenario?",
    "answer": "The Bounded staleness consistency level allows for some lag in data propagation but within a defined limit.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "e632583c647f21f7e3351faace65c4c1a979bcd7fa767f2fd590d5f8f6218669",
    "question": "What is the most suitable consistency level for a database solution that ensures sequential data writes are returned with no gaps, can process data using either its most recent or previous version, strives to minimize latency and negative impacts on data availability, and also keeps processing overhead as minimal as possible?",
    "answer": "The bounded Staleness consistency level ensures that data writes are returned sequentially (no gaps). You have the flexibility to set up staleness, deciding the version of data to be returned, based on either the count of versions or the time elapsed.  \nAll other options (except Eventual) also enabless gapless reads, but cannot configure staleness.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "5e702c3b76de7a9bb43c375cd735cec1451e43cfa7211d9220c513bea9fd9f5e",
    "question": "Your company operates a financial application which handles sensitive transactions. It's critical that all operations are immediately consistent across all regions, even if it might impact performance. What consistency level would be most suitable for this scenario?",
    "answer": "Strong consistency level ensures immediate consistency across all regions.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "c97d9544857f9ff7731cc8f134e7524659db8562355f90acb19dbdb8b27ff2fb",
    "question": "Your company is creating a shopping platform where users interact with their own shopping carts. A user's actions should be immediately consistent within their session, but it doesn't have to be strongly consistent with other users' sessions. What consistency level would be most suitable for this scenario?",
    "answer": "Session consistency level ensures that within a single session.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "413f0ba0c80968281c56895e71b1d14286a2dafe4b8d2af6d740a3ca0e7b4118",
    "question": "Your company is building a workflow management system where sequences of tasks are processed. It's crucial that these tasks are processed in the order they were written, even if it means reading data a bit behind the latest. What consistency level would be most suitable for this scenario?",
    "answer": "Consistent prefix level ensures that the sequence of operations is preserved.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "100ced3515fd751a5cef80f6f3fe91636911626e90d6896dcc54a9c632c045a9",
    "question": "Your company is creating a social media platform with a feature to 'like' posts. It's more important to have high availability and throughput, and your application can handle some temporary inconsistencies, like the count of likes appearing out of order or slightly stale. What consistency level would be most suitable for this scenario?",
    "answer": "Eventual consistency level prioritizes availability and throughput over immediate consistency.",
    "options": ["Strong", "Session", "Bounded staleness", "Consistent prefix", "Eventual"],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "008af6d967c6ec8111485988db35966a41aa0f7a9fc703e6c295c60e2fce126b",
    "question": "Your company has recently started using Azure Cosmos DB and hasn't made any adjustments to the consistency level. What delay should the company expect in data propagation between different regions?",
    "answer": "Default consistency level is **Session**",
    "options": [
      "No delay, data is immediately consistent across all regions.",
      "Delay is limited to a specific number of operations or time interval.",
      "Delay may occur but is contained within a single user's session.",
      "Delay is possible and data might be read in the order of writes but a bit behind the latest.",
      "Delay and out-of-order reads may occur, but the system will eventually become consistent."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "bef662ba5c95db2b1666c9502ce4ff92cbec458d6738538975f12ac13df76e91",
    "question": "What is the primary reason for identifying access patterns in your solution when using Azure Cosmos DB?",
    "answer": "[Identifying access patterns](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/model-partition-example#identify-the-main-access-patterns) helps design the data model to optimize application interactions, such as minimizing cross-partition queries or reducing request numbers through denormalization.  \nAlthough cost can be influenced by data modeling, the main goal is performance optimization.  \nThe choice of programming language is largely unrelated to data modeling. Access patterns don't dictate language choice.  \nThe number of platform users is a business decision, not directly related to data modeling or access patterns.",
    "options": [
      "To determine the cost of the solution",
      "To ensure efficient data modeling",
      "To choose the right programming language",
      "To decide the number of users for the platform"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "07e2e6abd188dd5f5d9897ce03155a39da78a49fe8c2653859797e3de1cc48b1",
    "question": "What is the purpose of denormalization in the context of Azure Cosmos DB data modeling?",
    "answer": "[Denormalization in Azure Cosmos DB data modeling](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/model-partition-example#v2-introducing-denormalization-to-optimize-read-queries) is used to optimize read queries. By storing related data together (denormalizing), you can retrieve all the data you need in a single query, reducing the need for multiple queries and improving performance.  \nDenormalization doesn't aim to reduce the number of containers, increase the number of request units consumed, or add more users to the platform; it's a strategy to optimize read operations by organizing data efficiently within containers.",
    "options": [
      "To reduce the number of containers",
      "To optimize read queries",
      "To increase the number of request units consumed",
      "To add more users to the platform"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "657dac9763de9afba4870dadb9f9f68f3327e60ef523630be24db8041248c399",
    "question": "In Azure Cosmos DB, when dealing with a schema design that requires optimizing read operations and reducing the number of joins, what approach is commonly used?",
    "answer": "Denormalizing is the process of restructuring a database to reduce redundancy and improve read performance. In Azure Cosmos DB, denormalizing can help in optimizing read operations by reducing the need for complex joins and aggregations. This approach can lead to faster query execution, especially in a distributed database system like Cosmos DB.",
    "options": ["Normalizing", "Sharding", "Denormalizing", "Partitioning"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "08c774e4712070bf926497a8ad4e701e0439799428525d5c85cac992dea059e2",
    "question": "When would you use the manual failover feature in Azure Cosmos DB?",
    "answer": "Manual failover allows you to manually trigger a failover to a backup region, which is useful for testing your application's resilience to failures.",
    "options": [
      "When you want to test your application's resilience",
      "When you want to add a new region to your account",
      "When you want to enable multi-region writes",
      "When you want to create a new Azure Cosmos DB account"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "6282f10bf0b671c80df751dad84317c07112276f15955295a5e7afccc182d014",
    "question": "What is the prerequisite for enabling service-managed failover in Azure Cosmos DB?",
    "answer": "The service-managed failover feature requires your account to have two or more regions, as it needs a backup region to failover to if the primary region becomes unavailable.",
    "options": [
      "Your account must have at least one region",
      "Your account must have two or more regions",
      "Your account must have multi-region writes enabled",
      "Your account must be configured for manual failover"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "dd30848feacd055ac21712f818b16243f4cc2814ec581cf80da0c9591267cdad",
    "question": "What is the role of a partition key in Azure Cosmos DB?",
    "answer": "The partition key helps Azure Cosmos DB distribute the data efficiently across partitions. The value of this property is then used to route data to the appropriate partition to be written, updated, or deleted.",
    "options": [
      "It helps to distribute data efficiently across partitions.",
      "It is used to manage the Azure Cosmos DB account.",
      "It is used to create a unique DNS name for the account.",
      "It is used to group containers into a database."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "5cf392ea48b02b532bf5d6c341826115506aeae1beab254c7bc101a77c0dbc5e",
    "question": "What is the main characteristic of Azure Cosmos DB containers regarding data schemas?",
    "answer": "Containers in Azure Cosmos DB are schema-agnostic, meaning items within a container can have arbitrary schemas or different entities so long as they share the same partition key.",
    "options": [
      "Containers require a strict schema for all items.",
      "Containers are schema-agnostic.",
      "Containers only support JSON schema.",
      "Containers do not support any schema."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "b4582ee3bf32568585e992f5066ad8b833a5d30e7b75422cd54987a42003b8f4",
    "question": "What is the maximum amount of data a logical partition in Azure Cosmos DB can store?",
    "answer": "20 GB",
    "options": ["10 GB", "20 GB", "50 GB", "Unlimited"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "8736ca264c98058e3a7eae1c562f4b8ac956058703bdfe8b6bf62d5d1d8d8dab",
    "question": "What is the relationship between Azure Cosmos DB and Azure Resource Group?",
    "answer": "To begin using Azure Cosmos DB, you need to create an Azure Cosmos DB account in an Azure Resource Group in your subscription.",
    "options": [
      "Azure Cosmos DB is a type of Azure Resource Group.",
      "Azure Resource Group is a type of Azure Cosmos DB.",
      "Azure Cosmos DB account is created in an Azure Resource Group.",
      "Azure Resource Group is created in an Azure Cosmos DB account."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "f891252d14d8b57e5a07d59540685803847b38e7e408573a3a976e55825c681e",
    "question": "What is the maximum throughput amount a physical partition in Azure Cosmos DB can have?",
    "answer": "10,000 RU/s",
    "options": ["1,000 RU/s", "5,000 RU/s", "10,000 RU/s", "20,000 RU/s"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "027f86863fa41c7ba20dd55b2f948c2226846e9b89db0aeecb1e8eba6eb8d9b2",
    "question": "You are designing an application that will use Azure Cosmos DB as its data store. The application will store data about customers and their orders. Each customer can have multiple orders, and you expect a high volume of read and write operations on the orders.\n\nYou want to ensure that the application can handle the expected load and that the data is distributed efficiently. You also want to ensure that the application can retrieve all orders for a specific customer as efficiently as possible.\n\nHow would you design the data model in Azure Cosmos DB to meet these requirements?",
    "answer": "A container can hold items with different schemas as long as they share the same partition key. By storing customers and their orders in the same container and using the `customer ID` as the partition key, you can ensure that all orders for a specific customer are stored in the same partition. This makes it efficient to retrieve all orders for a specific customer.  \nThe other options are incorrect because they do not efficiently distribute the data or allow for efficient retrieval of all orders for a specific customer.",
    "options": [
      "Create separate databases for customers and orders, and use the `customer ID` as the partition key for the orders database.",
      "Create a single container for both customers and orders, and use the `customer ID` as the partition key.",
      "Create separate containers for customers and orders, and use the `customer ID` as the partition key for the orders container.",
      "Create separate containers for customers and orders, with the `customer ID` as the partition key for the customers container and the `order ID` as the partition key for the orders container."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "9f53c1e3bffbd8c0967309b5f65a9c7ba204bc490bcce98be1f138839d28d985",
    "question": "What is the main characteristic of shared throughput in Azure Cosmos DB?",
    "answer": "Shared throughput in Azure Cosmos DB is specified at the database level and then shared with up to 25 containers within the database. The other options are incorrect because they describe characteristics of dedicated throughput, not shared throughput.",
    "options": [
      "It is exclusively reserved for a single container.",
      "It is specified at the database level and shared with up to 25 containers.",
      "It automatically adjusts based on the current workload.",
      "It is a fixed amount of throughput."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "d5d1c01165fd52025c93bde4609ce1b224afdd7ace1abc071f3b6baedaedc98b",
    "question": "Can you switch between dedicated and shared throughput in Azure Cosmos DB?",
    "answer": "You cannot switch between dedicated and shared throughput once a container is created. If you want to change a container from shared to dedicated throughput, you must create a new container and copy the data to it.",
    "options": [
      "Yes, you can switch at any time.",
      "No, you cannot switch once a container is created.",
      "Yes, but only within the first 24 hours of creating a container.",
      "No, but you can increase the amount of dedicated throughput for a container."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "d27652d745eaa19e4a9b206554806feb7b08c7b0033e866f0d1e87cc10d7f97b",
    "question": "What is the main advantage of using autoscale throughput in Azure Cosmos DB?",
    "answer": "Autoscale automatically adjusts the amount of throughput based on the current workload.",
    "options": [
      "It provides a fixed amount of throughput regardless of workload.",
      "It automatically adjusts the amount of throughput based on the current workload.",
      "It allows throughput to be shared among multiple containers.",
      "It requires manual adjustment of throughput based on workload."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "22ccdee5e660b4f6b069a996448997aaca548e5a7cd5fe1d088d37643d98b294",
    "question": "You are designing an application that will use Azure Cosmos DB as its data store. The application will have a high volume of read and write operations. You want to ensure that each container in your database can handle the expected load independently. How should you configure the throughput?",
    "answer": "Dedicated throughput in Azure Cosmos DB is specified at the container level, meaning each container has its own amount of throughput that it can use independently of other containers.  \nShared throughput is specified at the database level (not at container level) and shared among containers, not at the individual database level.  \nDedicated throughput is not assigned at the database level but at the container level.",
    "options": [
      "Use shared throughput and create a separate database for each container.",
      "Use dedicated throughput and assign it to each container individually.",
      "Use shared throughput and assign it to each container individually.",
      "Use dedicated throughput and assign it at the database level."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "c6b5e4a19606c45e1a33ccaeb6b598d3160b3bac4c2fe764ddf64ef407aec706",
    "question": "You are designing an application that will use Azure Cosmos DB as its data store. The application will have several containers, each storing different types of data, but none of the containers are expected to have a high volume of read and write operations. You want to optimize costs while ensuring that all containers have enough throughput. Which of the following strategies would be the most cost-effective?",
    "answer": "Shared throughput in Azure Cosmos DB is specified at the database level and then shared among all containers within the database. This is ideal for scenarios where you have multiple containers that do not require a high volume of operations and you want to optimize costs.  \nDedicated throughput is specified at the container (not at dayabase level) level and would be more expensive for a scenario where the containers do not have a high volume of operations.  \nShared throughput is not assigned at the container level but at the database level.",
    "options": [
      "Assign dedicated throughput to each container individually, ensuring each has enough to handle its expected load.",
      "Assign shared throughput at the database level, allowing all containers to draw from a common pool of throughput.",
      "Assign dedicated throughput at the database level, ensuring the entire database has enough to handle its expected load.",
      "Assign shared throughput to each container individually, allowing each to draw from its own pool of throughput."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "71585d333c355c47f7439d5e5bd8131e642dd2fb703394f69f31e1ee6c4da006",
    "question": "What is the correct syntax to select all properties from items in a Cosmos DB container?",
    "answer": "Same as SQL. Note: The `SELECT *` syntax is only valid if FROM clause has declared exactly one alias.",
    "options": ["`SELECT *`", "`SELECT c.*`", "`SELECT ALL`", "`SELECT c.ALL`", "`SELECT c FROM root AS c`"],
    "answerIndexes": [0, 1, 4],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "9fe44b4ac881f727b0090a781d338fae9d41831d7e839715541d0b0f3d08d19a",
    "question": "Given the following query, which option correctly selects all properties from items in the container?\n\n```sql\nSELECT ?\nFROM c\nJOIN p IN c.phones\n```",
    "answer": "`SELECT c.*`\nThe `SELECT *` syntax is only valid if the FROM clause has declared exactly one alias. In this case, the FROM clause has declared two aliases (`c` and `p`), so `SELECT *` is not valid.  \nThe `SELECT p.*` syntax would select all properties from the items in the phones array  \nThe `SELECT c.*, p.*` would select all properties from both the items in the container and the phones array. However, in this context, SELECT c.\\* is the correct answer because the question asks for all properties from items in the container.",
    "options": ["`SELECT *`", "`SELECT c.*`", "`SELECT p.*`", "`SELECT c.*, p.*`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "d894967051225e897bf1ea20001efe058814bd8635525678957ff43fba132921",
    "question": "Given the following data:\n\n```json\n[\n  {\n    \"id\": \"6e9f51c1-6b45-440f-af5a-2abc96cd083d\",\n    \"categoryName\": \"Sleeping Bags\",\n    \"name\": \"Vareno Sleeping Bag (6') Turmeric\",\n    \"sku\": \"vareno-sleeping-bag-72109\",\n    \"price\": 120,\n    \"closeout\": true,\n    \"manufacturer\": {\n      \"name\": \"Vareno\"\n    },\n    \"tags\": [{ \"name\": \"Color Group: Yellow\" }, { \"name\": \"Bag Shape: Mummy\" }]\n  },\n  {\n    \"id\": \"12345\",\n    \"categoryName\": \"Surfboards\",\n    \"name\": \"Teapo Surfboard (6'10\\\") Grape\",\n    \"sku\": \"teapo-surfboard-72109\",\n    \"price\": 500,\n    \"closeout\": false,\n    \"manufacturer\": {\n      \"name\": \"Taepo\"\n    },\n    \"tags\": [{ \"name\": \"Color Group: Purple\" }]\n  }\n]\n```\n\nWhat result this query will produce:\n\n```sql\nSELECT {\n  \"name\": p.name,\n  \"sku\": p.sku,\n  \"vendor\": p.manufacturer.name\n} AS product\nFROM products p\nWHERE p.sku = \"teapo-surfboard-72109\"\n```\n\n```jsonc\n[\n  // Result here\n]\n```",
    "answer": "\n\n```json\n[\n  {\n    \"product\": {\n      \"name\": \"Teapo Surfboard (6'10\\\") Grape\",\n      \"sku\": \"teapo-surfboard-72109\",\n      \"vendor\": \"Taepo\"\n    }\n  }\n]\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "461649b92770607665ac64da9d06ecca4e0718c82fb32c38feb9cf5312d73d31",
    "question": "Given the following data:\n\n```json\n[\n  {\n    \"id\": \"6e9f51c1-6b45-440f-af5a-2abc96cd083d\",\n    \"categoryName\": \"Sleeping Bags\",\n    \"name\": \"Vareno Sleeping Bag (6') Turmeric\",\n    \"price\": 120,\n    \"closeout\": true,\n    \"manufacturer\": {\n      \"name\": \"Vareno\"\n    },\n    \"tags\": [{ \"name\": \"Color Group: Yellow\" }, { \"name\": \"Bag Shape: Mummy\" }]\n  }\n]\n```\n\nWrite a query that will return this result, filtering by `teapo-surfboard-72109`:\n\n```json\n[\n  {\n    \"name\": \"Teapo Surfboard (6'10\\\") Grape\",\n    \"sku\": \"teapo-surfboard-72109\",\n    \"vendor\": \"Taepo\"\n  }\n]\n```\n\n```tsql\n-- Code here\n```",
    "answer": "\n\n```sql\nSELECT VALUE {\n  \"name\": p.name,\n  \"sku\": p.sku,\n  \"vendor\": p.manufacturer.name\n}\nFROM products p\nWHERE p.sku = \"teapo-surfboard-72109\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "56e1e515269eb500cf5996fe79fd63d414479d210b0266c1dd5bd6d35f17c2bf",
    "question": "Which of the following best describes the format of data returned by a Cosmos DB query when using the PostgreSQL API?",
    "answer": "Cosmos DB internally stores data in a JSON-like format, regardless of the API used.",
    "options": [
      "XML, since XML is a widely used format for data interchange.",
      "Tabular data, since the PostgreSQL API transforms the data to resemble traditional SQL results.",
      "JSON, because Cosmos DB internally stores data in a JSON-like format, regardless of the API used.",
      "Binary data, encoded in a PostgreSQL-specific format."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "a42175a232f08c17492e8fcf2c8929ad610ac7672294f80c08bad94536f83527",
    "question": "When executing a query in Azure Cosmos DB utilizing the Table API as follows:\n\n```sql\nSELECT *\nFROM Invoices i\nWHERE i.id =1\n```\n\nWhat kind of outcome can you anticipate from this query?",
    "answer": "Cosmos DB internally stores data in a JSON-like format, regardless of the API used.",
    "options": [
      "Data in an XML format",
      "Data in a JSON format",
      "An error due to incorrect syntax",
      "A table structured in rows and columns"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "0b1493b1a80f2ab0ea14507593289a239f3477cf8729c19b6cf14384a453c7c2",
    "question": "Your organization has deployed a multi-region Azure Cosmos DB database to ensure high availability. Each region can simultaneously read from and write to the database. There is a chance that the same document is updated concurrently in different regions, potentially leading to conflicts. To handle this, you've decided to use the automatic conflict resolution policy provided by Azure Cosmos DB. Based on your understanding of this policy, which strategy best describes how Azure Cosmos DB would resolve these conflicts?",
    "answer": "Last Writer Wins",
    "options": [
      "The update from the region that first modified the document is always retained.",
      "The update from a randomly selected region is retained.",
      "The update from the region that last modified the document is retained.",
      "All updates from all regions are retained in some manner."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "a948fed2cf75370a65e9b9bbb3b049e8783bdfa6136b281e3684e9833ee3ff1c",
    "question": "You're running a global highly available e-commerce application with Azure Cosmos DB, where updates to product inventory are made concurrently across multiple regions. However, you've noticed that sometimes inventory data gets out of sync across different regions. To ensure consistency in inventory data, what feature can you use in Azure Cosmos DB?",
    "answer": "Setting up a conflict resolution policy helps manage conflicts that arise from concurrent writes across different regions, ensuring data consistency across regions.  \nThe other options are related to performance, consistency levels, and write capabilities, but they don't directly address data conflicts arising from concurrent writes.  \nDisabling multi-region writes could potentially avoid conflicts, but it would also negate the benefits of having a globally distributed application, like reduced latency for writes in different regions.",
    "options": [
      "Increase Request Units (RUs)",
      "Implement strong consistency level",
      "Disable multi-region writes",
      "Set up a conflict resolution policy"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "65d44013c77b6ce5df6448a3ecb9943ae1bdc2edeb89754d93a43c27945954a7",
    "question": "An e-commerce company is implementing a data migration operation on their Azure Cosmos DB database. The operation needs to be precisely controlled, allowing the engineering team to manually request the server for a set of changes, process them, and then request the next set. Which Change Feed interaction model best fits this use case?",
    "answer": "The pull model offers extra control for specific use-cases, especially for data migration.",
    "options": ["Pull Model", "Push Model", "Either Model", "None of the Models"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "6538b02d146a44bd7708c1889c32bc86b8799adb966451bf23e71a4abab135a2",
    "question": "A financial technology company is developing a real-time fraud detection system using Azure Cosmos DB. The system needs to react instantaneously to changes in the database, with minimal latency. Which Change Feed interaction model best fits this use case?",
    "answer": "In the push model, the change feed processor automatically sends work to a client.",
    "options": ["Pull Model", "Push Model", "Either Model", "None of the Models"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "a6f0055236a2b99b8f4148b3e54790ad094b52e3b4764a52da73b39adf74f3c3",
    "question": "Which of the following statements about Time to Live (TTL) in Azure Cosmos DB are true? Select all that apply.",
    "answer": "Even after TTL has expired, if the container is overloaded with requests and there aren't enough Request Units available, the data deletion is delayed until there are enough resources.  \nItems are removed after specific time period, since the time they were **last modified**.",
    "options": [
      "TTL can be set at the container level and can be overridden on a per-item basis.",
      "TTL can be set only at item level.",
      "When TTL has expired, if the container is overloaded with requests, data is immediately deleted.",
      "Items with TTL set to \"-1\" don't expire by default.",
      "If TTL is not set on a container, then the TTL on an item in this container has no effect.",
      "If a container's TTL is null or missing, items without a specific TTL will not expire, while others adhere to their own TTL.",
      "TTL is used for automatically deleting items after a certain period, based on their creation time."
    ],
    "answerIndexes": [0, 3, 4],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "dbcf670e2e3f144ddca51ed32cb1b7e4c4a398d7e7f11a81fb97d5da9b329544",
    "question": "You aim to create a Cosmos DB client, a database, and finally a container using the Azure Cosmos DB NoSQL API. Your intention is to utilize this container to manage a collection of IoT device telemetry data. Implementing .NET for this task, you find that after running the following code snippet, errors are encountered and no elements are created as anticipated. Several discrepancies in the code prevent its successful execution. What corrections should you apply to address these issues?\n\n```cs\nusing Azure.CosmosDb;\n\nCosmosClient cosmosClient = new CosmosClient(\"https://myCosmosAccount.documents.core.windows.net:443/\", configuration[\"CosmosKey\"]);\n\nDatabase database = await cosmosClient.CreateDatabaseIfNotExistsAsync(\"db\");\n\nContainer container = await database.CreateContainerAsync(\n    id: \"id\",\n    partitionKeyPath: \"pk\",\n    throughput: 200\n);\n```",
    "answer": "\n\n```cs\nusing Microsoft.Azure.Cosmos;\n\nCosmosClient cosmosClient = new CosmosClient(\"https://mycosmosdbaccount.documents.azure.com:443/\", configuration[\"CosmosKey\"]);\n\nDatabase database = await cosmosClient.CreateDatabaseIfNotExistsAsync(\"dbName\");\n\nContainer container = await database.CreateContainerAsync(\n    id: \"containerId\",\n    partitionKeyPath: \"/pk\",\n    throughput: 400\n);\n```\n\n- **Incorrect using** - should use `Microsoft.Azure.Cosmos`.\n\n- **Incorrect Cosmos DB account endpoint** - The endpoint used to initialize the CosmosClient object is incorrect. The provided endpoint URL contains `core.windows.net` instead of `documents.azure.com`.\n\n- **Incorrect database name** - should be between 3 and 63 chars, `db` is just 2.\n\n- **Incorrect container id** - should be between 3 and 63 chars, `id` is just 2.\n\n- **Incorrect partition key path** - The partition key path doesn't start with a `/`.\n\n- **Incorrect throughput value** - The throughput argument is set to 200, but minimum is 400 or higher.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "2317700ead93cfaeaf916a32fa08d75003366d03945b8d52e47bf305a16cb268",
    "question": "You are developing an application that requires the efficient and rapid addition of a large number of items using Azure Cosmos DB. Which of the following practices can be employed to optimize performance for this use case?",
    "answer": "Setting `EnableContentResponseOnWrite` to false for workloads with heavy create/write payloads and excluding unused paths from indexing can improve performance by eliminating the return of created or updated resources to the SDK and facilitating faster writes, respectively, benefiting write-heavy workloads.\n\nUsing `FeedIterator` can improve performance when reading multiple pages of query results, but it doesn't directly optimize write-heavy workloads.  \nUsing `PartitionKey` for point read can reduce the Request Unit (RU) charge and improve performance for read-heavy workloads, but it doesn't directly optimize write-heavy workloads.",
    "options": [
      "Set the `EnableContentResponseOnWrite` request option to false.",
      "Use `FeedIterator` for iterating multiple pages of query results.",
      "Exclude unused paths from indexing.",
      "Use `PartitionKey` for point read."
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "a50e86f1de8263afb08454b65953c83b96f73e5504225f5c149eb8c8af5a7359",
    "question": "You are developing an application with a read-heavy workload using Azure Cosmos DB. Which of the following practices could help improve performance?",
    "answer": "Utilizing `PartitionKey` for point read can reduce Request Unit (RU) charges and enhance performance, particularly for read-heavy workloads, while employing `FeedIterator` can improve performance when handling multiple pages of query results, a common scenario in read-heavy workloads.\n\nSetting `EnableContentResponseOnWrite` to false can improve performance for write-heavy workloads, but it doesn't directly optimize read-heavy workloads.  \nIncreasing the size of your documents can actually increase the Request Unit (RU) charge and potentially lead to more errors, so it's not a good practice for optimizing performance.",
    "options": [
      "Use `PartitionKey` for point read.",
      "Set the `EnableContentResponseOnWrite` request option to false.",
      "Use `FeedIterator` for iteratin multiple pages of query results.",
      "Increase the size of your documents."
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "ca9288a50ac468f69d88665e555a99239b171d9ef3358ad5bfa144dff6b50266",
    "question": "Your application is experiencing latency issues when connecting to Azure Cosmos DB. Which of the following could be a potential cause and how would you address it?",
    "answer": "Running your application in the same Azure region as your Azure Cosmos DB account and using Direct mode for connectivity can significantly reduce latency, optimizing the performance of your application.\n\nWhile using the latest version of the Azure Cosmos DB SDK can generally improve performance, it doesn't specifically target latency issues.  \nUsing a single instance of CosmosClient for the lifetime of your application is a best practice for better performance and resource utilization, so it's unlikely to be the cause of latency issues.",
    "options": [
      "The application is not using the latest version of the Azure Cosmos DB SDK.",
      "The application is running in a different Azure region than your Azure Cosmos DB account.",
      "The application is using a single instance of CosmosClient for the lifetime of the application.",
      "The application is using Gateway mode for connectivity."
    ],
    "answerIndexes": [1, 3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "63402f8093677ec8a11ac0f24c668a0e759c98d0ea00eae7595769d562829eb6",
    "question": "You are developing an application that needs to upload a large number of items into Azure Cosmos DB as quickly as possible. Which of the following practices could help improve performance?",
    "answer": "Enabling Bulk support, utilizing `ThroughputProperties` for managing throughput, setting connectivity mode to `Direct`, and setting `EnableContentResponseOnWrite` to false can collectively enhance performance when efficiently handling large volumes of data in Azure Cosmos DB by optimizing high-volume write operations, balancing resource usage, and minimizing network and serialization costs.  \nUsing Windows 64-bit host processing and a single instance of CosmoCLient can generally improve performance, but it doesn't specifically target scenarios where large volumes of data are being dumped into the database.",
    "options": [
      "Enable Bulk support.",
      "Use ThroughputProperties to manage throughput.",
      "Use Direct mode for connectivity.",
      "Use Windows 64-bit host processing.",
      "Use single instance of CosmosClient.",
      "Set the EnableContentResponseOnWrite request option to false."
    ],
    "answerIndexes": [0, 1, 2, 5],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "fc392d2c45d40ff05b1df6cbda370b7f130f06755b22c606c8c9a868f12e0c54",
    "question": "You are a software developer tasked with creating a C# utility that interacts with Azure Cosmos DB. Your script should establish a connection, create a database and a container if they don't exist, and finally create an item in the container. Use placeholders like `\"<connection-string>\"`, `\"<database>\"`, `\"<container>\"`.\n\n```cs\n// Code here\n```",
    "answer": "\n\n```cs\nvar cosmosClient = new CosmosClient(\"<connection-string>\");\n\n// Create a database\nvar database = await cosmosClient.CreateDatabaseIfNotExistsAsync(\"<database>\");\n\n// Create a container\nvar container = await database.CreateContainerIfNotExistsAsync(\"<container>\", \"/mypartitionkey\");\n\n// Create an item\ndynamic testItem = new\n{\n    id = \"1\",\n    mypartitionkey = \"mypartitionvalue\",\n    description = \"mydescription\"\n};\nawait container.CreateItemAsync(testItem, new PartitionKey(testItem.mypartitionkey));\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "69350a1a8b265c91d2380ecd6d91b2d926d4f2d4784f0f728f7395f2cf574768",
    "question": "You are developing a system that needs to interact with Azure Cosmos DB. Your task is to write a piece of code that connects to a specific Cosmos DB instance and retrieves sales orders for a given account number, with a maximum item count of 1.\n\n```cs\nvar connectionString = \"<connection-string>\";\nvar dbName = \"<database>\";\nvar containerName = \"<container>\";\nvar partitionKey = \"Accounts\";\nvar throughputValue = 400;\nvar accountNumber = \"190823\";\n\n// Code here\n```",
    "answer": "\n\n```cs\nvar connectionString = \"<connection-string>\";\nvar dbName = \"<database>\";\nvar containerName = \"<container>\";\nvar partitionKey = \"Accounts\";\nvar throughputValue = 400;\nvar accountNumber = \"190823\";\n\nvar cosmosClient = new CosmosClient(connectionString);\nDatabase database = await cosmosClient.CreateDatabaseIfNotExistsAsync(dbName);\nContainer container = await database.CreateContainerIfNotExistsAsync(id: , partitionKeyPath: $\"/{partitionKey}\", throughput: throughputValue);\nQueryDefinition query = new QueryDefinition(\n    \"select * from sales s where s.AccountNumber = @AccountInput \")\n    .WithParameter(\"@AccountInput\", accountNumber);\nFeedIterator<SalesOrder> resultSet = container.GetItemQueryIterator<SalesOrder>(\n    query,\n    requestOptions: new QueryRequestOptions()\n    {\n        PartitionKey = new PartitionKey(partitionKey),\n        MaxItemCount = 1\n    });\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "944ead406fd807c9e68eb876c4013f6ac23103c285c6550af197b5219882d485",
    "question": "Which of the following tools can you use to create and execute triggers, stored procedures, and UDFs in Azure Cosmos DB?",
    "answer": "The Azure Portal is one of the tools that allow you to create and execute triggers, stored procedures, and UDFs in Azure Cosmos DB. The other options listed are not directly used for this specific purpose within Azure Cosmos DB.",
    "options": ["Visual Studio Code", "Azure Portal", "SQL Server Management Studio", "Azure DevOps"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "5c7fa1d0371a61734f32d63b7eadab476ad0ac69ae7ace8b0ee384b16622736a",
    "question": "Your organization is planning to migrate a NoSQL database to Azure Cosmos DB and requires an API that can handle flexible document structures and operate using the BSON format. Which API within Azure Cosmos DB would best meet these requirements?",
    "answer": "In the context of Azure Cosmos DB, the BSON (Binary JSON) format is specifically associated with the MongoDB API.",
    "options": ["SQL API", "Table API", "Gremlin API", "Cassandra API", "MongoDB API"],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "c1a00c374de9bc21379f2608c90dfad8b65e93310978670e9652838c17942edf",
    "question": "In Azure Cosmos DB, when working with stored procedures, triggers, or user-defined functions, what specific method can be utilized to log the execution steps or trace information?",
    "answer": "stored procedures, triggers, and user-defined functions are written on JavaScript.",
    "options": ["Console.WriteLine()", "Trace.Debug()", "System.out.println()", "console.log()"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "d6dc222eeb6d7ec61dd10b11a1c3c87248386d0c22d619124c20100ebdbb0b44",
    "question": "What language should you use if you want to create a new user-defined function, stored procedure, or trigger?",
    "answer": "stored procedures, triggers, and user-defined functions are written on JavaScript.",
    "options": ["JSON", "JavaScript", "C#", "XML", "YAML"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "950bf4a142b3bc978764c662b46a758aab97c25a619f1ebc65ad9b8465bfa3d7",
    "question": "How many free tier Azure Cosmos DB accounts can you have per Azure subscription?",
    "answer": "You can have up to one free tier Azure Cosmos DB account per Azure subscription.",
    "options": ["Two", "Unlimited", "One", "None"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "be65cf74054bf6668eb9798891e6830f878a32af3dd57dea171665d9885e3a55",
    "question": "What operations can a stored procedure perform of document in Cosmos DB?",
    "answer": "Stored procedures are capable of performing CRUD and query operations on any document in a collection.",
    "options": ["Create", "Read", "Update", "Delete", "No operations allowed"],
    "answerIndexes": [0, 1, 2, 3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "3e60cfebbdb32f549ae50e855df8327d032654bbf82495fd034ec0e8df91c27e",
    "question": "Complete the following pre-trigger that adds timestampt to every new item:\n\n```js\nfunction validateToDoItemTimestamp() {\n  /* Code here */\n\n  // item to be created in the current operation\n  var itemToCreate = /* Code here */;\n\n  // validate properties\n  if (!(\"timestamp\" in itemToCreate)) {\n    var ts = new Date();\n    itemToCreate[\"timestamp\"] = ts.getTime();\n  }\n\n  // update the item that will be created\n  /* Code here */\n}\n```",
    "answer": "\n\n```js\nfunction validateToDoItemTimestamp() {\n  var context = getContext();\n  var request = context.getRequest();\n\n  // item to be created in the current operation\n  var itemToCreate = request.getBody();\n\n  // validate properties\n  if (!('timestamp' in itemToCreate)) {\n    var ts = new Date();\n    itemToCreate['timestamp'] = ts.getTime();\n  }\n\n  // update the item that will be created\n  request.setBody(itemToCreate);\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "12146b94e15e8a72a71c309c6eb1aa0706d7745bee804ac40ba5a204bea4df3a",
    "question": "You are working on a project that involves storing large amounts of data in Azure Cosmos DB using the SQL API. The data consists of user profiles, each with a variety of attributes. During the design phase, you realize that no single attribute is suitable for partitioning, as they either appear too frequently or not frequently enough to distribute the workload evenly across partitions. Which of the following strategies could you use to create a synthetic partition key that ensures even distribution of workloads?",
    "answer": "\n\n- Concatenating multiple common properties, followed by a random number, would create a synthetic partition key that is likely to distribute data more evenly across partitions. The random number ensures that the key is unique and helps in distributing the workload.\n- Appending a timestamp to the user's country of residence would also create a synthetic partition key that can distribute data more evenly. The timestamp ensures that even if many users are from the same country, their data will be spread across multiple partitions.\n- Using the user's email address as it is unique would not necessarily distribute the workload evenly, especially if the application has operations that need to query or update multiple documents that can't be guaranteed to be in the same partition.\n- Using the user's age would not be a good choice for partitioning, as it could lead to hot partitions. Age groups like 20-30 or 30-40 could have more data, leading to uneven distribution.\n- Using the user's membership status could also lead to hot partitions if one membership type is significantly more common than others.",
    "options": [
      "Concatenating multiple common properties, followed by a random number",
      "Using the user's email address as it is unique",
      "Appending a timestamp to the user's country of residence",
      "Using the user's age as the partition key",
      "Using the user's membership status (e.g., Free, Premium, VIP)"
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "698b72ccef27c6ff5e70100bc18f15df7b66f2519210e6cb9fd63e5c39b2e800",
    "question": "How to retain the change feed in Azure Cosmos DB until the data is deleted? What should be the value of TTL (Time to Live) property?",
    "answer": "If the TTL property is set on an item to -1, the change feed will remain if the data is not deleted.",
    "options": ["null", "-1", "0", "1"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "7409a6901852821a9e37b271ed86ce8c028a998d991d9cad0189989ea385d2a7",
    "question": "Create a stored procedure that creates a document.\n\n```js\nvar createDocumentStoredProc = {\n  id: 'createMyDocument',\n  // This stored procedure creates a new item in the Azure Cosmos container\n  body: function createMyDocument(documentToCreate) {\n    // Code here\n  },\n};\n```",
    "answer": "\n\n```js\nvar createDocumentStoredProc = {\n  id: 'createMyDocument',\n  // This stored procedure creates a new item in the Azure Cosmos container\n  body: function createMyDocument(documentToCreate) {\n    var context = getContext();\n    var collection = context.getCollection();\n\n    // Async 'createDocument' operation, depends on JavaScript callbacks\n    // returns true if creation was successful\n    var accepted = collection.createDocument(\n      collection.getSelfLink(),\n      documentToCreate,\n      // Callback function with error and created document parameters\n      function (err, documentCreated) {\n        // Handle or throw error inside the callback\n        if (err) throw new Error('Error' + err.message);\n        // Return the id of the newly created document\n        context.getResponse().setBody(documentCreated.id);\n      }\n    );\n\n    // If the document creation was not accepted, return\n    if (!accepted) return;\n  },\n};\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "95f5b1c712a1e92387a452704fb9e9470eb5de77f75a887d49eb85cddc7de0e2",
    "question": "How to set Time-To-Live property of `ContainerProperties` to be 1 hour?",
    "answer": "Property is `DefaultTimeToLive`, unit is _seconds_,",
    "options": ["`DefaultTimeToLive = 60`", "`DefaultTimeToLive = 3600`", "`TTL = 60`", "`TTL = 3600`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "0a1a727f2f397f618f855f5e50ee7737df079577ce39ae7f5e2022445294564a",
    "question": "You have to ensure the following query can be executed:\n\n```sql\nselect * from customers order by customername, city asc\n```\n\nYou have to define a policy for this:\n\n```jsonc\n{\n  \"automatic\": true,\n  \"indexingMode\": \"Consistent\",\n  \"includedPaths\": [{ \"path\": \"/*\" }],\n  \"excludedPaths\": [],\n  /* Code here */\n}\n```",
    "answer": "Queries that have an `ORDER BY` clause with two or more properties require a composite index.\n\n```json\n{\n  \"automatic\": true,\n  \"indexingMode\": \"Consistent\",\n  \"includedPaths\": [{ \"path\": \"/*\" }],\n  \"excludedPaths\": [],\n  \"compositeIndexes\": [\n    [\n      {\n        \"path\": \"/city\",\n        \"order\": \"ascending\"\n      },\n      {\n        \"path\": \"/customername\",\n        \"order\": \"descending\"\n      }\n    ]\n  ]\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Cosmos DB"
  },
  {
    "id": "06cf1626ba05f3deabe75ed57d48a633651db9c3c48f81dfde0568c10c246713",
    "question": "You are working on an Azure-based application that utilizes Azure Cosmos DB for data storage. You've noticed scalability issues attributed to hot partitioning. What is the likely cause of this hot partitioning issue in Azure Cosmos DB?",
    "answer": "Hot partitioning typically arises when there is an uneven distribution of requests or data across logical partitions. This imbalance prevents Azure Cosmos DB from scaling effectively. The ideal partition key should ensure a balanced distribution of both storage and throughput across partitions.",
    "options": [
      "Executing queries that don't use the partition key",
      "Running queries focused on a single partition key",
      "Failing to set an indexing policy",
      "Designing a partition key that leads to uneven request distribution"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Cosmos DB"
  },
  {
    "id": "5885ab8c4cbd895cfb4405b82aeaacce1b10d446c0967fa3b1987ff2d238e124",
    "question": "Which of the following docker images is used to build an ASP.NET app?",
    "answer": "The `dotnet/core/sdk` image includes the Command Line Tools (CLI) and is optimized for local development, debugging, and unit testing.",
    "options": ["dotnet/core/sdk", "dotnet/core/aspnet", "None of these", "Both"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Docker"
  },
  {
    "id": "2a02d276342c3d300b4dfcd8ccc372192a8087e7ec6b6efcae22bd43a7689e29",
    "question": "Which of the following docker images is used to run an ASP.NET app?",
    "answer": "The `dotnet/core/aspnet` image contains the ASP.NET Core runtime and libraries and is optimized for running apps in production.",
    "options": ["dotnet/core/sdk", "dotnet/core/aspnet", "None of these", "Both"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Docker"
  },
  {
    "id": "a2e6c86a7c6e058468a5b1b20e3d80d2f8ee46365238bf8c1dae1dda120765c9",
    "question": "Define a Dockerfile where build and run in different containers (multi-stage)\n\n```Dockerfile\nFROM mcr.microsoft.com/dotnet/core # Fill in details\n```",
    "answer": "\n\n```Dockerfile\nFROM mcr.microsoft.com/dotnet/core/sdk:3.0 AS build\nWORKDIR /app\n\n# copy csproj and restore as distinct layers\nCOPY *.sln .\nCOPY aspnetapp/*.csproj ./aspnetapp/\nRUN dotnet restore\n\n# copy everything else and build app\nCOPY aspnetapp/. ./aspnetapp/\nWORKDIR /app/aspnetapp\nRUN dotnet publish -c Release -o out\n\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.0 AS runtime\nWORKDIR /app\nCOPY --from=build /app/aspnetapp/out ./\nENTRYPOINT [\"dotnet\", \"aspnetapp.dll\"]\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Docker"
  },
  {
    "id": "dc45450b8f85ed2ab1406d6e280c51b9ef0a2444188f388b2730a8f2f8b271a0",
    "question": "Can you compose a Dockerfile for a .NET Core 3.1 web application, named \"WebApplication1\", which is capable of serving both secure and non-secure web traffic? The source code for the application resides in \"/src/WebApplication1\". How would you structure the Dockerfile stages to ensure the final image is lean and the build process adheres to best practices? Lastly, ensure that the application starts by executing a DLL file.\n\n```Dockerfile\nFROM mcr.microsoft.com/dotnet/core # Fill in details\n```",
    "answer": "Note: Because we are exposing our app, `FROM mcr.microsoft.com/dotnet/core/aspnet:3.1` is in the first step, not the last\n\n```Dockerfile\nFROM mcr.microsoft.com/dotnet/core/aspnet:3.1 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build\nWORKDIR /src\nCOPY [\"WebApplication1/WebApplication1.csproj\", \"WebApplication1/\"]\nRUN dotnet restore \"WebApplication1/WebApplication1.csproj\"\nCOPY . .\nWORKDIR \"/src/WebApplication1\"\nRUN dotnet build \"WebApplication1.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"WebApplication1.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"WebApplication1.dll\"]\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Docker"
  },
  {
    "id": "8ac62ce94f58e1501b048748e7e0a94f7153becef7a5c93b9022321d03c38054",
    "question": "You're migrating a .NET Core application named `InventoryManager` to run in containers. A Dockerfile has been created for this purpose. Now, you need to build, tag, and publish the `InventoryManager` container image to an Azure container registry named `ProductionRegistry`. Your version tag for this operation is `v1.0`. Considering that you need to authenticate with Azure and with `ProductionRegistry`, what sequence of actions (commands) should you undertake?\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Log in to Azure\naz login\n\n# Authenticate with the Azure Container Registry 'ProductionRegistry'\naz acr login --name ProductionRegistry\n\n# Build the image locally\ndocker build -t inventorymanager .\n\n# Tag the local image with the version and the ACR Login Server\ndocker tag inventorymanager:latest ProductionRegistry.azurecr.io/inventorymanager:v1.0\n\n# Push the image to 'ProductionRegistry' on Azure\ndocker push ProductionRegistry.azurecr.io/inventorymanager:v1.0\n```\n\n`inventorymanager:latest` is the image name followed by a tag. The tag part is optional, and if you don't specify it, Docker will automatically use the tag \"latest\". You can build and tag in one step by running `docker build -t ProductionRegistry.azurecr.io/app1:1.0 .`\n\nTo push a container image to an Azure container registry, you need to tag the image following the convention `<registry-name>.azurecr.io/<image-name>`",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Docker"
  },
  {
    "id": "8abe201e424309654a5cb3ccd55392c43da0c687b57790bcf87738446bcdc78d",
    "question": "You are tasked with providing secure remote access to an on-premises application for your organization's employees. Which action should you perform?",
    "answer": "The correct action is to activate application proxy in Microsoft Entra ID. Microsoft Entra ID Application Proxy provides secure remote access to on-premises applications.",
    "options": [
      "Activate application proxy in Microsoft Entra ID.",
      "Upgrade the staff accounts to Microsoft Entra ID Premium P1.",
      "Establish a new conditional access policy in Microsoft Entra ID.",
      "Set up the portal to utilize Microsoft Entra ID business-to-consumer (B2C)."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "1fc252f3ee0d5a2d6fb75c8ccb8a94d7a9f0f0ff097ab7be2703a564abde9205",
    "question": "Your organization is planning to collaborate with a partner company on a project. You need to provide the partner company's users with access to certain applications in your Microsoft Entra ID. Which action should you perform?",
    "answer": "The correct action is to configure the applications to use Microsoft Entra ID business-to-business (B2B). Microsoft Entra ID B2B allows you to share your company's applications with external users in a secure manner.",
    "options": [
      "Activate application proxy in Microsoft Entra ID.",
      "Upgrade the staff accounts to Microsoft Entra ID Premium P1.",
      "Establish a new conditional access policy in Microsoft Entra ID.",
      "Configure the applications to use Microsoft Entra ID business-to-business (B2B)."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "af2aba2251963e8016f4e572945ac1faae8dd58c2ab333136b2681a60123e8c8",
    "question": "As an administrator for a startup utilizing Microsoft Entra ID for identity and access management, you're tasked with implementing mandatory multi-factor authentication for all users. What is the most appropriate step to take?",
    "answer": "The most appropriate step is to activate security defaults in Microsoft Entra ID. This feature provides a basic level of security, including mandatory multi-factor authentication for all users, at no additional cost. The other options, while useful in certain scenarios, do not directly address the requirement of enabling mandatory multi-factor authentication for all users.",
    "options": [
      "Implement Microsoft Entra ID business-to-consumer (B2C).",
      "Upgrade to Microsoft Entra ID Premium P1 for all user accounts.",
      "Establish a new conditional access policy in Microsoft Entra ID.",
      "Activate security defaults in Microsoft Entra ID.",
      "Set up an application proxy in Microsoft Entra ID."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "f05ae8b0e6154bf6249cd9682d199ffe195ad35c45acb2fb6b5cca75df1f35ea",
    "question": "Your organization uses Microsoft Entra ID for identity management. You want to implement a policy that triggers multi-factor authentication (MFA) when a sign-in is deemed risky. Which two actions should you perform?",
    "answer": "The correct actions are to upgrade the staff accounts to Microsoft Entra ID Premium P2 and establish a new risk-based conditional access policy in Microsoft Entra ID. Risk-based policies require access to Microsoft Entra ID Identity Protection, which is an Microsoft Entra ID Premium P2 feature.",
    "options": [
      "Activate application proxy in Microsoft Entra ID.",
      "Upgrade the staff accounts to Microsoft Entra ID Premium P1.",
      "Upgrade the staff accounts to Microsoft Entra ID Premium P2.",
      "Establish a new risk-based conditional access policy in Microsoft Entra ID.",
      "Enable security defaults in Microsoft Entra ID.",
      "Configure the applications to use Microsoft Entra ID business-to-business (B2B)."
    ],
    "answerIndexes": [2, 3],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "c54c77c0ba78077a3bf75613ffbc12ffefbda01c7a73add70205750f40d9b937",
    "question": "In Microsoft Entra ID, what happens to Conditional Access policies when the licenses required for them expire?",
    "answer": "When licenses required for Conditional Access expire, the policies aren't automatically disabled or deleted. They remain active and can be viewed and deleted, but they can no longer be updated.",
    "options": [
      "The policies are automatically disabled.",
      "The policies are automatically deleted.",
      "The policies remain active and can be viewed and deleted, but no longer updated.",
      "Nothing changes for existing conditional policies.",
      "The policies are automatically replaced with default security policies."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "03971d16acfa3907ab8e1abec25290dff8c0bfc243b6a25e99bf2fbc2a99a72f",
    "question": "You are developing an application that uses Microsoft Entra ID for authentication. The application needs to authorize users based on their group membership in the organization. The application should receive this information in the user's token when they authenticate. How would you configure your application in Microsoft Entra ID to achieve this?",
    "answer": "`groupMembershipClaims` is used to emit group claims in the token that the application receives when a user authenticates. The other options do not directly influence the emission of group claims in the token.",
    "options": [
      "Enable the `OAuth2AllowImplicitFlow` attribute in the application manifest.",
      "Set the `requiredResourceAccess` attribute in the application manifest to include the group IDs.",
      "Configure the `groupMembershipClaims` attribute in the application manifest.",
      "Add the group IDs to the `knownClientApplications` attribute in the application manifest."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "2147771bcedcea73bdc2c8d9042875d9a2640d42c8c649b6f7d4c89368925b4f",
    "question": "You are developing an application that uses Microsoft Entra ID for authentication. The application needs to authorize users based on their group membership in the organization. However, due to the large number of groups in your organization, you want to limit the groups included in the user's token to only Security Groups. How would you configure the `groupMembershipClaims` attribute in your application's manifest in Microsoft Entra ID to achieve this?",
    "answer": "Set `groupMembershipClaims` to `SecurityGroup` will ensure that only Security Group memberships are included in the user's token. The other options either include too many groups (\"All\"), no groups (\"None\"), or a different type of group (\"DirectoryRole\").",
    "options": [
      "Set `groupMembershipClaims` to `All`",
      "Set `groupMembershipClaims` to `None`",
      "Set `groupMembershipClaims` to `SecurityGroup`",
      "Set `groupMembershipClaims` to `DirectoryRole`"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "cc41a00f4dd252c082455b52f8c13ae1eb24af6bd3fce8c26ea9c213cc5c2016",
    "question": "In the context of Microsoft Entra ID, which of the following statements correctly describes the difference between AppRoles and Groups?",
    "answer": "AppRoles are specific to an application and are removed with the app registration, while Groups are tenant-specific and persist even after the app is removed.",
    "options": [
      "AppRoles and Groups are both specific to an application and are removed when the application is removed.",
      "AppRoles are specific to an Microsoft Entra ID tenant, while Groups are specific to an application.",
      "AppRoles are specific to an application and are removed with the app registration, while Groups are tenant-specific and persist even after the app is removed.",
      "Groups are removed with the app registration, while AppRoles are tenant-specific and persist even after the app is removed."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "e0d09746abcb938a409a19961edda7e600f6f078c69cc9adf2a5860b55f4cda1",
    "question": "Which of the types of permissions supported by the Microsoft identity platform is used by apps that have a signed-in user present?",
    "answer": "Delegated permissions are used by apps that have a signed-in user present. The app is delegated with the permission to act as a signed-in user when it makes calls to the target resource.  \nApp-only access permissions are used by apps that run without a signed-in user present, for example, apps that run as background services or daemons.",
    "options": [
      "Delegated permissions",
      "App-only access permissions",
      "Both delegated and app-only access permissions"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "712918633b78effee77c2e601294e39a330aba32484848a958129a7e7d53e3ff",
    "question": "Which of the following app scenarios require code to handle Conditional Access challenges?",
    "answer": "Apps performing the on-behalf-of flow require code to handle Conditional Access challenges.  \nThe Integrated Windows authentication flow allows applications on domain or Microsoft Entra ID joined computers to acquire a token silently.",
    "options": [
      "Apps performing the device-code flow",
      "Apps performing the on-behalf-of flow",
      "Apps performing the Integrated Windows authentication flow"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "025002169cec62199fd74f2398d8f75349ae077c9dad9a5c9a5bbf06030b1fde",
    "question": "Which of the following MSAL libraries supports single-page web apps?",
    "answer": "MSAL.js supports single-page applications.",
    "options": ["MSAL Node", "MSAL.js", "MSAL.NET"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "0a3c6997a087437f7545f74efbddd11884fc9e0f5569b6bf83c96d7e71afd043",
    "question": "What does the Microsoft Identity platform assume if the resource identifier is omitted in the scope parameter during requests to authorization, token, or consent endpoints?",
    "answer": "if the resource identifier is omitted in the scope parameter, the resource is assumed to be Microsoft Graph.",
    "options": [
      "The resource is Microsoft Outlook.",
      "The resource is Microsoft Azure.",
      "The resource is Microsoft Graph.",
      "The resource is Microsoft Office 365.",
      "The resource is Key Vault."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "d80797cd6a3fcca55adb92f43fff358c8f93d8eebae2e621fa2a7a4b82612eca",
    "question": "In the context of the Microsoft Identity platform, what is the function of the `scope` parameter?",
    "answer": "It indicates the type of resource being requested.",
    "options": [
      "It sets the access level of the user.",
      "It indicates the type of resource being requested.",
      "It determines the user's authentication mechanism.",
      "It manages the duration of the session."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "deb6b603b2a481cbcf3c297b5016659196b37457bb5bd657581700d635ecec98",
    "question": "What happens if a user attempts to access Microsoft Key Vault with `scope=User.Read` in the Microsoft Identity platform?",
    "answer": "The request fails because `User.Read` is a scope for Microsoft Graph and not valid for Key Vault. Use `https://vault.azure.net/.default` to access Key Vault with app-configured permissions.",
    "options": [
      "The request is successful and the user can access Key Vault data.",
      "The request fails because `User.Read` is intended for a different resource.",
      "The request fails due to insufficient user permissions.",
      "The request is successful but the user the user can only see key names, not the key values."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "dd08b9bd03a6b6da3037ad9a869bc08914f4fce9f912a6690c7b3e243fdd89a7",
    "question": "You are tasked to build an application that will run on mobile devices and web browsers. This application needs to securely obtain tokens in the name of the user, using an Authorization Code flow. Which application builder should you use?",
    "answer": "User-facing apps without the ability to securely store secrets",
    "options": ["`PublicClientApplicationBuilder`", "`ConfidentialClientApplicationBuilder`"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "de9ed7c9ebf896eb6ac282cee8cc32e9ff592fbc4049f046685c0fcca50d3c23",
    "question": "Your company is developing a service application that will run without user interaction. It will make use of the Client Credentials flow to access resources. Which application builder should you use?",
    "answer": "Server-based apps that can securely handle secrets",
    "options": ["`PublicClientApplicationBuilder`", "`ConfidentialClientApplicationBuilder`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "0a2bcb3e31a0e0eadd81f832a40f1a20afbacc8603aaf07cb001d84391a03202",
    "question": "You have an application registered in Microsoft Entra ID and you have configured `appsettings.json` as follows:\n\n```json\n{\n  \"clientId\": \"your-client-id\",\n  \"tenantId\": \"your-tenant-id\"\n}\n```\n\nYour application is currently configured to perform requests to Microsoft Graph API on behalf of a user. Here is the relevant code snippet:\n\n```cs\nprivate static IAuthenticationProvider CreateAuthorizationProvider(string authType)\n{\n    var config = new ConfigurationBuilder()\n        .SetBasePath(System.IO.Directory.GetCurrentDirectory())\n        .AddJsonFile(\"appsettings.json\", false, true).Build();\n\n    List<string> scopes = new List<string>();\n    scopes.Add(clientId + \"/.default\");\n\n    IPublicClientApplication client;\n    string authority = $\"https://login.microsoftonline.com/{config.tenantId}/v2.0\";\n\n    client = PublicClientApplicationBuilder.Create(config.clientId)\n                .WithAuthority(new Uri(authority))\n                .Build();\n\n    return MsalAuthenticationProvider.GetInstance(client, scopes.ToArray());\n}\n```\n\nHowever, your application is evolving into a service application (daemon) that needs to access Microsoft Graph API on behalf of itself, not on behalf of a user. Additionally, due to varying security requirements across different environments, your application needs to support both client secret and client certificate for authentication. The `authType` parameter in the `CreateAuthorizationProvider` function is intended to determine the authentication method, but it is currently unused. Certificate must by obtained by name (`MyCertificate.pfx` - stored in configuration) from KeyVault account `cert-holder`.\n\nHow should you modify the `appsettings.json` and the code to meet these new requirements?",
    "answer": "You should modify `appsettings.json` to include the client secret, path, and password of your client certificate:\n\n```json\n{\n  \"clientId\": \"your-client-id\",\n  \"tenantId\": \"your-tenant-id\",\n  \"clientSecret\": \"your-client-secret\",\n  \"certificateName\": \"MyCertificate.pfx\"\n}\n```\n\nThe code should be modified as follows:\n\n```cs\nprivate static IAuthenticationProvider CreateAuthorizationProvider(string authType)\n{\n    var config = new ConfigurationBuilder()\n        .SetBasePath(System.IO.Directory.GetCurrentDirectory())\n        .AddJsonFile(\"appsettings.json\", false, true).Build();\n\n    List<string> scopes = new List<string>();\n    scopes.Add(clientId + \"/.default\");\n\n    IConfidentialClientApplication client;\n    string authority = $\"https://login.microsoftonline.com/{config.tenantId}/v2.0\";\n\n    ConfidentialClientApplicationBuilder builder = ConfidentialClientApplicationBuilder.Create(config.clientId)\n                .WithAuthority(new Uri(authority));\n\n    if (authType == \"secret\")\n        client = builder.WithClientSecret(config.clientSecret).Build();\n    else if (authType == \"certificate\")\n    {\n        var client = new CertificateClient(new Uri(\"https://cert-holder.vault.azure.net\"), new DefaultAzureCredential());\n        var certificate = await client.GetCertificateAsync(config.certificateName);\n        // Alt: X509Certificate2 certificate = new X509Certificate2(config.certificatePath, config.certificatePassword);\n        client = builder.WithCertificate(certificate).Build();\n    }\n    else\n        throw new ArgumentException(\"Invalid authentication type\");\n\n    return MsalAuthenticationProvider.GetInstance(client, scopes.ToArray());\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Entra ID"
  },
  {
    "id": "e95bc83ef75d7fd562eca2f6daeb5d0e3724f66c1a9c16184a7be8e2fd3bde9d",
    "question": "You have a Microsoft Azure subscription in which you want to deploy a .NET Core v3.1 model-view controller (MVC) application. You add the following authorization policies in your Startup.ConfigureServices method:\n\n```csharp\nservices.AddAuthorization(options =>\n{\n    options.AddPolicy(\"EmployeePolicy\", policy =>\n    {\n        policy.RequireAuthenticatedUser();\n        policy.RequireRole(\"Employee\");\n    });\n    options.AddPolicy(\"ReatailPolicy\", policy =>\n    {\n        policy.RequireAuthenticatedUser();\n        policy.RequireRole(\"Manager\");\n        policy.RequireClaim(\"Department\", \"Sales\");\n    });\n    options.AddPolicy(\"AreaPolicy\", policy =>\n    {\n        policy.RequireAuthenticatedUser();\n        policy.RequireClaim(\"Department\");\n    });\n});\n```\n\nHow should you assign the authorization directives in the code, based on requirements stated in comments? Fill in the blanks:\n\n```cs\n// All users with both the `Employee` and `Manager` roles should always be allowed\n[Authorize(/* ... */)]\npublic class EmployeeController : Controller\n{\n    public IActionResult Index() => Content(\"Employee Controller\");\n}\n\npublic class SalesController : Controller\n{\n    // Users with the `Manager` role and with a `Department` claim of `Sales`\n    [Authorize(/* ... */)]\n    public IActionResult ManageSales() => Content(\"Sales Management\");\n}\n\npublic class DepartmentController : Controller\n{\n    // Users with `Department` claim\n    [Authorize(/* ... */)]\n    public IActionResult ManageDepartment() => Content(\"Department Management\");\n}\n```",
    "answer": "\n\n```cs\n[Authorize(Roles = \"Employee\")]\n[Authorize(Roles = \"Manager\")]\npublic class EmployeeController : Controller\n{\n    public IActionResult Index() => Content(\"Employee Controller\");\n}\n\npublic class SalesController : Controller\n{\n    [Authorize(Policy = \"ReatailPolicy\")]\n    public IActionResult ManageSales() => Content(\"Sales Management\");\n}\n\npublic class DepartmentController : Controller\n{\n    [Authorize(Policy = \"AreaPolicy\")]\n    public IActionResult ManageDepartment() => Content(\"Department Management\");\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Entra ID"
  },
  {
    "id": "e55ccbe57998c849323b67a0a15b567c0c813c4bd50a246781c10b3794d00a37",
    "question": "You are creating an internal portal for staff members to access confidential financial reports. The portal uses Microsoft Entra ID for user authentication. The staff accounts are currently on Microsoft Entra ID Basic licenses. You are tasked with setting up multi-factor authentication (MFA) for a specific team of staff members. Which two steps should you take?",
    "answer": "Conditional access policies require Microsoft Entra ID Premium P1 licenses.  \nSecurity defaults enable MFA for **ALL** users, which does not meat requirenments for specific staff members.",
    "options": [
      "Activate application proxy in Microsoft Entra ID.",
      "Set up the portal to utilize Microsoft Entra ID business-to-consumer (B2C).",
      "Enable security defaults in Microsoft Entra ID.",
      "Upgrade the staff accounts to Microsoft Entra ID Premium P1.",
      "Establish a new conditional access policy in Microsoft Entra ID."
    ],
    "answerIndexes": [3, 4],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "d83cbb7f5d8758d7e252c7f93fab9f7a163a95a2508b106e66f50365c8eb658f",
    "question": "Which component is responsible for directing the user to the location defined by the redirect URI or reply URL, following successful authorization of the app and the granting of an authorization code or access token?",
    "answer": "The authorization Server is the specific component in the OAuth process that directs the user to the designated location (either a redirect URI or reply URL) after the app has been successfully authorized and an authorization code or access token has been granted.",
    "options": ["Client Application", "Authorization Server", "Resource Server", "User's Browser"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "d8c63940b23bf07f9f681ae749d4df30eb04f61e2a0606ac8e72c3632beecac1",
    "question": "In Microsoft Entra ID, what is the requirement for applications to delegate identity and access management?",
    "answer": "In Microsoft Entra ID, all applications must register to delegate identity and access management. This registration process allows the application to be integrated with Microsoft Entra ID, enabling features like authentication, authorization, and more.",
    "options": [
      "Applications can optionally register with Microsoft Entra ID.",
      "Applications must register with a third-party identity provider.",
      "Applications must register with Microsoft Entra ID.",
      "Registration with Microsoft Entra ID is discouraged for client applications."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "1fbc1c4dd682ca9f164372efd7299aa622e10e5e4fa47dce6e85f4b4848ca935",
    "question": "You are developing a web application using ASP.Net Core. The application needs to authenticate users via Microsoft Entra ID and also needs to access Azure Blob Storage on behalf of the authenticated users. What settings should you set in API permissions pane to allow this?",
    "answer": "\n\nMicrosoft Graph: Set `User.Read` to \"Delegate\" to allow application to read the user's profile only when the user is signed in.\n\nAzure Storage: Set `user_impersonation` to \"Delegate\" to allow the application to access Azure Storage only when the user is signed in.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "0e6c0a69fe8920643ebd0e255462d9d91f634c653e4ac1104d847e7d9e83cfee",
    "question": "You are developing a web application using ASP.Net Core. The application needs to authenticate users via Microsoft Entra ID. What value should be set for the `Microsoft Graph > User.Read` permission in the Azure Portal to meet this requirement?",
    "answer": "\"Delegated\" allows the application to authenticate users and read their basic profile on their behalf.",
    "options": ["None", "Application", "Delegated", "Admin"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "4ec05358a1da8a084399310a776645e0e230b5db17a75013e9798f679ee97795",
    "question": "Your ASP.Net Core web application also needs to access Azure Blob Storage on behalf of the authenticated users. What value should be set for the `Azure Storage > user_impersonation` permission in the Azure Portal to meet this requirement?",
    "answer": "\"Delegated\" allows the application to access Azure Blob Storage on behalf of the authenticated users.",
    "options": ["None", "Application", "Delegated", "Admin"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "2f05d2d0934c36aaaa7d9389a895f0d1c75d1e365ea1d9738d0471aa8ac949c8",
    "question": "You are a solutions architect for 'StreamBox,' a video streaming service. The company wants to implement a feature that allows users to quickly sign in using their social media credentials like Instagram, Reddit, or Google, in addition to their regular email accounts. Which Azure service should be utilized to meet this requirement?",
    "answer": "Emtra ID B2C supports authentication through a variety of social media platforms and email accounts, making it the most suitable choice for this scenario.",
    "options": [
      "Microsoft Entra ID B2B",
      "Azure Multi-Factor Authentication",
      "Microsoft Entra ID B2C",
      "Microsoft Entra ID Single Tenant Authentication"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "89ae2fb57ebb06a7ef4f642c21310e846bfc7e857edacc8f62515bf2b2c8b1c0",
    "question": "What is the correct order to integrate authentication and authorization working with the Microsoft identity platform for Web API?\n\nSteps to order:\n1. Store token cache\n2. Register app\n3. Control access to web API (authorization)\n4. Configure app with code sample\n5. Configure permission & call API of choice\n6. Validate access token\n7. Configure secrets & certificates",
    "answer": "The correct sequence is: **Register app → Configure app with code sample → Validate access token → Configure secrets & certificates → Configure permission & call API of choice → Control access to web API (authorization) → Store token cache** (2, 4, 6, 7, 5, 3, 1)",
    "options": ["1, 2, 3, 4, 5, 6, 7", "2, 4, 7, 6, 5, 3, 1", "2, 4, 6, 7, 5, 3, 1", "2, 7, 4, 6, 5, 1, 3"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "abaa8895a544d353e005a590d76243d992020b5dc52163da702740ed485c1352",
    "question": "What is the correct order to protect an API in Azure API Management with Microsoft Entra ID?\n\nSteps to order:\n1. Register the backend API application in Microsoft Entra ID.\n2. Register the web App (website containing summarized results) in Microsoft Entra ID so that it can call the backend API.\n3. Allow permissions between the website app to call the backend API app. Add the configurations in Microsoft Entra ID.\n4. Enable OAuth 2.0 user authorization and add the validate-jwt policy to validate the OAuth token for API calls.",
    "answer": "The correct sequence is: **Register backend API → Register web app → Allow permissions → Enable OAuth 2.0 and validate-jwt policy** (1, 2, 3, 4)",
    "options": ["1, 2, 3, 4", "2, 1, 3, 4", "1, 3, 2, 4", "4, 1, 2, 3"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "5edb26454774201ea0097e221c01a735b58cc75e59b1f99ea2088bc754fc6b55",
    "question": "You need to implement authentication for two applications: a web application and an API. Only users from a specific Entra ID tenant should be able to access both applications. Which of the following account types should you select to meet the security requirements?",
    "answer": "This option restricts access to only users in the specific Entra ID tenant, meeting the requirement.  \nAllowing accounts from any organizational directory would violate the requirements.  \nPersonal Microsoft accounts are related to Skype, Xbox, etc accounts.",
    "options": [
      "Accounts in any organizational directory (Any Microsoft Entra ID directory - Multitenant)",
      "Accounts in any organizational directory (Any Microsoft Entra ID directory - Multitenant) and personal Microsoft accounts",
      "Accounts in this organizational directory only (Single tenant)",
      "Personal Microsoft accounts only"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Entra ID"
  },
  {
    "id": "593e09d1bc9faffcbce87a2b735a21e1791e66808bbb8045b98fcc4747d33a3a",
    "question": "You are tasked with developing a web-based solution that will be hosted on Azure. The application should enforce secure access through Microsoft Entra ID. The requirements are as follows:\n\n- Users should authenticate using their Microsoft Entra ID accounts.\n- The application should offer personalized experiences based on the user's Entra ID group memberships.\n\nHow would you modify the Microsoft Entra ID application manifest file to meet these requirements?\n\n```jsonc\n{\n  \"appId\": \"your-app-id-here\",\n  \"displayName\": \"Your App Name\",\n  // Write related properties here\n}\n```",
    "answer": "Administrator is a role.  \nAdding the user to the admin group doesn’t mean that the Application Administrator’s role is automatically assigned to the user account.",
    "options": [
      "Assign the `Administrator` role",
      "Add to `Administrators` group",
      "Add \"Request Admin\" access policy"
    ],
    "answerIndexes": [0],
    "hasCode": true,
    "topic": "Entra ID"
  },
  {
    "id": "16611a35556548f29f206f155348e08bf5c4193137c9afe8eba16736e0341668",
    "question": "Which of the following event schema properties requires a value?",
    "answer": "The subject property specifies the publisher-defined path to the event subject and is required.  \nEvent Grid will provide Topic.  \nA value isn't required for Data",
    "options": ["Topic", "Data", "Subject"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "b29d783554902563245a494cd139beabddb59b7875b9fc3e5ef9a84298badfe1",
    "question": "You are tasked with creating and deploying an Azure Function App that will handle events from a custom Azure Event Grid topic. To allow subscribers to effectively filter and route these events, which event schema property should you leverage?",
    "answer": "Explanation: The `subject` property is used by subscribers to filter and route events when dealing with custom topics in Azure Event Grid. You can include the path where the event occurred in the `subject` property, enabling subscribers to filter events based on segments of that path. This flexibility allows subscribers to filter events either narrowly or broadly.",
    "options": ["`eventTime`", "`eventType`", "`subject`", "`data`"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "2aaa98c0a63cff6323a40d5ade43fe4b7189df7a3c265f1e7c524206b3f5dab0",
    "question": "You are responsible for monitoring various Azure resources in your organization. You want to set up an Azure Event Grid subscription to specifically receive only failure messages for any type of resource. Which of the following options should you configure in your Event Grid subscription to achieve this?",
    "answer": "In Azure Event Grid, you can configure your subscription to filter events based on `EventType`. This allows you to specifically receive failure messages for any type of Azure resource. By setting up your Event Grid subscription to only pass along events of a certain type, such as failure messages, you can effectively monitor the health of various resources in your organization.",
    "options": ["Subject begins with or ends with", "Advanced fields and operators", "ResourceType", "EventType"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "80d1a2460079c5231f7dc6bad899df33a89a2d5ca81cdc5657edb906497aeabb",
    "question": "You manage Azure Virtual Machines, SQL Databases, and Blob Storage. Each resource type requires a unique action when an event occurs: VMs need restarting, SQL Databases need backups, and Blob Storage needs old log deletion. To route events to specific actions based on the Azure resource involved, which Event Grid option should you configure?",
    "answer": "By using `ResourceTypes`, you can ensure that each type of resource triggers its corresponding action, allowing for targeted and efficient automated responses.",
    "options": ["Subject begins with or ends with", "Advanced fields and operators", "ResourceType", "EventType"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "c24c46ee3eeca2a97686df1137336f130400cc34855d5b109d671c5512638d10",
    "question": "You are managing an Azure Blob Storage account that contains multiple containers. One of these containers is critical and stores time-sensitive data. You need to set up an alerting mechanism to receive messages whenever objects are added to this specific container. To receive messages specifically when objects are added to a particular container in Azure Blob Storage, which Event Grid option should you configure?",
    "answer": "By using \"Subject begins with or ends with,\" you can precisely target events related to the specific container in Azure Blob Storage, ensuring that you receive messages only when objects are added to that container. This allows for targeted alerting and monitoring.",
    "options": ["Subject begins with or ends with", "Advanced fields and operators", "ResourceType", "EventType"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "05a5a6bb3aab25977b6657599034cfcde3635d6d03594904c435e033c34fe7d9",
    "question": "Which of the following Event Grid built-in roles is appropriate for managing Event Grid resources?",
    "answer": "The Event Grid Contributor role has permissions to manage resources.  \nThe Event Grid Subscription Contributor role has permissions to manage subscription operations.  \nThe Event Grid Data Sender role has permissions to send events to topics.",
    "options": ["Event Grid Contributor", "Event Grid Subscription Contributor", "Event Grid Data Sender"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "e7df6a2171af1babfe82a5d8b2a55986c6616e490619c77060291cde8ffbd747",
    "question": "You are trying to send a 130KB event from Event Hub to Event Grid. Which of the following statements are true:",
    "answer": "Only events up to 64KB are covered by SLA.  \n`lastTimeModified` is a property for Azure Storage.  \n`413 Payload Too Large` happens only for events over 1MB.  \nMath.roof(130 / 64) = 3",
    "options": [
      "Event is covered by the Service Level Agreement (SLA)",
      "Event contains `lastTimeModified` property",
      "Event contains Capture file URL property",
      "You'll receive `413 Payload Too Large` error",
      "You be charged for 3 separate events"
    ],
    "answerIndexes": [2, 4],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "b4f375e05a9abd5318b7c5095c9bea56359b15873eef21487a8d9ed0539eccce",
    "question": "What best describes the optimistic batching behavior in Azure Event Grid?",
    "answer": "Optimistic batching results in smaller batch sizes at low event rates.  \nEvent Grid works on _All-or-None_ policy, no partial delivery allowed when batching.  \nIt's not necessary to specify both the settings (Maximum events per batch and Approximate batch size in kilobytes).",
    "options": [
      "Optimistic batching ensures that the batch size is always equal to the requested maximum events per batch.",
      "Optimistic batching respects policy settings on a best-effort basis, often leading to smaller batch sizes at low event rates.",
      "Optimistic batching operates with flexible semantics, allowing partial success of a batch delivery.",
      "Optimistic batching is responsible for handling events that cannot be delivered to the endpoint.",
      "Optimistic batching requires both the Maximum events per batch and Approximate batch size in kilobytes to be specified."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "354b603c5632dc47ee6f4a933b9683e2d8122c4d24a2b3a4118c66f91480d03d",
    "question": "What command you need to run to enable your Azure subscription to send events to Event Grid?",
    "answer": "It will take some minutes. You need to run this only once per subscription.",
    "options": [
      "`az provider register --namespace Microsoft.EventGrid`",
      "`az group create --name Microsoft.EventGrid`",
      "`az eventgrid topic create --name \"ServiceBus, BlobStorage\"`",
      "`az subscription --enable Microsoft.EventGrid`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "22fb1130088bc8e3ff0ccf454f9be066fbfea0c62ce1cb2dea2acd910c81a3de",
    "question": "Which event format can be used to represent events in a standardized way across different cloud providers and platforms?",
    "answer": "CloudEvent is a standardized specification designed to provide interoperability across services, platforms, and systems. It can be used across different cloud providers and platforms, unlike EventGridEvent, which is specific to Azure.",
    "options": ["EventGridEvent", "CloudEvent", "AzureBlobEvent", "CustomEvent"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "f61f20cdb52d8834e4600f6ee7bec58b24d11020e8bfa0a65a3408b8de28b54e",
    "question": "If you are working specifically within the Azure ecosystem and want to take advantage of Azure-specific features, which event format would you likely use?",
    "answer": "EventGridEvent is specific to Azure Event Grid and is designed to work seamlessly with Azure services. It includes additional features specific to Azure, like support for Event Domains, making it the suitable choice for Azure-specific implementations. CloudEvent, on the other hand, is a more generalized standard and doesn't include Azure-specific features.",
    "options": ["CustomEvent", "CloudEvent", "EventGridEvent", "AzureQueueEvent"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "52bd99e99d1ac5c5a5d828d680c86d928f3960c83649ea382a35eddad849700d",
    "question": "What is the minimum number of subscribers a publisher in Event Grid must have?",
    "answer": "Publishers emit events, but have no expectation about how the events are handled.",
    "options": ["0", "1", "3", "5"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "6ae5429f60acdd4cbc5b411b0ef29ed9f370f3833fd8b5d730996de412a7f202",
    "question": "Does Event Grid allow Azure Functions to respond to Azure Blob storage events, such as generating thumbnails for newly uploaded images?",
    "answer": "It's a valid use case.",
    "options": ["Yes", "No"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "9a820a9d89feac539df924d5651cbe0d8e4d746b0ff12e0d37ba5485524cbe86",
    "question": "Fill in \"XXX\" and \"YYY\" for this Event Grid filter:\n\n```jsonc\n{\n  \"XXX\": [\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"YYY\": [\"container1\", \"container2\"],\n    },\n  ],\n}\n```",
    "answer": "\n\n```jsonc\n{\n  \"advancedFilters\": [\n    {\n      \"operatorType\": \"StringContains\",\n      \"key\": \"Subject\",\n      \"values\": [\"container1\", \"container2\"],\n    },\n  ],\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Event Grid"
  },
  {
    "id": "9a41b26de9044d5850b7ff7df22fe081907d901b986ddf466e38c8e35eef3002",
    "question": "Configure Azure Event Grid service to send events to an Azure Event Hub instance.\n\n```ps\ntopicName=\"<Topic_Name>\"\nlocation=\"<Location>\"\nresourceGroupName=\"<Resource_Group_Name>\"\nnamespaceName=\"<Namespace_Name>\"\neventHubName=\"<Event_Hub_Name>\"\n\n# Code here\n```",
    "answer": "\n\n```ps\ntopicName=\"<Topic_Name>\"\nlocation=\"<Location>\"\nresourceGroupName=\"<Resource_Group_Name>\"\nnamespaceName=\"<Namespace_Name>\"\neventHubName=\"<Event_Hub_Name>\"\n\naz eventgrid topic create --name $topicName --location $location --resource-group $resourceGroupName\n\n# az eventgrid topic show --name $topicName --resource-group $resourceGroupName --query \"{endpoint:endpoint, primaryKey:primaryKey}\" --output json\n\n# Create a namespace\naz eventhubs namespace create --name $namespaceName --location $location --resource-group $resourceGroupName\n\n# Create an event hub\naz eventhubs eventhub create --name $eventHubName --namespace-name $namespaceName --resource-group $resourceGroupName\n\ntopicId=$(az eventgrid topic show --name $topicName --resource-group $resourceGroupName --query \"id\" --output tsv)\nhubId=$(az eventhubs eventhub show --name $eventHubName --namespace-name $namespaceName --resource-group $resourceGroupName --query \"id\" --output tsv)\n\n# Link the Event Grid Topic to the Event Hub\naz eventgrid event-subscription create \\\n  --name \"<Event_Subscription_Name>\" \\\n  --source-resource-id $topicId \\\n  --endpoint-type eventhub \\\n  --endpoint $hubId\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Event Grid"
  },
  {
    "id": "ca5e6266a5dad4029095f66bba19e01447e2b744df1a35cdb00797701ceba6b0",
    "question": "You need to enable a third-party SaaS application to send events directly to your Azure subscription for processing by your custom applications. What type of Event Grid topic should you configure to achieve this?",
    "answer": "Partner Topics are designed specifically for enabling third-party services or SaaS applications to push events into your Azure subscription. These events can then be processed by Azure Functions, Logic Apps, or other subscribers to the topic.  \nSystem Topic: This is for Azure services that natively emit events, like Blob Storage or Event Hubs. You use system topics when you’re dealing with Azure-native resources.  \nCustom topics are for your own applications to send events into Event Grid—not for third-party SaaS. If the source isn’t a partner but something you built, this would be the choice.  \nEvent Hub Topic: Event Hub isn’t even part of Event Grid’s topic system. It’s used for high-throughput streaming of telemetry or event data, not the discrete events Event Grid handles.",
    "options": ["System Topic", "Partner Topic", "Custom Topic", "Event Hub Topic"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Grid"
  },
  {
    "id": "ed3674cc865b1336a6f5e5f7042c025ef721f5f7cc882336544e0da65eb280ae",
    "question": "Which of the following Event Hubs concepts represents an ordered sequence of events that is held in an Event Hubs?",
    "answer": "A partition is an ordered sequence of events that is held in an Event Hub.  \nA consumer group is a view of an entire Event hub.  \nAn Event Hub producer is a type of client.",
    "options": ["Consumer group", "Partition", "Event Hubs producer"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "2fd9fe6673919e382a0a7b248a571fc094c425a21ef470a37ea15fe6628f9a03",
    "question": "Which of the following options represents when an event processor marks or commits the position of the last successfully processed event within a partition?",
    "answer": "Checkpointing is a process by which an event processor marks or commits the position of the last successfully processed event within a partition.  \nScale covers the number of consumers and taking ownership of reading partitions.  \nIncrease or reduce the consumers dynamically. The pool of consumers can rebalance the number of partitions they own, to share the load with the newly added consumers.",
    "options": ["Checkpointing", "Scale", "Load balance"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "363186373e9645777a3b5aa73f38690500b5fe998764998f1d452a466bdeeabf",
    "question": "Which of the following is a valid EventHub connection string?",
    "answer": "`Endpoint=sb://...`",
    "options": [
      "`Endpoint=sb://example-namespace.servicebus.windows.net/;SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey`",
      "`sb://example-namespace.servicebus.windows.net/;SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey`",
      "`https://example-namespace.servicebus.windows.net/;SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey`",
      "`Endpoint=https://example-namespace.servicebus.windows.net/;SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "59f46921e443bb1c49e7e246abfc046be3568a18394f35b15fa7089341bd511e",
    "question": "Specify the correct parameter for `EventHubProducerClient.SendAsync()` if `text` is of type `string`.",
    "answer": "Prefer using `byte[]` with `EventData`.",
    "options": [
      "No parameter",
      "`new EventData(Encoding.UTF8.GetBytes(text))`",
      "`new EventData(text)`",
      "`Encoding.UTF8.GetBytes(text)`",
      "`text`"
    ],
    "answerIndexes": [1, 2],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "76700c26bf869b197d60b95f7a9081b432d0243e4979d930d8512a5b09451c5c",
    "question": "Which class allows you to send messages in Event Hub?",
    "answer": "`EventHubProducerClient`",
    "options": ["`EventHubProducerClient`", "`EventHubConsumerClient`", "`EventHubSender`", "`EventHub`"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "4556c12e51b91dfa4bd280114caab95217169f1a9776a561a234066422c5f464",
    "question": "In a distributed temperature monitoring system, various sensors are sending data to an Event Hub. You need to guarantee that the temperature readings from all sensors are processed in the exact sequence they were sent. How can you achieve this?",
    "answer": "You should use a common partition key for all sensors. By using the same partition key, you ensure that the readings from different sensors are kept together in the same partition and processed in the order in which they arrived.",
    "options": [
      "Use a common partition key for all sensors.",
      "Utilize the same EventHubProducerClient instance across all sensors.",
      "Switch to Premium tier",
      "Implement a buffering mechanism within the Event Hub."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "c7cc2c5c6a5482a87b46fb9aa0f31d47122ac8ab37a2d4de6f8ce08b2b88ce6c",
    "question": "How does Event Hubs Capture handle scalability?",
    "answer": "Event Hubs Capture scales automatically with throughput units",
    "options": [
      "It requires manual scaling with throughput units",
      "It scales automatically with throughput units",
      "It does not scale",
      "It scales based on the number of partitions"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "0540a2cb7bfcf5cf9634b62baca1c70e4c43e1e24d0fecff0fca28afd57a06fb",
    "question": "Which of the following is true about the storage accounts in Event Hubs Capture?",
    "answer": "Storage accounts can be in the same region as your event hub or in another region.",
    "options": [
      "They must be in the same region as your event hub",
      "They can be in any region",
      "They must be in a different region from your event hub"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "a9286867e5d820ff5a3d4ad2aadc189c61b46f6d92a4524078072adf01aa1a33",
    "question": "What is the format in which captured data is written by Event Hubs Capture?",
    "answer": "Captured data is written in Apache Avro format",
    "options": ["JSON", "XML", "Apache Avro", "CSV", "YAML"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "5426a5c53bfbc88877cbd9d51151654490936cdb78d50b9767aee5d362a6f0bf",
    "question": "A global manufacturing company is deploying thousands of sensors across its facilities. Each sensor generates a continuous stream of data that needs to be captured automatically. The data must be stored in Azure Data Lake V2 storage and organized by sensor type. The solution must also allow processing real-time and batch-based pipelines on the same stream. Which Azure service would be the most suitable for implementing this solution?",
    "answer": "Event Hubs Capture allows automatic capturing of streaming data, can process real-time and batch-based pipelines on the same stream, and scales automatically with throughput units. It also supports storing the captured data in Azure Data Lake Storage.",
    "options": ["Azure IoT Hub", "Azure Stream Analytics", "Event Hubs Capture", "Azure Data Lake Analytics"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "fa5a9f778656ca009ba4564b5f8447c62a070469feae7de3f46a46540f27485a",
    "question": "Which should you choose if you publish events frequently and you want higher throuput and lower latency?",
    "answer": "AMQP offers higher performance for frequent publishers.  \nTier does not affect performance.",
    "options": [
      "Use HTTPS protocol",
      "Use HTTP protocol",
      "Use AMQP protocol",
      "You need Premium plan to achieve better performance",
      "Event Hub is optimized for high throuput, lower latency scenarios by default"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "ff3fb900864d3e94c77cbf03ef42bb6bc4ef89a4633f6b040e9795f47b7e3413",
    "question": "Which protocol should you choose if you want to publish a single event, with minimal network cost?",
    "answer": "AMQP requires a persistent bidirectional socket plus TLS or SSL/TLS, resulting in higher initial network costs.",
    "options": [
      "Use HTTPS protocol",
      "Use HTTP protocol",
      "Use AMQP protocol",
      "You need Dedicated plan to mimimize costs",
      "Network cost is the same regardless of the protocol or tier"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "6acc89314491579f946dd72b68965d5c9cbdb5726af02c90f6f36794202c52ab",
    "question": "A healthcare organization is developing a real-time patient monitoring system. The system will monitor vital signs from patients in 5 different wards. The data from the sensors in each ward is sent to Azure Event Hubs for real-time analytics. Three different teams within the organization—Emergency Response, Data Analytics, and Hospital Administration—consume this data through Azure Web Apps for various purposes. You are tasked with setting up the Azure Event Hub for this system. Your goal is to optimize for high data throughput and low latency. How many partitions should you configure in the Azure Event Hub?",
    "answer": "To achieve the highest data throughput, it's advisable to allocate an individual partition for each ward. Given that the partitioning should be aligned with the source of incoming data, alternative approaches would be incorrect.  \nBonus answer: Ward. Because we use it for partitioning.",
    "options": [
      "3 Partitions",
      "5 Partitions",
      "8 Partitions",
      "12 Partitions",
      "Bonus question: What would you use as the partition key?"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "b0a049c752fffc709ed69d62e69f4d97af732d64fbbd80b1f1f94b892f0f8402",
    "question": "Your team is tasked with creating an application to collect data from various IoT devices. You've decided to use Azure Event Hubs for event ingestion. To meet the project requirements, you need to store these events in Azure Blob Storage. Which Azure Event Hubs feature should you utilize to save the data in Azure Blob Storage?",
    "answer": "To store data in Azure Blob Storage, you would use the Azure Event Hubs Capture feature. Azure Event Hubs Capture allows for the automatic saving of event data to Azure Blob Storage.",
    "options": ["Throughput Units", "Partition Keys", "Event Hubs Capture", "Event Streams"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "baedce69e58aa5748c8e7abd8ba797971f7ce8a70f1b0cc4b6e6ac070d93a399",
    "question": "Complete the following command: `az eventhubs ??? authorization-rule keys list --resource-group MyResourceGroupName --namespace-name MyNamespaceName --eventhub-name MyEventHubName --name SomeName`. What should you put instead of `???`?",
    "answer": "The command is to get connection string for a specific event hub within a namespace.",
    "options": ["`namespace`", "`eventhub`", "`get`", "`show`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "0669c264a1240dcca1d79259833b7b0ecaabd70b3c707e0024a905aa68dd4071",
    "question": "Complete the following command: `az eventhubs ??? authorization-rule keys list --resource-group MyResourceGroupName --namespace-name MyNamespaceName --name SomeName`. What should you put instead of `???`?",
    "answer": "The command is to get connection string for a namespace.",
    "options": ["`namespace`", "`eventhub`", "`get`", "`show`"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Event Hubs"
  },
  {
    "id": "7002bac275539eaa9e7823a48bb6ad576182eb832f791bd0997f3b18a3297b8e",
    "question": "An organization wants to implement a serverless workflow to solve a business problem. One of the requirements is the solution needs to use a designer-first (declarative) development model. Which of the choices below meets the requirements?",
    "answer": "Azure Logic Apps enables serverless workloads and uses a designer-first (declarative) development model.",
    "options": ["Azure Functions", "Azure Logic Apps", "WebJobs"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "be5b5994b2dd8f7c8503f2ab402353fb232ac1bd2a3ecf3cd70758625aae05ff",
    "question": "What is a key benefit of the Flex Consumption plan in Azure Functions hosting options?",
    "answer": "The Flex Consumption plan provides high scalability, compute choices, virtual networking, and pay-as-you-go billing.  \nFully predictable billing and manual scale instances is a feature of the Dedicated plan.  \nPackaging of custom libraries with function code is a feature of Container Apps, not the Flex Consumption plan.",
    "options": [
      "It provides fully predictable billing and manual scale instances.",
      "It offers high scalability with compute choices, virtual networking, and pay-as-you-go billing.",
      "It allows for the packaging of custom libraries with function code."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "6a11034993b6b5a5fbe9b2605180ae3eb97e31f848685434893cf1c6abbc2112",
    "question": "What is the maximum number of instances for a function app on a Consumption plan in Windows?",
    "answer": "The maximum number of instances for a function app on a Consumption plan in Windows is 200.  \n300 is the maximum number of instances for Container Apps.  \n100 is the maximum number of instances for a function app on a Consumption plan in Linux.",
    "options": ["300", "100", "200"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "98b5be8ff0b5d252a4dd972afa66597d83df703602cf0fafe4dafcc3a68f9991",
    "question": "You need your Azure Function to use different connection strings when working locally, and when working on production. How to achieve that?",
    "answer": "When running locally, Azure Functions read configuration values from `local.settings.json`, which emulates the cloud-based `Application Settings`. In Azure, the function reads directly from `Application Settings` configured in the Function App. This separation allows you to manage environment-specific settings without modifying the codebase.",
    "options": [
      "Store connection strings in `function.json`",
      "Store connection strings in Key Vault and read them in your function",
      "Create two versions of the function, one for production and one for local development",
      "Store connection string into `host.json` file",
      "Store connection string into `local.settings.json` file"
    ],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "a96b4f10b51b5aca79ccef6bb8159f15f09c0676353f5c37c536854224c86408",
    "question": "You’re deploying an Azure Function written in C# script. It needs to connect to an Azure SQL database in production. Where should the connection string be stored for the deployed app?",
    "answer": "In production, secrets like connection strings must be stored securely, and Application Settings (under Configuration in the Azure Function's app settings) is the correct place. Azure automatically makes these values available as environment variables.\n\n- `host.json`: Defines global runtime settings (e.g., logging, timeouts). Not for secrets like connection strings.\n- `function.json`: only references configuration values from `Application Settings`; it does not define or store secrets.\n- `local.settings.json`: Can be used for connection string, but it's used for local development only. Ignored in deployment; never reaches Azure.",
    "options": ["`host.json`", "`function.json`", "`local.settings.json`", "`Application Settings` in Azure Portal"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "619b175f285e29c4d1ae023934068deefcadd7f002f472450271125bebc62815",
    "question": "You are tasked with altering the `host.json` file to adjust runtime settings for a function app, aiming to keep the process as straightforward as possible. What method would you utilize?",
    "answer": "Azure Portal allows for direct and simple changes to the host.json file in the Function App settings, reducing effort needed.  \nVisual Studio Code could be used to edit the host.json file, but this requires downloading the project locally, making changes, and then re-deploying it, which increases effort.  \nAzure CLI and Azure Powershell does not include the necessary commands to modify `hosts.json`  \nAzure Resource Manager (ARM) templates can be used to manage Azure resources, but they do not allow direct editing of the `host.json` file for a Function App, making it an unsuitable choice.",
    "options": [
      "Azure Portal",
      "Visual Studio Code or other text editor",
      "Azure Powershell cmdlets",
      "Azure CLI commands",
      "Azure Resource Manager (ARM) template"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "6269e5096287746860611eaa44c41f8611b22be62c81f4fd90c1259046c37114",
    "question": "Which of the following is a benefit of stateless design in cloud applications?",
    "answer": "Stateless applications are easier to scale horizontally. Because each request is processed independently, you can easily add more instances of the application to handle more requests.",
    "options": [
      "Stateless applications are easier to scale horizontally.",
      "Stateless applications are more secure because they don't use databases.",
      "Stateless applications are faster because they use less memory.",
      "Stateless applications are cheaper because they don't require any storage."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "60d062909d8de68417580bea4d05a40295516c9d02086b891e40c058add968d6",
    "question": "You are developing a serverless function that needs to maintain some information between requests. What is a good way to manage this state?",
    "answer": "Store the state in a database or other external storage. Because serverless functions are stateless, they do not maintain any information between requests. To manage state, you can store it in an external storage system like a database.",
    "options": [
      "Store the state in a global variable in the function.",
      "Store the state in a cookie.",
      "Store the state in a database or other external storage.",
      "Stateless functions cannot maintain state."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "988408f9f260f063695280b88b5ee1a281efd4131d15e255552eafe56c933ef6",
    "question": "You are developing a web application that needs to maintain user sessions. The application is deployed on multiple servers for load balancing. What is a common issue you might encounter due to the stateless nature of HTTP, and how can it be addressed?",
    "answer": "Users might lose their session data when their requests are handled by different servers. This can be addressed by implementing sticky sessions or by storing session data in a shared database.",
    "options": [
      "Users might lose their session data when their requests are handled by different servers. This can be addressed by implementing sticky sessions or by storing session data in a shared database.",
      "Users might not be able to make multiple requests at the same time. This can be addressed by implementing asynchronous request handling.",
      "Users might not be able to access the application if one of the servers goes down. This can be addressed by implementing a failover mechanism.",
      "Users might experience slow response times if the servers are located in different geographical locations. This can be addressed by implementing a content delivery network (CDN)."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "e8f5651f097ae68ece6160ebc9b64ccd2330e95a5a01a45bd320fc9b50756a76",
    "question": "Your web application is deployed across multiple servers for load balancing. If one of the servers goes down, what strategy can be implemented to ensure high availability and prevent users from experiencing downtime?",
    "answer": "Implementing a Failover Mechanism. A failover mechanism can automatically redirect traffic to a backup server if the primary server fails, ensuring high availability.",
    "options": [
      "Implementing a Content Delivery Network (CDN)",
      "Implementing Sticky Sessions",
      "Implementing a Failover Mechanism",
      "Implementing Asynchronous Request Handling"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "f00ec7bd0abf9d76ea656ba9ff35b18e878e8920fc60db7eee52284e28811433",
    "question": "Which Azure Functions hosting plan is most suitable for always-on workloads that require full control over scaling rules and consistently allocated infrastructure resources?",
    "answer": "The Dedicated Plan runs on the Azure App Service Plan, which is ideal for always-on workloads, supports manual and rule-based autoscaling, and provides predictable resource allocation. It’s best when your functions are part of a larger app that’s running continuously and you want full control over the hosting environment.",
    "options": ["Dedicated Plan", "Premium Plan", "Container Apps Plan"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "0262642de849aa5c0e71883747a4a87e592cb7c91a3219331916ab7c638f348c",
    "question": "Which Azure Functions hosting plan is most suitable for always-on workloads that require event-driven auto-scaling?",
    "answer": "Both Premium and Dedicated plans supports always-on workloads, but only Premium supports event-driven auto-scaling (Dedicated supports only manual or rule-based auto-scaling).",
    "options": ["Dedicated Plan", "Premium Plan", "Consumption Plan"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "55af713473afea047e66e8620933a7aab0af4a63102a266d5e04dced30ef434f",
    "question": "You are assigned to create a function app in Azure which has to automatically scale based on the workload and you don't want to worry about capacity planning. Write down the Azure CLI commands you would use to create such a function app starting from scratch, assuming no resources or prerequisites have been set up.\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\n# Variables\nresourceGroupName=\"MyResourceGroup\"\nstorageName=\"mystorageaccount\"\nfunctionAppName=\"myfunctionappconsumption\"\nlocation=\"westus2\"\n\n# Create a resource group\naz group create --name $resourceGroupName --location $location\n\n# Create an Azure Storage Account\naz storage account create --name $storageName --location $location --resource-group $resourceGroupName --sku Standard_LRS\n\n# Create a consumption plan function app\naz functionapp create --resource-group $resourceGroupName --consumption-plan-location $location --runtime node --functions-version 3 --name $functionAppName --storage-account $storageName\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "3b92bc31e8f8fab34b693140fdca941c8738cdb17bbe2d32493813edfcc4a5ef",
    "question": "Your team is working on an application that requires data to be processed immediately. You decide Azure Functions would be an excellent fit for this application. Provide a list of Azure CLI commands you would execute to set up the function app from the ground up, given that there are no pre-existing resources or configurations.\n\n```ps\n# Code here\n```",
    "answer": "`\"data to be processed immediately\"` requires pre-warmed instances, supported in `Premium` plan.\n\n```ps\n# Variables\nresourceGroupName=\"MyResourceGroup\"\nstorageName=\"mystorageaccount\"\nplanName=\"MyPremiumPlan\"\nfunctionAppName=\"myfunctionapppremium\"\nlocation=\"westus2\"\n\n# Create a resource group\naz group create --name $resourceGroupName --location $location\n\n# Create an Azure Storage Account\naz storage account create --name $storageName --location $location --resource-group $resourceGroupName --sku Standard_LRS\n\n# Create a premium plan\naz functionapp plan create --name $planName --resource-group $resourceGroupName --location $location --sku EP1\n\n# Create a function app with the premium plan\naz functionapp create --resource-group $resourceGroupName --plan $planName --name $functionAppName --storage-account $storageName --runtime node --functions-version 3\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "1d8d66445e721c8d8a0bcb38e6207f565ffbbe07eecabc795f0c1be1ad6d9006",
    "question": "You are asked to create a function app in Azure where you want to have dedicated compute resources and you don't want the function app to be paused during periods of inactivity. How would you set up such an Azure function app using Azure CLI? Assume you are starting from scratch and you need to take care of any required setup or prerequisites in your CLI commands.\n\n```ps\n# Code here\n```",
    "answer": "Dedicated plan with `\"Always on\"` set.\n\n```ps\n# Variables\nresourceGroupName=\"MyResourceGroup\"\nstorageName=\"mystorageaccount\"\nplanName=\"MyAppServicePlan\"\nfunctionAppName=\"myfunctionappservice\"\nlocation=\"westus2\"\n\n# Create a resource group\naz group create --name $resourceGroupName --location $location\n\n# Create an Azure Storage Account\naz storage account create --name $storageName --location $location --resource-group $resourceGroupName --sku Standard_LRS\n\n# Create an App Service plan\naz appservice plan create --name $planName --resource-group $resourceGroupName --location $location --sku S1\n\n# Create a function app with the App Service plan\naz functionapp create --resource-group $resourceGroupName --plan $planName --name $functionAppName --storage-account $storageName --runtime node --functions-version 3\n\n# Set the function app to always be on\naz functionapp config set --name $functionAppName --resource-group $resourceGroupName --always-on true\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "e527888696162e787f10651de92264986931c6d1aed103d2e28f65a3571fc1a6",
    "question": "What is the difference in terms of cost between `Consumption` and `Premium` plans and any of the App Service plans?",
    "answer": "`Consumption` and `Premium` plans can be more cost-efficient if you have sporadic usage patterns. App Service plans could be more predictable in terms of cost and potentially cheaper for continuous heavy usage.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "2a28f3d3059ba682a084bc1a9f4440113c201c546da43795e1cc530278ff90c6",
    "question": "Which of the following plans does not support VNet?",
    "answer": "VNet is supported only in Premium plan, Standard App Service plan, Premium App Service plan, Isolated App Service plan",
    "options": [
      "Premium plan",
      "Standard App Service plan",
      "Premium App Service plan",
      "Isolated App Service plan",
      "Free App Service plan",
      "Shared App Service plan",
      "Basic App Service plan"
    ],
    "answerIndexes": [4, 5, 6],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "293f12ee5a10f1f85f4567c3c0a0342faf5924d66ac8327cae373962bf910398",
    "question": "Which of the following choices is required for a function to run?",
    "answer": "A trigger defines how a function is invoked and a function must have exactly one trigger.",
    "options": ["Binding", "Trigger", "Both triggers and bindings"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "82e5254e527437aa58b4d264c95ead80fd6da9ef4b23960eb3d5623738448a59",
    "question": "Which of the following choices supports both the `in` and `out` direction settings?",
    "answer": "Input and output bindings use `in` and `out`.",
    "options": ["Bindings", "Trigger", "Connection value"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "84b9af7c629c2cb0812429a24c42c10da014d392152de1dbed35fbb646dd1339",
    "question": "What are the similarities and the differences between input and output binding? Give example for blob storage.",
    "answer": "Both access an external storage object, the difference is input bindings read from it, while output write to it\n\n- `Blob(\"samples-workitems/{queueTrigger}\", FileAccess.Read)` is an input binding (`Read`)\n- `Blob(\"samples-workitems/{queueTrigger}\", FileAccess.Write)` is an output binding (`Write`)",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "f518d6c641b7ab6ccd7af0ae0df55ae89a568feaef01bc234d9c5514570dc36c",
    "question": "Write an Azure function that uses HTTP trigger and takes `id` from the url and `text` from request body, then appends the `text` to existing blob with id `id` and logs its new content. Use `Steam` for the blob.\n\n```cs\n[FunctionName(\"BlobAppend\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Code here\n}\n```",
    "answer": "\n\n```cs\n[FunctionName(\"BlobAppend\")]\npublic static void Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)] HttpRequest req,\n    [Blob(\"{id}\", FileAccess.ReadWrite)] Stream myBlob,\n\n    ILogger log)\n{\n    var id = req.Query[\"id\"];\n    var requestBody = new StreamReader(req.Body).ReadToEnd();\n    dynamic data = JsonConvert.DeserializeObject(requestBody);\n\n    // Read existing content from the blob\n    var reader = new StreamReader(myBlob);\n    var content = reader.ReadToEnd() + data?.text;\n    var contentBytes = System.Text.Encoding.UTF8.GetBytes(content);\n\n    // Write to the blob\n    myBlob.Write(contentBytes, 0, contentBytes.Length);\n    myBlob.Position = 0;\n\n    log.LogInformation($\"Blob: {id}\\n Content:{content} \\n Size: {myBlob.Length} bytes\");\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "383da4a17e3690c0c29fc32afcbc13968330610be2678f47c8266c26ddffb16a",
    "question": "You're working for an e-commerce company that has high traffic and they're dealing with large amounts of customer orders. The company wants to keep a track of every order received in real-time and then send an automated confirmation email to the customer. Which Azure service would you choose to trigger your Azure function? Fill in the triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"ProcessOrder\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process the incoming order\n    // Prepare an email message\n    // Assign the message to the 'message' output binding to send email\n}\n```",
    "answer": "In this case, the Azure function could be triggered by the Service Bus where each order corresponds to a message in a Service Bus Queue or Topic. The function could then use the SendGrid output binding to send a confirmation email to the customer.\n\n```cs\n[FunctionName(\"ProcessOrder\")]\npublic static void Run(\n    [ServiceBusTrigger(\"<queue-name>\", Connection = \"ServiceBusConnectionString\")]\n    Order order,\n\n    [SendGrid(ApiKey = \"SendGridApiKey\")]\n    out SendGridMessage message,\n\n    ILogger log)\n{\n    // Process the incoming order\n    // Prepare an email message\n    // Assign the message to the 'message' output binding to send email\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "b2146660a1ff09928d2bb774a4d864057bff2b9d9b3faa8ecb0530ba344eff4f",
    "question": "You've been assigned a project where your task is to design an Azure function that activates automatically every 10 minutes. This function has to interact with a vast amount of unstructured data such as text or binary data from a source, perform necessary computations, and subsequently modify another source with the outcome. Now, in the context of this task, provide the appropriate triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"ProcessData\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Read data from input\n    // Process the data\n    // Write the processed data to output\n}\n```",
    "answer": "To accomplish this, you could utilize a Timer Trigger for the Azure Function, enabling it to initiate every 10 minutes. This function could be configured with an input binding to retrieve data from the data source, in this case, Blob Storage, which is apt for handling unstructured data. Once the data processing is completed, an output binding to Blob Storage can be used to deposit the processed data.\n\n```cs\n[FunctionName(\"ProcessData\")]\npublic static void Run(\n    [TimerTrigger(\"0 */10 * * * *\")] TimerInfo myTimer,\n\n    [Blob(\"<container-name>/<blob-name>\", FileAccess.Read, Connection = \"BlobStorageConnectionString\")]\n    Stream inputData,\n\n    [Blob(\"<container-name>/<blob-name>\", FileAccess.Write, Connection = \"BlobStorageConnectionString\")]\n    Stream outputData,\n\n    ILogger log)\n{\n    // Read data from input\n    // Process the data\n    // Write the processed data to output\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "6c890d37398e821e6405ea00e2c3b7dff1a033503eb8c6e0d8fcc180502ede74",
    "question": "A company requires a function that can process incoming files automatically. They upload images regularly into a storage service, and they want those images to be resized automatically. Which Azure service could you use to trigger this function? Fill in the triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"ResizeImage\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    string name,\n    ILogger log)\n{\n    // Read the input image\n    // Resize the image\n    // Write the resized image to output image\n}\n```",
    "answer": "This scenario is perfect for a Blob Storage trigger. The function could be triggered whenever a new blob (image) is added to the Blob Storage. Inside the function, you could use an image processing library to resize the images.\n\n```cs\n[FunctionName(\"ResizeImage\")]\npublic static void Run(\n    [BlobTrigger(\"<container-name>/{name}\", Connection = \"BlobStorageConnectionString\")]\n    Stream inputImage,\n\n    [Blob(\"<container-name>/{name}\", FileAccess.Write, Connection = \"BlobStorageConnectionString\")]\n    Stream outputImage,\n\n    string name,\n    ILogger log)\n{\n    // Read the input image\n    // Resize the image\n    // Write the resized image to output image\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "a1aa75e2f897191093e0dcf930fae96623cbeac17cf434c88839f34e02ef902b",
    "question": "You are tasked with developing an Azure Function that will be triggered by Azure Blob storage. The function should only be triggered when `.jpg` files are added to a container named `images`. Which of the following filter criteria for `path` in the `function.json` file would meet the requirement?",
    "answer": "The `{name}` is a placeholder that captures the blob name.  \n`images/*.jpg` syntax is incorrect.",
    "options": ["`images/jpg`", "`images/{name}.jpg`", "`images/{name}/jpg`", "`images/*.jpg`", "`images/jpg/{name}`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "2adce35482cc6126a7ea269a40ff9d47b2715436ad45641f449e2cbf38728754",
    "question": "Imagine you're devising a real-time analytics solution for a social media company. They desire to oversee and scrutinize user posts and reactions instantaneously. Your assignment is to develop an Azure function that is activated by this live activity. Considering the company's requirement for low-latency data access, complex querying, and globally distributed data, which Azure services would you utilize to accomplish this task? Please provide the triggers and bindings in the given code:\n\n```cs\n[FunctionName(\"AnalyzeUserActivity\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Loop over the events array\n    // For each event, analyze the event data\n    // Write the analysis results to globally distributed storage\n}\n```",
    "answer": "Event Hubs would be a great choice for triggering the Azure function for real-time monitoring and analysis. Event Hubs are designed to capture streaming data like the one from social media feeds. After capturing this real-time data, it can then be processed using Azure Cosmos DB. Cosmos DB, a globally-distributed multi-model database service, would provide the required low-latency data access and support for handling large amounts of data, making it an excellent choice for processing this real-time data according to the company's requirements.\n\n```cs\n[FunctionName(\"AnalyzeUserActivity\")]\npublic static void Run(\n    [EventHubTrigger(\"<event-hub-name>\", Connection = \"EventHubConnectionString\")]\n    EventData[] events,\n\n    [CosmosDB(\"<database-name>\", \"<collection-name>\", ConnectionStringSetting = \"CosmosDBConnectionString\")]\n    out dynamic document,\n\n    ILogger log)\n{\n    // Loop over the events array\n    // For each event, analyze the event data\n    // Write the analysis results to globally distributed storage\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "daa38ee893176532733266efd290c23b1449df9d6d08cf29fb399f72d9c8c750",
    "question": "You are responsible for creating a serverless application that communicates with an API. This application will ingest data through HTTP requests and, dependent on the received data, it must modify a structured NoSQL database in the cloud. This database should have a key/attribute store with a schemaless design. Could you outline how you would construct the Azure Function to fulfill this purpose? Please provide the triggers and bindings in the code below:\n\n```cs\n[FunctionName(\"UpdateDatabase\")]\npublic static async Task<IActionResult> Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Parse the incoming data\n    // Add the processed data to storage\n    // Return a success response\n}\n```",
    "answer": "This is an example of an HTTP Trigger. The Azure function can be activated by an HTTP request containing the data. In response to the data received, the function could use an output binding to a structured NoSQL service, Azure Table Storage in this case, to modify the database accordingly.\n\n```cs\n[FunctionName(\"UpdateDatabase\")]\npublic static async Task<IActionResult> Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)]\n    HttpRequest req,\n\n    [Table(\"<table-name>\", Connection = \"<storage-connection-string>\")]\n    IAsyncCollector<dynamic> outputTable,\n\n    ILogger log)\n{\n    // Parse the incoming data\n    // Add the processed data to storage\n    // Return a success response\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "bb07a4a55449c594c4e1c1635a1a8fc4bd3c73f8c3b9812bf990c6bc40ddf827",
    "question": "A logistics company wants to build a system that can handle incoming requests for package pickups and deliveries. They need a function that gets triggered every time a new pickup or delivery request is placed. The function should then push this information to a queue for further processing. How would you approach this requirement? Fill in the triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"HandleDeliveryRequest\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process the incoming delivery request\n    // Prepare the output request\n    // The output request is automatically added to the queue\n}\n```",
    "answer": "Queue Storage could be the trigger in this scenario. When a new pickup or delivery request is received, a new message could be added to a Queue Storage, which would trigger the function. The function could then perform necessary processing or further add the information to another queue for further processing.\n\n```cs\n[FunctionName(\"HandleDeliveryRequest\")]\npublic static void Run(\n    [QueueTrigger(\"<queue-name>\", Connection = \"StorageConnectionString\")]\n    DeliveryRequest deliveryRequest,\n\n    [Queue(\"<output-queue-name>\", Connection = \"StorageConnectionString\")]\n    out DeliveryRequest outputRequest,\n\n    ILogger log)\n{\n    // Process the incoming delivery request\n    // Prepare the output request\n    // The output request is automatically added to the queue\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "4133042ff37748707dcd9acf3f4918c43135e97320487cd3897f6005fb1a0558",
    "question": "You are working on a project that requires automatic clean-up of older records in a database every day at a specific time. The clean-up task should be an Azure function that gets triggered at the specified time. The target database has high-throughput transactional capacity, and tunable consistency levels, can you delineate a solution for this task? Please provide the triggers and bindings in the given code.\n\n```cs\n[FunctionName(\"CleanUpOldRecords\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Loop over oldRecords\n    // For each record, mark it for deletion or update it as per your logic\n    // Add the updated records to database\n}\n```",
    "answer": "A Timer trigger would be the optimal selection in this scenario. This allows for the scheduling of an Azure function to execute at a specified time each day. The function should utilize input and output bindings to Azure Cosmos DB, which provides specified capabilities. Its role would be to retrieve and clear out older records.\n\n```cs\n[FunctionName(\"CleanUpOldRecords\")]\npublic static void Run(\n    [TimerTrigger(\"0 0 * * *\")] TimerInfo myTimer,\n\n    [CosmosDB(\"<database-name>\", \"<collection-name>\", ConnectionStringSetting = \"CosmosDBConnectionString\", SqlQuery = \"<SQL-query-for-old-records>\")]\n    IReadOnlyList<dynamic> oldRecords,\n\n    [CosmosDB(\"<database-name>\", \"<collection-name>\", ConnectionStringSetting = \"CosmosDBConnectionString\")]\n    IAsyncCollector<dynamic> outputDocuments,\n\n    ILogger log)\n{\n    // Loop over oldRecords\n    // For each record, mark it for deletion or update it as per your logic\n    // Add the updated records to database\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "01b442adfdb6f30d07da08987e21874a108f9d731203c1283390c9838ede34e0",
    "question": "Let's consider a case where you're dealing with an IoT project. You're receiving huge amounts of telemetry data from IoT devices, and this data should trigger an Azure function that analyses the data in near real-time. Assuming you would need a service that can handle vast amounts of data with millisecond response times, storage should provide low latency at any scale. How would you structure the Azure function for this situation? Please provide the triggers and bindings in the given code.\n\n```cs\n[FunctionName(\"ProcessTelemetryData\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Loop over the events array\n    // For each event, process the telemetry data\n    // Write the processed data to storage\n}\n```",
    "answer": "Event Hubs would again be the right choice for this scenario, given their capability to handle large amounts of data in real-time. The telemetry data could be sent to an Event Hub, which would trigger the function. The function could then analyze the data and use an output binding to store the analysis results. Cosmos DB's capabilities to provide low latency at any scale and its global distribution n\n\n```cs\n[FunctionName(\"ProcessTelemetryData\")]\npublic static void Run(\n    [EventHubTrigger(\"<event-hub-name>\", Connection = \"EventHubConnectionString\")]\n    EventData[] events,\n\n    [CosmosDB(\"<database-name>\", \"<collection-name>\", ConnectionStringSetting = \"CosmosDBConnectionString\")]\n    out dynamic document,\n\n    ILogger log)\n{\n    // Loop over the events array\n    // For each event, process the telemetry data\n    // Write the processed data to storage\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "b9a5894aa9773b617e33a6e3a75b2585e33f682949320da7a2858b31020a9fac",
    "question": "A media company requires an Azure function to accumulate real-time news data from various online sources (webhooks) and subsequently deposit the collected data in a particular format into a database. Given the need to store large amounts of unstructured data that will be accessed infrequently, what approach would you suggest? Please provide the triggers and bindings in the provided code:\n\n```cs\n[FunctionName(\"CollectNewsData\")]\npublic static async Task<IActionResult> Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Parse the incoming data\n    // Process the news data, if necessary\n    // Add the news data to storage\n    // Return a success response\n}\n```",
    "answer": "To gather real-time news data from diverse web sources, an HTTP trigger could be utilized. The function could then reformat the amassed data and employ an output binding to a service that specializes in storing large amounts of unstructured data and is cost-effective for infrequent data access - Azure Blob Storage - to deposit the data.\n\n```cs\n[FunctionName(\"CollectNewsData\")]\npublic static async Task<IActionResult> Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)]\n    HttpRequest req,\n\n    [Blob(\"<container-name>/<blob-name>\", FileAccess.Write, Connection = \"<storage-connection-string>\")]\n    Stream outputBlob,\n\n    ILogger log)\n{\n    // Parse the incoming data\n    // Process the news data, if necessary\n    // Add the news data to storage\n    // Return a success response\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "0257a5124a865c85d1b2de4338c191389c531a6cf2c3f40b609d345c3dd12819",
    "question": "In a microservice architecture, you have an Azure function that needs to send messages to multiple subscribers. The subscribers then process these messages in their own time. How would you design this system? Fill in the triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"BroadcastMessage\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process the incoming message, if necessary\n    // Prepare the output message\n    // The output message is automatically sent for the subscribers to read later\n}\n```",
    "answer": "In this microservice architecture, the Azure function could use the Service Bus topic to send messages to multiple subscribers. Each subscriber could listen to the topic and process the message in their own time. This way, the Azure function doesn't need to know about the subscribers and the subscribers can process messages independently.\n\n```cs\n[FunctionName(\"BroadcastMessage\")]\npublic static void Run(\n    [ServiceBusTrigger(\"<topic-name>\", \"<subscription-name>\", Connection = \"ServiceBusConnectionString\")]\n    Message inputMessage,\n\n    [ServiceBus(\"<output-topic-name>\", Connection = \"ServiceBusConnectionString\")]\n    out Message outputMessage,\n\n    ILogger log)\n{\n    // Process the incoming message, if necessary\n    // Prepare the output message\n    // The output message is automatically sent for the subscribers to read later\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "f54286da9c1d9ff5362d3dc8128fda29ca362def3f7891a5bac12c6d3016bedc",
    "question": "Imagine you're part of an organization that operates in the cloud and manages numerous resources across a wide variety of Azure subscriptions. For compliance and auditing purposes, the company aims to log all changes to resource states. Considering that your organization operates worldwide, requiring the data to be available without significant latency regardless of the location, which Azure service would you recommend to trigger your Azure Function? Fill in the triggers and bindings in the code provided:\n\n```cs\n[FunctionName(\"LogResourceChanges\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process the incoming event\n    // Prepare the document based on the event data\n    // The document is automatically added to storage\n}\n```",
    "answer": "For this scenario, an Event Grid trigger would be ideal. Azure Event Grid is capable of dispatching an event whenever there is a state change in Azure resources, which in turn triggers your Azure Function. Following activation, the function could log these changes to a data service that can offer seamless access to data from any corner of the world, such as Azure Cosmos DB, using an output binding. This would ensure that no matter where the request is coming from, data access will remain consistent and fast, supporting your global operations.\n\n```cs\n[FunctionName(\"LogResourceChanges\")]\npublic static void Run(\n    [EventGridTrigger]\n    EventGridEvent eventGridEvent,\n\n    [CosmosDB(\"<database-name>\", \"<collection-name>\", ConnectionStringSetting = \"CosmosDBConnectionString\")]\n    out dynamic document,\n\n    ILogger log)\n{\n    // Process the incoming event\n    // Prepare the document based on the event data\n    // The document is automatically added to storage\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "5db8034e1415c5c894efd13fd0e00a3f2727a0610c272efa9771ccd579a3f238",
    "question": "You've been tasked with developing a system to handle real-time reactions to blog posts on a website. The system needs to notify all registered users whenever a new blog post is published. What Azure services would you recommend to set up the Azure function? Fill in the triggers and bindings in the following code:\n\n```cs\n[FunctionName(\"NotifyUsers\")]\npublic static async void Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process the incoming event\n    // Prepare the notification based on the blog post data\n    // Use HttpClient to send the notification to all registered users\n}\n```",
    "answer": "For real-time notifications, you could use an Event Grid trigger. When a new blog post is published, it can trigger an event which in turn triggers the Azure function. The function could then use an output binding to a Service Bus topic to notify all registered users.\n\n```cs\n[FunctionName(\"NotifyUsers\")]\npublic static async void Run(\n    [EventGridTrigger]\n    EventGridEvent eventGridEvent,\n\n    ILogger log)\n{\n    // Process the incoming event\n    // Prepare the notification based on the blog post data\n    // Use HttpClient to send the notification to all registered users\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "0566c697db142cf0c2257b70ae3ca7c7e3c9ad690e284a7fae15a27a0874e624",
    "question": "A customer is keen on setting up a system that is capable of accepting JSON payloads from an assortment of services. This payload data is then intended to be stored in a database that is optimal for storing large amounts of structured, non-relational data, such as metadata, user data, and diagnostic data. How would you structure an Azure Function to accommodate this need? Please provide the triggers and bindings in the following code.\n\n```cs\n[FunctionName(\"ProcessPayload\")]\npublic static async Task<IActionResult> Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // Process incoming JSON payload data\n    // Add the processed data to designated storage\n    // Return a success response\n}\n```",
    "answer": "In this circumstance, an HTTP trigger could be employed to receive the JSON payloads. The payload data could then be stored into a database service that is designed for storing structured NoSQL data using an output binding - in this case, Azure Table Storage.\n\n```cs\n[FunctionName(\"ProcessPayload\")]\npublic static async Task<IActionResult> Run(\n    [HttpTrigger(AuthorizationLevel.Function, \"post\", Route = null)]\n    HttpRequest req,\n\n    [Table(\"<table-name>\", Connection = \"StorageConnectionAppSetting\")]\n    IAsyncCollector<dynamic> outputTable,\n\n    ILogger log)\n{\n    // Process incoming JSON payload data\n    // Add the processed data to designated storage\n    // Return a success response\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "3882effcc853c96c8961da282c86f4332bf932ce636ad2288b5409084a1d2d47",
    "question": "How to define an HTTP output binding in Azure Function?",
    "answer": "This binding requires an HTTP trigger and allows you to customize the response associated with the trigger's request.",
    "options": [
      "Using `[Http(AuthorizationLevel.Function, \"post\", Route = null)]` parameter",
      "Returning `Task<IActionResult>` result from the function",
      "There is only HTTP input binding, output binding does not exist"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "d0edcecb1ad6ab83e1465623b0ced03a7c3cd24656a8ee48ec10d58109a506d3",
    "question": "Write an Azure function that is invoked when there are inserts or updates in the `Items` collection of `ToDoItems` database for Cosmos DB.\n\n```cs\n[FunctionName(\"CosmosTrigger\")]\npublic static async Task<IActionResult> Run(\n    /* Triggers and Bindings here */\n\n    ILogger log)\n{\n    // TODO: List modified documents\n}\n```",
    "answer": "\n\n```cs\n[FunctionName(\"CosmosTrigger\")]\npublic static async Task<IActionResult> Run(\n    [CosmosDBTrigger(databaseName: \"ToDoItems\", collectionName: \"Items\", ConnectionStringSetting = \"CosmosDBConnection\",LeaseCollectionName = \"leases\", CreateLeaseCollectionIfNotExists = true)]\n    IReadOnlyList<Document> documents,\n\n    ILogger log)\n{\n    if (documents != null && documents.Count > 0)\n    {\n        log.LogInformation($\"Documents modified: {documents.Count}\");\n        for (var i = 0; i < documents.Count; i++)\n            log.LogInformation($\"First document Id: {documents[0].Id}\");\n    }\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "673ba3b3034f36dd1cf0aaca7924ccfe4aea92cd8e96a7cb444e823eb361ed44",
    "question": "You're part of a large digital library that digitizes and stores thousands of books. Once a book is digitized and stored in Blob Storage, a process is triggered to generate metadata for the digitized book. After the metadata is generated, an event should be fired to notify other services for further processing (e.g., updating the search index, notifying users who requested this book). How would you set up the Azure function to handle this situation?\n\n```cs\n[FunctionName(\"ProcessDigitizedBook\")]\npublic static void Run(\n    /* Triggers and Bindings here */\n\n    string name,\n    ILogger log)\n{\n    // Process the inputBook to generate metadata\n    // Prepare an EventGridEvent with the metadata\n    // Add the event to the 'outputEvents' output binding, which will send it to Event Grid\n}\n```",
    "answer": "\n\n```cs\n[FunctionName(\"ProcessDigitizedBook\")]\npublic static void Run(\n    [BlobTrigger(\"<container-name>/{name}\", Connection = \"BlobStorageConnectionString\")]\n    Stream inputBook,\n\n    [EventGrid(TopicEndpointUri = \"<topic-endpoint>\", TopicKeySetting = \"<topic-key>\")]\n    ICollector<EventGridEvent> outputEvents,\n\n    string name,\n    ILogger log)\n{\n    // Process the inputBook to generate metadata\n    var metadata = GenerateMetadata(inputBook);\n\n    // Prepare an EventGridEvent with the metadata\n    var event = new EventGridEvent(\"/subject/path\", \"Book.Digitized\", \"1.0\", metadata);\n\n    // Send the event\n    await outputEvents.AddAsync(event);\n\n    log.LogInformation($\"Processed blob\\n Name:{name} \\n Size: {inputBook.Length} Bytes\");\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Functions"
  },
  {
    "id": "3cc2318aed6fcc394fe92455b04c79a6e598d2b1650abc2f23f54d91c8e964ef",
    "question": "Write a cron schedule expression that will execute a command every 30th minute of every two hours, for the first 15 days of the summer months (June, July, and August), on weekdays (Monday through Friday).",
    "answer": "30 \\*/2 1-15 6-8 1-5",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "3d2e036dd5d0d34358bd735e7dc1ac908463514b8ae490bb08ad5ab78b7e2a60",
    "question": "How often are new instances allocated for HTTP triggers?",
    "answer": "HTTP triggers get new instances at most every second. Non-HTTP triggers get them at most every 30 seconds.",
    "options": ["As soon as there is a demand for them", "Once a second", "Every 30 seconds", "Every minute"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "2d74de7441956090d964762fef71107b041a1720558de6bef31f1be641c6aeed",
    "question": "What method should you use to obtain function keys, example: `https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Web/sites/{name}/{scope}/{host-or-function-name}/listkeys?api-version=2022-03-01`",
    "answer": "`POST`... [what?!?!?](https://learn.microsoft.com/en-us/rest/api/appservice/web-apps/list-function-keys)",
    "options": ["POST", "GET", "PUT"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "a4629defd64872fe5568523ad2331fa4cc83e684f70581323b22ae6f9b6dacfd",
    "question": "What method should you use to create a function key, example: `https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Web/sites/{name}/{scope}/{host-or-function-name}/keys/{keyName}?api-version=2022-03-01`",
    "answer": "`PUT`, huh...",
    "options": ["POST", "GET", "PUT"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "47d3e02aa32140c42be74f5b65c005218b2bbadf2e5422a7b13642b45f61fcff",
    "question": "You're working on a collection of Azure Functions and have recently deployed a new Function App that runs on .Net Core 3.1. Where would you specify the logging settings for these Azure Functions?",
    "answer": "The logging configurations for Azure Functions are set in the `host.json` file.",
    "options": ["`local.function.json`", "`function.json`", "`host.json`", "`app.json`", "`appsettings.json`"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "33b1d3687f3afa14c44d5e5664f83986f833b9fb3d5df7963ff646db69d9abdb",
    "question": "You have an Azure Function set up in your Azure account. This Function pulls data from an Azure Storage Queue and deposits it into an Azure SQL Database. Sometimes, the function fails and shows a \"Timeout expired\" error. This could be because the connection pool has reached its limit. How can you address this problem?",
    "answer": "The root cause could be the function running too many tasks at once, using up all the available database connections. You can either expand the database's connection pool or alter the batchSize parameter in the host.json file to limit concurrent function runs.",
    "options": [
      "Tweak the batchSize value in the `host.json` file",
      "Adjust the `queueMax` setting in the function.json file",
      "Switch to a Timer-based trigger",
      "Upgrade to a Premium-tier App Service Plan"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "d1843fc8b8e495819598bb0aa6ff4abee13120d87b356a8eb54b3f55b8614315",
    "question": "You are developing an application using Azure Functions to handle HTTP requests. The programming language you wish to use is not natively supported by Azure Functions. You decide to use a custom handler for this purpose. Which of the following custom handler web servers would meet the requirements of custom handlers?",
    "answer": "The requirement is to have a custom handler web server that starts up in less than 60 seconds. Azure Functions custom handlers must initialize within 60 seconds to be considered healthy.",
    "options": [
      "Node.js Express server with a startup time of 45 seconds",
      "Python Flask server with a startup time of 65 seconds",
      "Ruby on Rails server with a startup time of 55 seconds",
      "Java Spring Boot server with a startup time of 70 seconds",
      "Go HTTP server with a startup time of 50 seconds",
      ".NET Core server with a startup time of 90 seconds"
    ],
    "answerIndexes": [0, 2, 4],
    "hasCode": false,
    "topic": "Functions"
  },
  {
    "id": "3c63812c2aebc030058345eee5eda5c49d54b290009f52f54d532e8641d3664f",
    "question": "How a user can obtain their own photo in Graph?",
    "answer": "`https://graph.microsoft.com/v1.0/me/photo/$value`",
    "options": [
      "`https://graph.microsoft.com/v1.0/me/photo/`",
      "`https://graph.microsoft.com/v1.0/me/photo/$value`",
      "`https://graph.microsoft.com/v1.0/me/photo/value`",
      "`https://graph.microsoft.com/v1.0/me/photo/$src`",
      "`https://graph.microsoft.com/v1.0/me/photo/src`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "97a1ff1c1fd575985af4844072a19b0a8e38bdba38cd4a9f083f2a57091beb53",
    "question": "How a user can obtain the metadata of their own photo in Graph?",
    "answer": "`https://graph.microsoft.com/v1.0/me/photo/`",
    "options": [
      "`https://graph.microsoft.com/v1.0/me/photo/`",
      "`https://graph.microsoft.com/v1.0/me/photo/$metadata`",
      "`https://graph.microsoft.com/v1.0/me/photo/metadata`",
      "`https://graph.microsoft.com/v1.0/me/photo/$info`",
      "`https://graph.microsoft.com/v1.0/me/photo/info`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "fa2ba8ba917410653402d11929e7350e343ab486dfa59334c36429e907f47339",
    "question": "Write a query that get fields `displayName`, `givenName`, `postalCode`, `identities` from the profile of user with id `87d349ed-44d7-43e1-9a83-5f2406dee5bd` (`v1.0`)",
    "answer": "Using `$select`\n\n```http\nGET https://graph.microsoft.com/v1.0/users/87d349ed-44d7-43e1-9a83-5f2406dee5bd?$select=displayName,givenName,postalCode,identities\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "d6f9191dfcad1dfaa932e97a45391ccceea8993134382f02b5453ddea4cd3cc2",
    "question": "What is the proper way to select `displayName` field of current user?",
    "answer": "`/me?$select=displayName`",
    "options": [
      "`https://graph.microsoft.com/v1.0/me?$select=givenName`",
      "`https://graph.microsoft.com/v1.0/me?select=displayName`",
      "`https://graph.microsoft.com/v1.0/me?$select=displayName`",
      "`https://graph.microsoft.com/v1.0/me/displayName`",
      "`https://graph.microsoft.com/v1.0/me?fields=displayName`",
      "`https://graph.microsoft.com/v1.0/me?$fields=displayName`",
      "`https://graph.microsoft.com/v1.0/me|displayName`"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "2f9950fd6fb72eae9fa8458d3aa63b64716fb866be785f27841066a1306005f9",
    "question": "You are tasked with building a web service that integrates with Microsoft Graph. You need to fetch users who have the job title \"Developer\" and only return their email addresses. Which of the following query parameter combinations should you use?",
    "answer": "Use `$filter` query parameter with the correct syntax `jobTitle eq 'Developer'` to filter users who have the job title \"Developer\". Use `$select` query parameter to only return the email addresses of those users.",
    "options": [
      "`?$filter=jobTitle eq 'Developer'&$select=email`",
      "`?$filter=equals(jobTitle,'Developer')&$select=email`",
      "`?$filter=jobTitle eq 'Developer'&$only=email`",
      "`?$filter=equals(jobTitle,'Developer')&$only=email`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "a1cbc0790e40ef73a161ce06530f74d6234efe69a04715568e7dfd8cf4927a83",
    "question": "Which HTTP method below is used to update a resource with new values?",
    "answer": "The PATCH method does update a resource with a new value.  \nThe POST method creates a new resource.  \nThe PUT method replaces a resource with a new one.",
    "options": ["POST", "PATCH", "PUT"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "57922e3c9da954c567c2cf5a6ffc144d8740e4396f2ce735170c53ffa0355330",
    "question": "Which HTTP method below is used to replace a resource with a new one?",
    "answer": "The PUT method replaces a resource with a new one.  \nThe POST method creates a new resource.  \nThe PATCH method does update a resource with a new value.",
    "options": ["POST", "PATCH", "PUT"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "2f090ff444bce74ba76e495236ab6009fd097e72089305919c686f141d79a957",
    "question": "Which HTTP method below is used to create a new resource?",
    "answer": "The POST method creates a new resource.  \nThe PUT method replaces a resource with a new one.  \nThe PATCH method does update a resource with a new value.",
    "options": ["POST", "PATCH", "PUT"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "c56a917d7f35e18e6da1a1d2fd8bbb5a23f33b087596a12f163b880d89715022",
    "question": "Which of the components of the Microsoft 365 platform is used to deliver data external to Azure into Microsoft Graph services and applications?",
    "answer": "Microsoft Graph connectors work in the incoming direction. Connectors exist for many commonly used data sources such as Box, Google Drive, Jira, and Salesforce.  \nThe Microsoft Graph API offers a single endpoint to use with APIs and SDKs.  \nMicrosoft Graph Data Connect provides a set of tools to streamline secure and scalable delivery of Microsoft Graph data to popular Azure data stores.",
    "options": ["Microsoft Graph API", "Microsoft Graph connectors", "Microsoft Graph Data Connect"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "9b2f7b0ac2bb6e03fc1b8919a64193cc463199971669a1a313ada423b40e3cb6",
    "question": "Which of the following endpoints may assume the resource as Microsoft Graph when the resource identifier is omitted in the scope parameter?",
    "answer": "Consent",
    "options": ["Synchronization", "Consent", "Processing", "Access"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "cccf54965a704d8b5820f0b1e978f64c406bc7c9a691754d1e6424c3fd3d5392",
    "question": "You are trying to update the `passwordProfile` of a user to reset their password. What method should you use?\n\n```http\n{METHOD} https://graph.microsoft.com/v1.0/users/{id}\nContent-type: application/json\n\n{\n  \"passwordProfile\": {\n    \"forceChangePasswordNextSignIn\": false,\n    \"password\": \"xWwvJ]6NMw+bWH-d\"\n  }\n}\n```",
    "answer": "The PATCH method does update a resource with a new value.",
    "options": ["GET", "POST", "PATCH", "PUT"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "dbe0390da4bc4c6225f1036166eeb283a4c7556f531f92a4b115cd81946cb574",
    "question": "Which of the following headers is mandatory in the response from Microsoft Graph?",
    "answer": "request-id is always included in responses from Microsoft Graph. It is useful for diagnosing issues and for support purposes, as it allows the Microsoft support team to trace the request in their logs.  \nAuthorization is not a mandatory response header. It's a request header used to authenticate the client (user or application) to the server, typically with a bearer token.  \nContent-Type is a response header but it's not always mandatory. It is used to specify the media type of the resource, i.e., it tells the client what the content type of the returned content actually is.  \nIf-Match is not a mandatory response header. It's a request header used with requests that are conditionally based on the ETag header, for instance, when implementing optimistic concurrency control. It is used to ensure that the client is modifying a resource that hasn't been changed by someone else since it was last fetched.",
    "options": ["Authorization", "Content-Type", "request-id", "If-Match"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "3efecb2fc1ef28e43301a995734adf6bfd05858e76787d49b14075379c61ad4b",
    "question": "You are developing a web application that will utilize Microsoft Graph to interact with Microsoft 365 services. The application must be restricted to performing read operations solely on the resources belonging to the authenticated user. You have already granted the `User.Read` permission. How should you set up the permission constraint in Microsoft Graph to meet this requirement?",
    "answer": "By leaving the constraint part blank, the `User.Read` permission will only allow the app to perform read operations on the signed-in user's profile, without any additional constraints.  \n`Files` and `User` are not applicable to `User.Read`.  \n`All` gives access to everything.  \n`User.ReadBasic.All` grants access to all users' basic information.",
    "options": [
      "Apply the `User` constraint.",
      "Specify the `Read.All` constraint.",
      "Implement the `Files` constraint.",
      "Leave the constraint part blank.",
      "Use the User.ReadBasic.All permission."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "78d7009dd616a98aa24d64b10bb82c250202c4cdfa1416bf4ebfdba40c53c7b7",
    "question": "The app is granted the `Calendars.Read` permission to access Microsoft 365 calendar services from your new web app through Microsoft Graph. You need to ensure that the app can not only access the calendars owned by the signed-in user but also the calendars that other users have shared with the signed-in user. How should you configure the permission?",
    "answer": "The correct way to ensure that the app can read both the calendars owned by the signed-in user and the calendars that other users have shared with the signed-in user is to use the `Calendars.Read.Shared` permission.",
    "options": [
      "Apply rhe `ReadWrite` constraint.",
      "Apply the `Shared`` constraint.",
      "Leave the constraint blank.",
      "Use the `User.Read` permission."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "d332938d38369409abb15e20ed500761464a8dd39142ab71fdbcb71ce1267088",
    "question": "Which of the following permissions require admin consent?",
    "answer": "Generally, permissions ending in `.All` or involving org-wide data require admin consent. Also, some app-specific permissions like `AppCatalog.Submit` do too.  \nSource: [Microsoft Graph permissions reference](https://learn.microsoft.com/en-us/graph/permissions-reference)",
    "options": [
      "`AppCatalog.Submit`",
      "`BusinessScenarioConfig.Read.OwnedBy`",
      "`Calendars.ReadBasic.All`",
      "`Calendars.Read.Shared`",
      "`Users.Read`",
      "`Users.Read.All`"
    ],
    "answerIndexes": [0, 1, 2, 5],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "2f92f844a7c768edaebf44d3b27e481bfa297ff6a4d1a4856c1786c6000f1303",
    "question": "You are building an application that leverages the Microsoft Graph API to manage user activities. You need to retrieve the most recent activities for a user and also be able to update a specific activity. Which REST API calls should you opt for?",
    "answer": "The `GET` method fetches the specified resources, and the `/recent` endpoint specifies that only the most recent activities should be returned.\n\nThe `PATCH` method is used for partial updates to a resource. In this case, you would replace `{activityId}` with the ID of the activity you wish to update.",
    "options": [
      "`GET /me/activities/recent`",
      "`GET /me/activities`",
      "`PATCH /me/activities/{activityId}`",
      "`POST /me/activities`",
      "`PUT /me/activities/{activityId}/recent`"
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Graph"
  },
  {
    "id": "8e7bb2fab51be8c3cd789debf968ed3e1e8c6598b9909fcef8760cc0e9ebe3bb",
    "question": "Which of the below methods of authenticating to Azure Key Vault is recommended for most scenarios?",
    "answer": "The benefit of this approach is that Azure automatically rotates the identity.  \nService principal and secret is not recommended because the application owner or developer must rotate the certificate.  \nService principal and certificate is not recommended because it is difficult to automatically rotate the bootstrap secret that's used to authenticate to Key Vault.",
    "options": ["Service principal and certificate", "Service principal and secret", "Managed identities"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "8e17e7a59ce2c214ff780c9cf9fb1a7479b3c1bce7037b70eedc05b5fc0f3d43",
    "question": "Azure Key Vault protects data when it's traveling between Azure Key Vault and clients. What protocol does it use for encryption?",
    "answer": "Azure Key Vault enforces Transport Layer Security protocol to protect data when it’s traveling between Azure Key Vault and clients.  \nThe Secure Sockets Layer protocol has been replaced with the Transport Layer Security protocol.  \nPresentation Layer is part of the Open Systems Interconnection model and is not a security protocol.",
    "options": ["Secure Sockets Layer", "Transport Layer Security", "Presentation Layer"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "41635b62515e02c7420ea247550775d714f5b70c9e96babc92e42d5e51ce852f",
    "question": "You have downloaded an Azure Functions codebase that is set to be triggered by an HTTP request. This function needs to access a database, and for that, it requires a connection string. You need to ensure the connection string is not stored in plain text within the code or configuration files. You are preparing to create the necessary components to achieve your goal. Which of the following should you create to achieve your goal? Answer by selecting the correct options from the list below.",
    "answer": "**Azure Key Vault**: Provides secure storage for secrets like connection strings. **Access Policy**: Determines access to secrets in Azure Key Vault.  \nMicrosoft Entra ID Identity Protections: Focuses on risk-based conditional access, not secret storage.  \nAzure Storage Account: Used for data storage, not secret storage.  \nAzure Policy: Evaluates resource properties against business rules, not for secret storage.",
    "options": [
      "Azure Key Vault",
      "Access Policy",
      "Microsoft Entra ID Identity Protections",
      "Azure Storage Account",
      "Azure Policy"
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "333651f46aca7a1bc3aaedb412e2af3bbd073a67f10663e7773c50cab82edf38",
    "question": "Your organization is migrating sensitive data to Azure and has decided to use Azure Key Vault for secure storage. To enhance data protection, the IT team wants to ensure that if any key vault or its associated objects are unintentionally deleted, they can be recovered. Moreover, they want to ensure that once an item is marked as deleted, it cannot be permanently removed until a specified retention period elapses. Which Azure CLI commands should the IT team execute to implement these protective measures?",
    "answer": "You need to enablesoft delete (`--enable-soft-delete true`) and purge protection (`--enable-purge-protection true`)  \n`az keyvault create --name <YourKeyVaultName> --retention-days 90` is not a valid command  \n`az keyvault set-policy --name <YourKeyVaultName> --object-id <ObjectId> --key-permissions purge` grants a user the permission to purge objects from the Key Vault. However, it doesn't prevent purging during the retention period.  \n`az keyvault delete --name <YourKeyVaultName> --force-immediate` is not a valid command",
    "options": [
      "`az keyvault update --name <YourKeyVaultName> --enable-soft-delete true`",
      "`az keyvault update --name <YourKeyVaultName> --enable-purge-protection true`",
      "`az keyvault create --name <YourKeyVaultName> --retention-days 90`",
      "`az keyvault set-policy --name <YourKeyVaultName> --object-id <ObjectId> --key-permissions purge`",
      "`az keyvault backup --name <YourKeyVaultName> --file <BackupFileName>`",
      "`az keyvault delete --name <YourKeyVaultName> --force-immediate`"
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "b1c5772f4a5cd7ce21dab75fe0638529284dfb513c96500b1228734e6babce53",
    "question": "Your organization is developing a secure application that involves handling confidential data. You have been tasked with the following requirements:\n\n1. Establish a connection to a remote vault service to manage sensitive keys and secrets.\n1. Retrieve a designated secret from the vault.\n1. Obtain a specific cryptographic key from the vault.\n1. Use the obtained key to perform the following cryptographic operations:\n   - Add the secret value to a given plaintext message, then encrypt it using an asymmetric encryption algorithm.\n   - Decrypt the resulting ciphertext using the same algorithm.\n\nProvide the code to fulfill these requirements, adhering to industry standards for secure communication and cryptographic practices.\n\n```cs\nvar vaultUrl = \"https://<your-key-vault-name>.vault.azure.net/\";\nvar credential = new DefaultAzureCredential();\nvar secretKeyName = \"<YourSecretName>\";\nvar plaintext = \"<To be encrypted>\";\nvar encryptionAlgorithm = EncryptionAlgorithm.RsaOaep;\n\n// Code here\n```",
    "answer": "\n\n```cs\nvar vaultUrl = \"https://<your-key-vault-name>.vault.azure.net/\";\nvar credential = new DefaultAzureCredential();\nvar secretKeyName = \"<YourSecretName>\";\nvar plaintext = \"<To be encrypted>\";\nvar encryptionAlgorithm = EncryptionAlgorithm.RsaOaep;\n\nvar client = new KeyClient(vaultUri: new Uri(vaultUrl), credential: credential);\nKeyVaultSecret secret = await client.GetSecretAsync(secretKeyName);\nstring secretValue = secret.Value;\n\nvar keyResponse = await client.GetKeyAsync(secretKeyName);\nKeyVaultKey key = keyResponse.Value;\nCryptographyClient cryptoClient = client.GetCryptographyClient(key.Name, key.Properties.Version);\nEncryptResult encryptResult = cryptoClient.Encrypt(encryptionAlgorithm, Encoding.UTF8.GetBytes(plaintext + secretValue));\nDecryptResult decryptResult = cryptoClient.Decrypt(encryptionAlgorithm, encryptResult.Ciphertext);\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Key Vault"
  },
  {
    "id": "42202fc5a83fbfa98859271c4248ed20a7b081658f4ae48e05ed5594839148b9",
    "question": "Your company is preparing to deploy an application on an Azure Linux virtual machine (VM) named myLinuxVM, and there is a requirement to configure Azure Disk Encryption. You have created a resource group named myResourceGroup in the East US region.\n\nTo achieve the desired configuration, you need to use a key encryption key (KEK) to protect the encryption secret and enable the Key Vault for both disk encryption and template deployments.\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz keyvault create --name \"<keyvault-id>\" --resource-group \"myResourceGroup\" --location \"eastus\"\naz keyvault update --name \"<keyvault-id>\" --resource-group \"MyResourceGroup\" --enabled-for-disk-encryption \"true\"\naz keyvault update --name \"<keyvault-id>\" --resource-group \"MyResourceGroup\" --enabled-for-template-deployment \"true\"\naz keyvault key create --name \"myKEK\" --vault-name \"<keyvault-id>\" --kty RSA --size 4096\naz vm encryption enable -g \"MyResourceGroup\" --name \"myLinuxVM\" --disk-encryption-keyvault \"<keyvault-id>\" --key-encryption-key \"myKEK\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Key Vault"
  },
  {
    "id": "e9ac192b35fbc9c99a1811f150e486cb3d535ee4094ebb53597f5e83589d4db9",
    "question": "How to create an access policy for your key vault that grants certificate permissions to your user account?\n\n```ps\naz keyvault\n```",
    "answer": "\n\n```ps\naz keyvault set-policy --name <your-key-vault-name> --upn user@domain.com --certificate-permissions delete get list create purge\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Key Vault"
  },
  {
    "id": "71f678b65bab68c92e7d8d088e40bfc7089ec386799bdfd20618be1621f0cf11",
    "question": "What format the response of this command is: `az keyvault secret show --name \"ExamplePassword\" --vault-name $myKeyVault`?",
    "answer": "JSON.",
    "options": ["JSON", "XML", "YAML", "Plain text"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "c35bc269e947275991b8eb48690b1a00373e37ba5dc904c6f73b504b9588970a",
    "question": "You have configured an Azure Key Vault and stored a secret in it. The secret has multiple versions. You need to retrieve a particular version of the secret using a REST API call. What should you include in the REST API call to specify the version you want to retrieve?",
    "answer": "To retrieve a specific version of the secret, you would include it as a query string argument in the REST API call: `GET {vaultBaseUrl}/secrets/{secret-name}/{secret-version}?api-version=7.4`. This allows you to specify which version of the secret you wish to retrieve from the Azure Key Vault.",
    "options": ["A session variable", "A query string parameter", "A JSON payload", "A POST parameter"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "00ac06758ed848e0d6cd0fab08b1707e14f69d21524a74ed1789e15f062359e8",
    "question": "Write the Azure CLI commands to perform the following tasks:\n\n1. Create a Key Vault in Azure.\n1. Add a certificate to the newly created Key Vault using the default policy.\n\n```ps\nkvName=\"<your-unique-keyvault-name>\"\nrgName=\"myResourceGroup\"\nlocationName=\"EastUS\"\ncertName=\"ExampleCertificate\"\n\n# Code here\n```",
    "answer": "\n\n```bash\nkvName=\"<your-unique-keyvault-name>\"\nrgName=\"myResourceGroup\"\nlocationName=\"EastUS\"\ncertName=\"ExampleCertificate\"\n\n# Create a key vault\naz keyvault create --name $kvName --resource-group $rgName --location $locationName\n\n# Add a certificate to Key Vault\naz keyvault certificate create --vault-name $kvName -n $certName -p \"$(az keyvault certificate get-default-policy)\"\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Key Vault"
  },
  {
    "id": "5cb5dac5b508f1b901b677f9c323208570dda229850888f7e53ced8627d11ec6",
    "question": "You are tasked with deploying a cloud-based application that leverages Azure Key Vault for storing sensitive information like certificates and API keys. The application will be rolled out in the following phases:\n\n- Development\n- Testing\n- Staging\n- Production\n\nWhat is the optimal Azure Key Vault configuration to ensure secure and efficient management of secrets across these phases?",
    "answer": "By allocating a separate Azure Key Vault for each phase you ensure isolation of sensitive data, enabling you to manage access policies, secret lifecycles, and audit logs independently for each phase. This approach enhances security and operational efficiency.",
    "options": [
      "Use a single Azure Key Vault for all phases",
      "Set up two Azure Key Vaults: one for Development and Testing, and another for Staging and Production",
      "Establish a unique Azure Key Vault for Production and merge the others into one",
      "Create a dedicated Azure Key Vault for each phase"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Key Vault"
  },
  {
    "id": "fe71196bfc331978696fdf230225194659c5a4ed015c65ffbfa42d032afd49a5",
    "question": "Which of the following managed identity characteristics is indicative of user-assigned identities?",
    "answer": "User-assigned identities exist independently from the resources they're associated with and must be explicitly deleted.  \nThe same user-assigned managed identity can be associated with more than one Azure resource.",
    "options": [
      "Shared lifecycle with an Azure resource",
      "Independent life-cycle",
      "Can only be associated with a single Azure resource"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "7fb0e644e3db09066e17a0451f71afd86682b5bc6cf37991b25ba84340ddf492",
    "question": "Which of the following managed entity types does not require explicit enabling to work?",
    "answer": "Both System-assigned and User-assigned managed identities require explicit enabling.",
    "options": ["System-assigned identity", "User-assigned identity", "Both of these", "None of these"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "9e3e80b12a2332fb48e64445b63762809f170ed769b030d1970c8f051212371b",
    "question": "A client app requests managed identities for an access token for a given resource. Which of the below is the basis for the token?",
    "answer": "The token is based on the managed identities for Azure resources service principal.  \nOauth 2.0 is a protocol that can be used to acquire a token, but isn't the basis for the token.",
    "options": ["Oauth 2.0", "Service principal", "Virtual machine"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "24a71f6e620c58f6f2dc864d2dac3d970a6fde8c6817e46fc271f6130dc8f4ca",
    "question": "You are the administrator of an Azure SQL database that is set up to allow Entra ID-based access. You want your developers to connect to this database using Microsoft SQL Server Management Studio (SSMS) using their on-premises Entra ID credentials without frequent (minimizing) login prompts. Which approach should you adopt?",
    "answer": "Entra ID integrated authentication allows seamless authentication using on-premises Emtra ID credentials.  \nEntra ID B2C is designed for customer identity access, not for on-premises Emtra ID integration.  \nAzure Conditional Access sets access policies but doesn't ensure seamless on-premises Emtra ID authentication.  \nEntra ID guest user access allows external users to access company resources, not related to on-premises Emtra ID authentication.  \nEntra ID token can be used for Azure SQL Database, they would not necessarily reduce the number of authentication prompts.",
    "options": [
      "Entra ID B2C.",
      "Azure Conditional Access.",
      "Entra ID integrated authentication.",
      "Entra ID guest user access.",
      "Entra ID token."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "7998338fc07d3a9712b64deadec58affe0e02428ed746390cab1dddbacb76151",
    "question": "You are managing an Azure subscription and want to delegate the management of virtual machines to a user, but you don't want to give them access to other resources. Which role should you assign to the user?",
    "answer": "The Virtual Machine Contributor role allows a user to create and manage virtual machines, but not the virtual network or storage account they're connected to. The Owner and Contributor roles would give the user access to all resources, not just virtual machines. The Reader role would only allow the user to view resources, not manage them.",
    "options": ["Owner", "Contributor", "Virtual Machine Contributor", "Reader"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "a231d96757e775036dfcc1ce8c134dcb858fa6c6115099ee7508bb1d81c65631",
    "question": "You have assigned a user the Reader role at the subscription scope. Later, you decide to assign the same user the Contributor role at a resource group scope within that subscription. However, you want to limit their Contributor access to only certain resources within the resource group. What should you do?",
    "answer": "Assigning the Contributor role at the resource scope for the specific resources would limit the user's Contributor access to those resources. The other options would either broaden the user's access or limit their access too much.",
    "options": [
      "Assign the Contributor role at the resource scope for the specific resources.",
      "Remove the Reader role at the subscription scope.",
      "Assign the Contributor role at the subscription scope.",
      "Assign the Reader role at the resource scope for the specific resources."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "3f7748a995ed744e23f3367d3030bc5f949bb71ca686dc0c785432fde4b42060",
    "question": "What is the effect of assigning the Reader role at the subscription scope to a user who already has the Contributor role at a resource group scope within that subscription?",
    "answer": "Azure RBAC is an additive model, so a user's effective permissions are the sum of their role assignments. However, a more specific scope (like a resource group) takes precedence over a broader scope (like a subscription). So in this case, the user would have Contributor access to the resource group and Reader access to the rest of the subscription.",
    "options": [
      "The user will have Contributor access to the resource group and Reader access to the rest of the subscription.",
      "The user will have Reader access to the entire subscription, including the resource group.",
      "The user will have Contributor access to the entire subscription.",
      "The user will have both Reader and Contributor access to the entire subscription."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "01427f2bcb67926c7c885c52a5b5082e5f4b77a94a20309f531d9e3eda4ec75a",
    "question": "What is a security principal in Azure RBAC?",
    "answer": "A security principal in Azure RBAC is an object that represents a user, group, service principal, or managed identity that is requesting access to Azure resources. It's not a collection of permissions (that's a role definition), it's not the set of resources that the access applies to (that's scope), and it's not the process of attaching a role definition to a user, group, service principal, or managed identity (that's a role assignment).",
    "options": [
      "A collection of permissions",
      "The set of resources that the access applies to",
      "An object that represents a user, group, service principal, or managed identity",
      "The process of attaching a role definition to a user, group, service principal, or managed identity"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "37592bb7f427ec630e2c56d568551d28c63eb2015336072221df6dcc76900d5e",
    "question": "You have assigned a user the Contributor role at the resource group scope. Later, you decide to assign a deny assignment at the subscription scope that blocks the user from deleting resources. However, you want to allow the user to delete resources within a specific resource group. What should you do?",
    "answer": "Deny assignments in Azure RBAC take precedence over role assignments. Therefore, even if the user has the Contributor or Owner role at the resource group scope, the deny assignment at the subscription scope would still block them from deleting resources. The only way to allow the user to delete resources within the specific resource group would be to remove (not applicable per requirement) or change the scope of the deny assignment at the subscription scope (not listed). Allowing delete operations for the resource group would not override the deny assignment at the subscription scope.",
    "options": [
      "Assign the Contributor role at the subscription scope.",
      "Remove the deny assignment at the subscription scope.",
      "Assign the Owner role at the resource group scope.",
      "Allow delete operations for the resource group.",
      "None of the listed."
    ],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "73ab3f0278e782d3befe27dcb2220af79e9d7bd8d35907df04769ac3b3e4a5a2",
    "question": "You have developed a web API hosted on Azure App Services. You are required to secure this web API using OAuth 2.0 and integrate it with your organization's Entra ID tenant. Which two actions should you take?",
    "answer": "\n\n- **Register an application in Entra ID**: Essential for setting up OAuth 2.0 authentication.\n- **Configure an Azure API Management instance**: Can validate OAuth tokens and manage APIs.\n\nIncorrect options:\n\n- **Create an application proxy**: Used for accessing on-premises web applications remotely, not for OAuth 2.0 setup.\n- **Establish an Azure VPN Gateway**: Connects on-premises networks to Azure, not used for OAuth 2.0 setup.\n- **Implement Azure Traffic Manager**: Manages distribution of user traffic, not related to OAuth 2.0 setup.",
    "options": [
      "Register an application in Entra ID.",
      "Configure an Azure API Management instance.",
      "Create an application proxy.",
      "Establish an Azure VPN Gateway.",
      "Implement Azure Traffic Manager."
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "f9f074328cadd8089c12e909b8b0d66ec79e828af1dc46a1aaf758c243043140",
    "question": "You are managing various applications across different platforms, and you have implemented both user-assigned and system-assigned managed identities in Azure. You need to determine which of the following statements accurately describes the capabilities of these managed identities:",
    "answer": "Managed identities can only be assigned to resources hosted in the Azure cloud. You cannot assign a managed identity to instances of any AWS services.",
    "options": [
      "User-assigned managed identities can be assigned to resources hosted in any cloud platform, while system-assigned managed identities can only be assigned to Azure resources.",
      "Both user-assigned and system-assigned managed identities can be assigned to resources hosted in any cloud platforms.",
      "Both user-assigned and system-assigned managed identities can only be assigned to resources hosted in the Azure cloud."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "59e62dd802a339901324c0c65d9c4507d7f75822cd5760ad520f67f8512eef29",
    "question": "Which of the following workflows are suitable for using system-assigned identities in Azure?",
    "answer": "Common use cases for system-assigned identities:\n\n- Workloads contained within a single Azure resource.\n- Workloads needing independent identities.\n- For example, an application that runs on a single virtual machine.",
    "options": [
      "A single-instance web application hosted on an Azure Virtual Machine, requiring access to an Azure SQL Database for storing user data. The identity is used to authenticate and authorize the VM to access the database without storing credentials in the code.",
      "Three separate Azure Functions, each with a unique task such as processing orders, sending notifications, and generating reports. Each function requires access to different Azure resources, and the identity is used to manage permissions independently for each function.",
      "An analytics application running exclusively on a single Azure VM, tasked with retrieving and processing data from Azure Blob Storage. The identity is used to authenticate the VM to the Blob Storage, allowing secure access without manual credential management.",
      "A distributed e-commerce application running on multiple Azure VMs, all needing to access the same Azure Key Vault to retrieve encryption keys for securing customer data. The identity is shared across all VMs.",
      "An Azure Logic App designed to automate the provisioning of new VMs for a development environment, requiring pre-authorization to a specific Azure Storage Account where deployment scripts are stored. The identity is used to grant immediate access to the storage account upon VM creation.",
      "A containerized microservices application running on Azure Kubernetes Service, where containers are frequently scaled up and down. The identity ensures that permissions to access a shared Azure Queue remain consistent across all containers.",
      "A cluster of VMs running a big data processing application, all needing to read and write data to the same Azure Data Lake for a weather analysis project. The identity is shared across all VMs, allowing them to collectively access the Data Lake with the same permissions."
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "46cab02fc72fac079e999f6905e201410c3609da4a2574d6a22aa13d8d6645ad",
    "question": "Which of the following workflows are suitable for using user-assigned identities in Azure?",
    "answer": "Common use cases for user-assigned identities:\n\n- Workloads that run on multiple resources and can share a single identity.\n- Workloads needing pre-authorization to a secure resource, as part of a provisioning flow.\n- Workloads where resources are recycled frequently, but permissions should stay consistent.\n- For example, a workload where multiple virtual machines need to access the same resource.",
    "options": [
      "A single-instance web application hosted on an Azure Virtual Machine, requiring access to an Azure SQL Database for storing user data. The identity is used to authenticate and authorize the VM to access the database without storing credentials in the code.",
      "Three separate Azure Functions, each with a unique task such as processing orders, sending notifications, and generating reports. Each function requires access to different Azure resources, and the identity is used to manage permissions independently for each function.",
      "An analytics application running exclusively on a single Azure VM, tasked with retrieving and processing data from Azure Blob Storage. The identity is used to authenticate the VM to the Blob Storage, allowing secure access without manual credential management.",
      "A distributed e-commerce application running on multiple Azure VMs, all needing to access the same Azure Key Vault to retrieve encryption keys for securing customer data. The identity is shared across all VMs.",
      "An Azure Logic App designed to automate the provisioning of new VMs for a development environment, requiring pre-authorization to a specific Azure Storage Account where deployment scripts are stored. The identity is used to grant immediate access to the storage account upon VM creation.",
      "A containerized microservices application running on Azure Kubernetes Service, where containers are frequently scaled up and down. The identity ensures that permissions to access a shared Azure Queue remain consistent across all containers.",
      "A cluster of VMs running a big data processing application, all needing to read and write data to the same Azure Data Lake for a weather analysis project. The identity is shared across all VMs, allowing them to collectively access the Data Lake with the same permissions."
    ],
    "answerIndexes": [3, 4, 5, 6],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "e172648fa872191116b6ae0b20f199742fc537892eba934ca21f6d68dbc2f9f2",
    "question": "Which managed entity requirest minimum effort to use?",
    "answer": "A system-assigned identity is tied to your container app and is deleted when your container app is deleted.",
    "options": ["System-assigned identity", "User-assigned identity"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "5aae5f0d020849666b24e79e5350154e7950b7956e7afe29ec4e24baabd503e4",
    "question": "You have an existing container app with a system assigned identity. What happens when you finish using that app and delete it?",
    "answer": "A system-assigned identity is tied to your container app and is deleted when your container app is deleted.",
    "options": [
      "You can reuse this identity for another container app",
      "The system assigned identity gets deleted, thus cannot be reused",
      "You cannot delete a container app before deleting it system assigned identity"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "0528ad87802a7edb311fb844d697bbc21650dab777cf743e3efbc9e9f727251b",
    "question": "You have an existing container map with a user assigned identity. What happens when you finish using that app and delete it?",
    "answer": "A user-assigned identity is a standalone Azure resource that can be assigned to your container app and other resources.",
    "options": [
      "You can reuse this identity for another container app",
      "The user assigned identity gets deleted, thus cannot be reused",
      "You cannot delete a container app before deleting it user assigned identity"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "2a97cc22e4b413beaecebcb31b0babf18e9046984fb2056182c2ad0fed62ff75",
    "question": "You want to use managed identity across multiple container apps. Which one would you choose?",
    "answer": "A user-assigned identity is a standalone Azure resource that can be assigned to your container app and other resources.",
    "options": ["System-assigned identity", "User-assigned identity", "Any", "None"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "2d031b15bfa0084614a22c69e9a28d77170d8dbd21c9f39d24b4c453dd596f7b",
    "question": "You want to use multiple managed identities for your container app. Which ones can you use?",
    "answer": "A container app can have multiple user-assigned identities.",
    "options": ["System-assigned identities only", "User-assigned identities only", "Both", "None"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "786b7f8852ff65d44ea617a2bb040b87d9402483e118285042b07b73101c1f5f",
    "question": "An e-commerce company is planning to migrate their monolithic application to a microservices architecture. They want to leverage Azure Container Apps for this purpose. The application needs to interact with Azure Key Vault to retrieve secrets and Azure SQL Database for storing and retrieving data. The company wants to avoid storing and managing credentials in their application code. Which type of managed identity would fit this scenario?",
    "answer": "This scenario is best suited for a System-assigned managed identity. System-assigned managed identities are tied to the lifecycle of the Azure resource (in this case, the Azure Container App) and are automatically cleaned up by Azure when the resource is deleted. The application can use the System-assigned managed identity to authenticate to any service that supports Entra ID authentication without having any credentials in the code. The Azure Container App can be granted access to the Azure Key Vault and Azure SQL Database using Azure role-based access control (RBAC).",
    "options": ["System-assigned identity", "User-assigned identity", "Any of these", "None of these"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "8543b63cb19c60caa78700314be998fafef081a7941048f1e384b0f89ed8a074",
    "question": "A software company is developing a multi-tenant SaaS application that will be hosted on Azure Container Apps. Each tenant will have their own Azure Storage account for storing data. The application needs to access these storage accounts on behalf of the tenants. The company wants to manage the identities separately from the Azure Container Apps and wants to have one or more pre-configured entities for each tenant. Which type of managed identity would fit this scenario?",
    "answer": "This scenario is best suited for User-assigned managed identities. User-assigned managed identities are standalone Azure resources that can be assigned to one or more instances of an Azure service. In this case, each tenant can have a User-assigned managed identity that is granted access to their Azure Storage account. The Azure Container App can then use these identities to authenticate to the storage accounts on behalf of the tenants. When a tenant leaves, the User-assigned managed identity can be removed from the Azure Container App and deleted.",
    "options": ["System-assigned identity", "User-assigned identity", "Any of these", "None of these"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "85a1f1b3ca0bbf23119ea5276e06a7704ca80502915366f145e1b605c11dbece",
    "question": "What is the primary purpose of `ChainedTokenCredential` in Azure?",
    "answer": "It combines multiple credentials for flexible authentication.",
    "options": [
      "Combines multiple credentials, attempting each in sequence until successful authentication.",
      "Binds multiple Entra ID groups into a single token.",
      "Creates a chain of Entra ID users for fallback authentication.",
      "Encrypts a token using a chain of cryptographic keys."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "fc4c3eddc139df7bafb21331eafd268588dde3ef14e175d976fa5a215e39e34c",
    "question": "You are tasked with assigning Azure roles and want to ensure that you have the necessary permissions. Which of the following roles would grant you the ability to assign Azure roles?",
    "answer": "Both the `User Access Administrator` and `Owner` roles include the `Microsoft.Authorization/roleAssignments/write` permission, which is required to assign Azure roles.  \nThe other roles listed do not inherently include the necessary permission for role assignment.",
    "options": ["`Reader`", "`Contributor`", "`User Access Administrator`", "`Owner`"],
    "answerIndexes": [2, 3],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "2267539b18348c97fa260cb1986b63e0da1defd97ae87b2b8aef7adb6ac3f321",
    "question": "You are an administrator for a company with various Azure resources. Different teams require different levels of access to these resources. Which of the following options should be used to manage access according to the specific needs of each team?",
    "answer": "Azure Role-Based Access Control (Azure RBAC) is the authorization system used to manage access to Azure resources. It allows administrators to assign permissions to users, groups, and applications at different scopes, providing the flexibility to grant access according to specific needs.  \nThe other options listed, such as Entra ID, ARM Templates, and VNet, are important Azure services but do not provide the specific functionality needed to manage access to Azure resources based on roles.",
    "options": [
      "Microsoft Entra ID",
      "Azure Resource Manager Templates (ARM Templates)",
      "Azure Role-Based Access Control (Azure RBAC)",
      "Azure Virtual Network (VNet)"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "c37f8891c9bc0ccce81f1cdb707e677a4b95194f23dd6efe557b412accd20816",
    "question": "In ASP.NET Core 3, in which file do you configure Authentication and Authorization?",
    "answer": "In ASP.NET Core 3, the `Startup.cs` file is where you configure various services, including authentication and authorization. The `ConfigureServices` method is used to register services, and the `Configure` method is used to add middleware to the request pipeline, including authentication middleware.  \n`Program.cs` - This file is used for configuring services in ASP.NET Core 6.0 and later, not in version 3.",
    "options": ["`Program.cs`", "`Startup.cs`", "`App.config`", "`Web.config`"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "7344bf4678c8104fe448f752031a07fe7a8219ce353305e4b2978ecbca4814fa",
    "question": "What is the minimum required role to assign an access policy in Azure Key Vault, adhering to the principle of least privilege?",
    "answer": "`The Key Vault Administrator` has the necessary permissions to assign access policies within the Key Vault.  \nOther roles listed grant more permissions than required for this specific task, violating the principle of least privilege.",
    "options": ["Key Vault Owner", "Global Administrator", "Key Vault Administrator", "Azure Subscription Owner"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "f317965c9bb51c573f3b6a7cf3f765f5c24d11d5f30a4bcb8073b2636ae89307",
    "question": "Within the context of assigning access policies in Azure Key Vault, what is the minimum required permission, ensuring that only the necessary rights are granted?",
    "answer": "A user with specific permissions like \"Set\" and \"Delete\" on the access policies of the Key Vault has the minimum required permissions to assign access policies.  \nOther roles listed grant broader permissions, which is not necessary for this specific task.",
    "options": [
      "User with Access Policy Permissions",
      "Key Vault Owner",
      "Global Administrator",
      "Azure Subscription Owner"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "3f4905a2f3ecd76b79aaec3a84c003a2a95e741cf37d0636cd910e0854f07abe",
    "question": "If you want to manage access policies in Azure Key Vault without granting unnecessary additional permissions, what is the minimum required built-in role?",
    "answer": "The Key Vault Contributor Role is a built-in role that allows you to manage access policies in the Key Vault without granting additional unnecessary permissions.  \nOther roles listed grant more permissions than required for this specific task, which is not aligned with the principle of least privilege.",
    "options": ["Key Vault Contributor Role", "Global Administrator", "Key Vault Owner", "Azure Subscription Owner"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "855c8eeecd005612ce33387c893bca162d423194736d92fc757cbc71a50fbbbe",
    "question": "In the Azure Portal, where do you navigate to assign roles and grant access to specific Azure resources?",
    "answer": "Access control (IAM) is the specific section within the Azure Portal where you can manage access to Azure resources by assigning roles.",
    "options": [
      "Microsoft Entra ID",
      "Access Policies",
      "Access control (IAM)",
      "Resource Configuration",
      "Microsoft Identity Platform"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "e7897e079ca885cb0081df85a600a699604b1047760206095e2e36605031b169",
    "question": "A new web application has been deployed in a specific Azure subscription. The DevOps team will be responsible for monitoring the application and executing future deployments. You need to set up Role-Based Access Control (RBAC) for the DevOps team to allow them to view and manage deployment pipelines within the subscription. What RBAC role should you assign to the DevOps team?",
    "answer": "The correct option is `Microsoft.DevOps/write` because to manage deployment pipelines and monitor the application, the DevOps team would require write access to DevOps resources within the subscription. This role allows them to view and manage DevOps resources, including deployment pipelines.  \n`*/write`: Provides write access to all resources within the subscription, which could be overly permissive and pose a security risk.",
    "options": [
      "`*/write`",
      "Microsoft.Pipelines/write",
      "Microsoft.DevOps//write",
      "Microsoft.Management/write",
      "Microsoft.Management/read"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "699a10e5b556311d98e3c7fe07b8d94415a274245ebc6518f464eeb653db81b5",
    "question": "A freshly deployed application service exists within a specific subscription. When the application goes live and gains users, a support team will oversee logs and assist with subsequent deployments. To enable the support team to view subscription details via RBAC, which access roles should be granted?",
    "answer": "`*/read` grants read access to all resources within the subscription.  \n`Microsoft.Insights/diagnosticSettings/*/read` is incorrect because it only allows read access to Microsoft Insights, but support team wants all resources.",
    "options": [
      "`*/read`",
      "`Microsoft.Support/*/read`",
      "`Microsoft.Compute/*`",
      "`Microsoft.Insights/diagnosticSettings/*/read`"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "861613b1a5d106aa40976f6cce2d12646424bad7ef8628cc93fc152134c70e98",
    "question": "Your organization utilizes Azure for various resources. The development team creates web applications and forwards the builds to the deployment team for implementation on Azure. You need to ensure that your development team can view Azure resources and also create support tickets for all subscriptions. You are tasked with creating a new custom role based on an existing role definition. What actions should you include in the \"Actions\" section of this custom role?",
    "answer": "`*/read` grants read permissions for resources. Adding `Microsoft.Support/*` allows the team to create support tickets for all subscriptions.",
    "options": ["`*/read`", "`*/write`", "`Microsoft.Support/*/read`", "`Microsoft.Support/*`"],
    "answerIndexes": [0, 3],
    "hasCode": false,
    "topic": "Managed Identities"
  },
  {
    "id": "c8f014c5d31dfdf5569516b02a588519720abb49bfe7a2ceeb12a528f0d0c667",
    "question": "Which of the following advanced features of Azure Service Bus creates a first-in, first-out (FIFO) guarantee?",
    "answer": "To create a first-in, first-out (FIFO) guarantee in Service Bus, use sessions. Message sessions enable joint and ordered handling of unbounded sequences of related messages.  \nScheduled delivery: Messages can be submitted to a queue or topic for delayed processing, but that doesn't guarantee a FIFO.  \nA transaction groups two or more operations together into an execution scope.",
    "options": ["Transactions", "Scheduled delivery", "Message sessions"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "806eee08320c754bdad19d304d699d7467f05c2300aa40040db30620eae10a6c",
    "question": "In Azure Service Bus messages are durably stored which enables a load-leveling benefit. Which of the following correctly describes the load-leveling benefit relative to a consuming application's performance?",
    "answer": "Intermediating message producers and consumers with a queue means that the consuming application only has to be able to handle average load instead of peak load.",
    "options": [
      "Performance needs to handle peak load",
      "Performance needs to handle average load",
      "Performance needs to handle low loads"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "a83e2db8f365a56995143f7ec7cec4d2f7e9051a5931523c0f6b9fdfdc700386",
    "question": "What is a key consideration when choosing to use Service Bus queues over Storage queues?",
    "answer": "Service Bus queues provide a guaranteed first-in-first-out (FIFO) ordered delivery.",
    "options": [
      "Your solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.",
      "Your application must store over 80 gigabytes of messages in a queue.",
      "You require server side logs of all of the transactions executed against your queues."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "b1737d01d4a8c19656fe06c81c1b029ebf83726463d9f757388d89d545561846",
    "question": "What is the main difference between Service Bus queues and topics with subscriptions?",
    "answer": "A queue allows processing of a message by a single consumer. In contrast, topics and subscriptions provide a one-to-many form of communication.",
    "options": [
      "Queues allow processing of a message by a single consumer, while topics with subscriptions provide a one-to-many form of communication.",
      "Queues allow processing of a message by multiple consumers, while topics with subscriptions provide a one-to-one form of communication.",
      "Topics with subscriptions allow processing of a message by a single consumer, while queues provide a one-to-many form of communication."
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "901b92a4ff955d5b9d4a15558008e7682b53084d14c46e45007fb74ecbd0b03a",
    "question": "What is the role of the `ContentType` property in Service Bus message payloads?",
    "answer": "The `ContentType` property describes the payload, suggesting a MIME content-type format.",
    "options": [
      "It encrypts the payload for secure transmission.",
      "It determines the size of the payload.",
      "It enables applications to describe the payload, with the suggested format for the property values being a MIME content-type description."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "367986c31cfa7c6ae1dd2a0f01ed04bd8012c732cfbcb5ca2007432e198635b8",
    "question": "What is the purpose of the 'QueueClient' class in Azure Queue Storage when using .NET?",
    "answer": "The 'QueueClient' class is used to interact with queues in Azure Queue Storage, including creating, retrieving, and deleting queues.",
    "options": [
      "It manages the configuration files for client applications.",
      "It retrieves and manipulates queues stored in Azure Queue Storage.",
      "It creates and manage messages within a specific queue."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "385bd955ebd14fbfaaf4fb51819d878230906386a5402984de57df4d4a356c7e",
    "question": "You are building a logging system for a large-scale manufacturing automation system. You want to send a message to a centralized monitoring system whenever a specific machine error occurs. Once the error has been logged and the necessary maintenance team has been alerted, the message must be removed so that the monitoring system will not attempt to log it again. Which service should you use?",
    "answer": "Azure Service Bus supports \"Receive and Delete\" mode, where messages are immediately consumed and removed from the queue.  \nMessages in Event Hubs are retained for a configured retention period, and consumers are responsible for tracking their position in the stream.  \nIn Queue Storage messages are hidden for a specified visibility timeout period, and if not deleted within that time, they become visible again.",
    "options": ["Azure Event Hubs", "Azure Queue Storage", "Azure Service Bus"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "1eae7c79d6185b822271f57e5b116b85303fe5210ac407a1f133690b66c16c5d",
    "question": "For which cases `param` could be of type `string`?",
    "answer": "`EventData` and `ServiceBusMessage` support both `string` and `byte[]`, `Message` is for `byte[]` only. Prefer `byte[]` for `EventData`.",
    "options": [
      "`ServiceBusSender.SendMessagesAsync(new ServiceBusMessage(param)`",
      "`QueueClient.SendAsync(new Message(param))`",
      "`EventHubProducerClient.SendAsync(new EventData(param))`",
      "None of the listed"
    ],
    "answerIndexes": [0, 2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "7559fb1668b6f70f3ad6d2159affdbdc090423771ed6c9d0b5b69e427d1f786e",
    "question": "For which cases `param` could be of type `byte[]`?",
    "answer": "All work with `byte[]`.",
    "options": [
      "`ServiceBusSender.SendMessagesAsync(new ServiceBusMessage(param)`",
      "`QueueClient.SendAsync(new Message(param))`",
      "`EventHubProducerClient.SendAsync(new EventData(param))`",
      "None of the listed"
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "8d6b3e1039c3307deb73fc245984b1fbbf427cdc2562d296b03c80009fa65e34",
    "question": "You are developing an application that requires message storage of 50 GB and real-time messaging without polling the queue. Additionally, the solution must support ordered delivery (FIFO). Which Azure messaging service would best fit these requirements?",
    "answer": "Storage Queue can handle large message storage (up to 500 TB), but it lacks real-time messaging and FIFO. Service Bus, on the other hand, supports all these requirements, making it the suitable choice.",
    "options": ["Service Bus", "Storage Queue", "Event Hub", "Event Grid"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "19f052b0ce6dc84dad1f17a0ade4fb35c8492ef1904e544c139440fa95930603",
    "question": "Your application requires an event-based architecture that can handle millions of events per second with low latency. Additionally, the solution must provide support for custom topics and a maximum queue size of 30 GB. Which Azure messaging service would you choose?",
    "answer": "Both Event Grid and Event Hub can handle millions of events per second. However, only Event Grid provides support for custom topics (the equivalent would be a namespace - a scoping container in which you can have multiple Event Hubs - but this does not allows advanced filtering and fine-grained control over the events that are published and subscribed to), and the maximum queue size requirement does not apply to Event Grid, making it the ideal choice.",
    "options": ["Service Bus", "Storage Queue", "Event Hub", "Event Grid"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "dfd5442363df84c8ae6196b9d8fc17144cafb2cb8e5b215c01b6cc6f2545d480",
    "question": "Your application needs to process large-scale data streams, retain messages for a 1-day retention period, and support a maximum message size of 1 MB. Which Azure messaging service would you choose?",
    "answer": "Both Event Hub and Event Grid are suitable for event-based architectures and support message size of 1 MB. However, only Event Hub provides configurable message retention (up to 7 days).",
    "options": ["Service Bus", "Storage Queue", "Event Hub", "Event Grid"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "31774b3045edd1d07bbeaa4b8ad4137b0a32fdfc5f5ea4850c07d510eb2ad830",
    "question": "You are developing a real-time analytics system in Azure for a global e-commerce platform. You need to separate the order processing system from the analytics engine to enhance the scalability and robustness of the system. You must select an Azure service that allows one-to-one data communication, monitors the status of data manipulation, and adheres to the HTTPS protocol. The data transfer is anticipated to peak at 100 GB. Which Azure service would be the most suitable for this scenario?",
    "answer": "Azure Queue Storage enables point-to-point data exchange via HTTPS and can handle large volumes (up to 500 TB), and tracking data processing.  \nAzure Service Bus supports HTTPS and point-to-point connections (queues) or one-to-many scenarios (topics), it is limited to 80 GB of message storage.",
    "options": ["Azure Blob Storage", "Azure Service Bus", "Azure Event Hub", "Azure Queue Storage"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "10a2c8adc50b608bc1fe68390aab7c5e2d878d4e9ec512e18435c9492c1d6d2a",
    "question": "Which of the following queues supports automatic dead-lettering?",
    "answer": "Azure Service Bus Queues support automatic dead-letter queuing natively. Messages that exceed the configured maximum delivery count, expire past their Time-To-Live (TTL), or are explicitly dead-lettered by the receiver are automatically moved to a dead-letter sub-queue (/$DeadLetterQueue). Azure Storage Queues (B) do not have a built-in dead-letter mechanism — you must implement message poison handling manually in your application code. Therefore, only Service Bus Queues provide automatic dead-lettering out of the box.",
    "options": [
      "Service Bus Queues",
      "Storage Queues",
      "Both has automatic dead-lettering enabled by default",
      "None has automatic dead-lettering enabled by default"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "dec3d5c5d1203890c69a5acd6384caf93d1e792e54298fb281dba7d43ef3c012",
    "question": "Your company's financial analytics team has created a FraudDetectionService that scans incoming transactions for potential fraudulent activity. Any transaction with a risk score above a certain level must be manually verified by a company employee. You are tasked with securely storing these high-risk transaction records, which are expected to number in the millions every hour, for subsequent scrutiny. Which Azure service would be most appropriate for storing these records for later evaluation?",
    "answer": "Azure Event Hub is ideal for storing high-volume data like high-risk transaction records for future scrutiny.",
    "options": ["Azure Storage Queue", "Azure Service Bus Topic", "Azure Event Grid Topic", "Azure Event Hub"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "3b050ab8ac1832849ef79322cfc37d8af47d0fe0816ed3752f6cdb68ea4518bd",
    "question": "You are tasked with designing an Azure-based solution to collect data from 10,000 IoT devices located globally. Each device will send up to 1 MB of data every 24 hours. The data must be stored in Azure Blob Storage and should be easily correlated based on the device identifier. Which Azure service would be the most appropriate to receive and route this device data?",
    "answer": "Designed for high-throughput, real-time data ingestion and processing. It can handle the data volume and allows for easy correlation based on sensor identifiers. It also integrates well with Azure Blob Storage for long-term data retention. Note: Another option would be _Azure IoT Hub_.  \nAzure Event Grid: While it can route events effectively, it's not designed for high-throughput data ingestion. The payload size is also limited to 64 KB.  \nAzure Data Lake Storage with Azure Databricks: This combination is more suitable for big data analytics and could be an overkill for simple storage requirements. It's also more expensive.  \nAzure Queue Storage: Useful for message queuing but not optimized for the IoT data ingestion scenario described.",
    "options": [
      "Azure Event Grid",
      "Azure IoT Hub",
      "Azure Data Lake Storage with Azure Databricks",
      "Azure Event Hubs",
      "Azure Queue Storage"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "22d42f37347be83a098d599bf3985117e5eb9520f8fa00300c070514c2fcbf26",
    "question": "Create a sequence of actions in Azure Logic App service with these objectives:\n\n- Activate the sequence when a virtual machine undergoes deallocation.\n- Forward an email to the IT Administrator outlining the event's particulars.\n\nWhich Azure service would you enlist to kickstart your sequence?",
    "answer": "Azure Event Grid is specialized for event-driven architecture and can be seamlessly coupled with Azure Logic Apps. It's the optimal choice for being alerted about VM deallocation events within a specific resource group.",
    "options": ["Azure Event Hubs", "Azure Event Grid", "Azure Service Bus", "Azure Functions"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "9932264b8ab44537810b838bff40561644a030b7cf5dc1b94c3698ae8a58f94d",
    "question": "Your organization is tasked with building a solution requiring a messaging service in Azure. The messaging framework must:\n\n- Enable transactional operations\n- Detect duplicate messages\n- Ensure messages have an indefinite lifespan\n\nWhich two of the following options meet these criteria?",
    "answer": "Both Azure Service Bus Queue and Azure Service Bus Topic can be configured to support transactions, detect duplicate messages, and allow messages to remain indefinitely. You can specify an extremely long time-to-live (TTL) for messages in both the queue and topic.  \nAzure Event Hubs is designed for high-throughput data streaming and doesn't offer transactional support or duplicate detection.  \nAzure Storage Queues lack features like transactional support and duplicate detection, making them unsuitable for these requirements.",
    "options": ["Azure Event Hubs", "Azure Service Bus Queue", "Azure Service Bus Topic", "Azure Storage Queues"],
    "answerIndexes": [1, 2],
    "hasCode": false,
    "topic": "Message Queues"
  },
  {
    "id": "56b8fe4e299fef42bec8f0bf23848321e6e576658bb1abfe3f48250e50666754",
    "question": "What is \"flapping\" during scaling events?",
    "answer": "Flapping happens when a scale event triggers the opposite scale event.",
    "options": [
      "Loop condition that causes a series of opposing scale events",
      "Time needed for the autoscale to happen",
      "Inability of service to scale due to some error"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "44a566ea1b442e7447aaa0d20da680d04a473eb6b9d37be22f040a478575819f",
    "question": "You are managing a set of Azure virtual machines for a high-traffic e-commerce website. To ensure optimal performance, you decide to set up an alert for any VM that experiences high CPU usage. Specifically, you want to be notified if a VM's average CPU usage exceeds 90% over a 5-minute window, and you want this condition to be evaluated every minute. Additionally, when this alert is triggered, a specific action group should be notified. Using the Azure CLI, craft the appropriate `az monitor metrics alert create` command to achieve this.\n\n```ps\naz monitor metrics alert create -n alert1 -g {ResourceGroup} --scopes {VirtualMachineID} \\\n    # Code here\n    --description \"High CPU\"\n```",
    "answer": "\n\n```ps\naz monitor metrics alert create -n alert1 -g {ResourceGroup} --scopes {VirtualMachineID} \\\n    --condition \"avg Percentage CPU > 90\" \\\n    --window-size 5m \\\n    --evaluation-frequency 1m \\\n    --action {ActionGroupResourceID} \\\n    --description \"High CPU\"\n```\n\n`--scopes {VirtualMachineID}`: Specifies the ID of the virtual machine you want to monitor.  \n`--condition \"avg Percentage CPU > 90\"`: Defines the alert criteria, which is an average CPU percentage greater than 90%.  \n`--window-size 5m`: Sets the period over which the metric data is aggregated to 5 minutes.  \n`--evaluation-frequency 1m`: Sets the frequency at which the metric data is evaluated to every minute.  \n`--action {ActionGroupResourceID}`: Specifies the action group to be triggered when the alert fires.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Monitor"
  },
  {
    "id": "a60eafc0a35dbb373c129f0554e76ff5384c29f5b161ea92c8a6d250913e1c82",
    "question": "You are managing an Azure App Service Web App named `mywebapp`, which hosts a Node.js application. This Web App is running within an App Service Plan called `myplan` in the standard tier, located in a resource group named `mygroup`. You are tasked with setting up monitoring for the CPU usage of `mywebapp`, and you must configure an alert if the CPU usage percentage exceeds 80%. Which resource and metric should you target for this monitoring?\"",
    "answer": "The CPU usage percentage is monitored at the App Service Plan level, as it defines the resources available to the Web Apps within that plan. Monitoring the App Service Plan provides an indication of the overall CPU usage across all instances of the plan. In the standard tier, multiple Web Apps could be running within the same plan, but monitoring the App Service Plan would still allow you to observe the CPU usage for the specific Web App in question.",
    "options": ["Subscription", "Resource Group (`mygroup`)", "App Service Plan (`myplan`)", "Web App (`mywebapp`)"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "d5eb7015207ebf43327df699c6f46b3182273a3471b57ce35d1407c78a4319d4",
    "question": "You are monitoring an Azure App Service Web App and want to use the CPU Time metric. In which of the following App Service plans is the CPU Time metric particularly useful as one of the quotas is defined in CPU minutes used by the app? (Choose all that apply)",
    "answer": "CPU Time is specifically useful for apps hosted in Free or Shared plans, as one of their quotas is defined in CPU minutes used by the app. Other plans do not have this specific quota, making the other options incorrect.",
    "options": ["Free", "Basic", "Standard", "Premium", "Shared"],
    "answerIndexes": [0, 4],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "b5984c60a61341d695c77fde09eee72790373a36823a14302628f907484fb378",
    "question": "You are tasked with monitoring the CPU usage of an Azure App Service Web App and want to use the CPU Percentage metric. In which of the following App Service plans is the CPU Percentage metric a good indication of the overall usage across all instances? (Choose all that apply.)",
    "answer": "CPU Percentage is a useful metric for apps hosted in Basic, Standard, and Premium plans, as these plans can be scaled out, and CPU Percentage provides a good indication of the overall usage across all instances. Options A and E are incorrect as CPU Percentage is not specifically mentioned as useful for Free or Shared plans.",
    "options": ["Free", "Basic", "Standard", "Premium", "Shared"],
    "answerIndexes": [1, 2, 3],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "a05bc139c51e299512986ac0f7ba9974150c13229a290de1c14837175be313eb",
    "question": "What should you use if you want to be notified if your website becomes unresponsive in different geographical regions within Azure?",
    "answer": "The correct option is to add a rule in the Availability Test in Azure Application Insights. This allows you to set up an alert that will notify you via email or SMS if the web app becomes unresponsive in different geographical regions.  \nLoad Balancer rule is used to distribute network traffic, not to monitor website responsiveness.  \nGeo-Redundancy alerts in Azure Storage are related to data replication and availability, not website responsiveness.  \nTraffic Manager monitoring is used for routing network traffic and does not directly provide notifications for website responsiveness.",
    "options": [
      "Configure a Load Balancer rule to monitor website traffic.",
      "Add a rule in the Availability Test in Azure Application Insights.",
      "Set up a Geo-Redundancy alert in Azure Storage.",
      "Enable Traffic Manager monitoring with email notifications."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "0a9d9c37d5114cfd1ee7ba31cbd1f65497e5a4d3262d80af395f0aeabceaf7bf",
    "question": "What do you need to do in order to send the activity log to a Log Analytics workspace?",
    "answer": "Create a diagnostic setting to send the activity log to a Log Analytics workspace.",
    "options": [
      "Configure `appsettings.json`",
      "Configure `Startup.cs`",
      "Create Diagnostic setting",
      "Upgrade to Premium"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "088d856a56eee9b38ed37ab4e1538ff5c0281c206b6acaca69799f46172825af",
    "question": "Which diagnostic setting destination in Azure allows you to perform complex analysis with Kusto queries?",
    "answer": "Log Analytics Workspace enables the correlation of activity log data with other monitoring data, complex analysis, and deep insights on activity log entries.",
    "options": ["Azure Event Hubs", "Azure Storage Account", "Log Analytics Workspace", "Azure Virtual Machine"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "98e5b781981866abe0db6505256dcebecb4011dcc70177d7d776786a1963ccc3",
    "question": "When you need to retain your Azure activity log data longer than 90 days for audit, static analysis, or backup, which diagnostic setting destination should you use?",
    "answer": "Azure Storage Account allows you to retain activity log data longer than the default 90 days, suitable for audit, static analysis, or backup purposes.",
    "options": ["Azure Event Hubs", "Azure Storage Account", "Log Analytics Workspace", "Azure SQL Database"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "67967e06cfb9d959bfbd3b5589d28331ed7f35366839ca158687ab8d188afb2f",
    "question": "If you want to send Azure activity log entries outside of Azure, which diagnostic setting destination would you choose?",
    "answer": "Azure Event Hubs enables the forwarding of activity log entries outside of Azure, making it suitable for integration with third-party SIEM or other log analytics solutions.",
    "options": ["Azure Event Hubs", "Azure Virtual Network", "Azure Storage Account", "Log Analytics Workspace"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "968bc1aa8575b346d4d909f7469a6c42a75d943d06bedc1f18d1fe9518dd133b",
    "question": "You are experiencing data loss in Azure Monitor. What could be the possible reasons for this issue?",
    "answer": "Missing required fields or exceeding size limits can lead to data being rejected by the back end.",
    "options": [
      "Required fields are missing.",
      "One or more fields exceed size limits.",
      "The data is being encrypted with an unsupported algorithm.",
      "The Azure Monitor logs are being archived to an unsupported storage account.",
      "The monitoring agent is running an outdated version.",
      "The network connection to the Azure data center is too slow."
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Monitor"
  },
  {
    "id": "de19655e84a90c58fc9ebea4bdcf9b8b6a68d74e5e9fbd6383cd70ce11174aad",
    "question": "Specify the correct parameter for `QueueClient.SendAsync()` if `text` is of type `string`.",
    "answer": "`Message` supports `byte[]` only.",
    "options": [
      "No parameter",
      "`new Message(Encoding.UTF8.GetBytes(text))`",
      "`new Message(text)`",
      "`Encoding.UTF8.GetBytes(text)`",
      "`text`"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Queue Storage"
  },
  {
    "id": "86ad6b10add48b455c4790c40250c933366a0b529a2287a256972e0976faac7e",
    "question": "You are managing a resource group named `MyResourceGroup` in Azure. You need to add `environment:production` tag this resource group. After adding the tag, you need to apply a read-only lock to all resource groups with this tag. Write the Azure CLI commands that would be needed to accomplish this.\n\n```ps\n# Add tag to resource group\n# Lock all resource groups with this tag\n```",
    "answer": "\n\n```ps\naz group update --name MyResourceGroup --set tags.environment=production\n\n# List all resource groups with the 'environment:production' tag and store the names in an array\nresource_groups=$(az group list --query \"[?tags.environment=='production'].name\" -o tsv)\n\n# Loop through the array and apply a read-only lock to each resource group\nfor rg in $resource_groups\ndo\n  az lock create --lock-type ReadOnly --name LockForProd --resource-group $rg\ndone\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Resource Groups"
  },
  {
    "id": "ab6cb90a3ffa980beba3b3c00e805e37bdf7780866a02a8f806cbb114d69bbe1",
    "question": "You are a .Net developer working on an application that needs to send a notification to queue \"iot-devices\" in Azure Service Bus. Write a code snippet in C# that sends the text \"data\".\n\n```cs\nvar connectionString = \"some-value\";\n// Code here\n```",
    "answer": "\n\n```cs\nvar connectionString = \"some-value\";\nusing (var client = new ServiceBusClient(connectionString))\n{\n    using (var sender = client.CreateSender(queueName))\n    {\n        sender.SendMessagesAsync(new ServiceBusMessage($\"Messages complete\"));\n    }\n}\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Service Bus"
  },
  {
    "id": "197c5c27b1e69abea4f52bf7e8932d8fd18a45afd62ea7fda74125c568f90a5f",
    "question": "Specify the correct parameter for `ServiceBusSender.SendMessagesAsync()` if `text` is of type `string`.",
    "answer": "`ServiceBusMessage` can be both `string` and `byte[]`.",
    "options": [
      "No parameter",
      "`new ServiceBusMessage(Encoding.UTF8.GetBytes(text))`",
      "`new ServiceBusMessage(text)`",
      "`Encoding.UTF8.GetBytes(text)`",
      "`text`"
    ],
    "answerIndexes": [1, 2],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "66f913698e39b77018b5a9e87e4e4962ad2ebbc7482cd38d7968aeea69d5da84",
    "question": "Write an az cli script that creates an isntance of Azure Service Bus:\n\n```ps\nnamespace=mynamespace\nresourceGroup=myresourcegroup\n\n# Code here\n```",
    "answer": "\n\n```ps\nnamespace=mynamespace\nresourceGroup=myresourcegroup\n\naz servicebus namespace create --name $namespace --resource-group $resourceGroup --location eastus\naz servicebus queue create --name myqueue --namespace-name $namespace --resource-group $resourceGroup\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Service Bus"
  },
  {
    "id": "9a1364bd5d186f18e31202004fe675928951dadacb5cc26ddb0b75bbdf45acbc",
    "question": "You are managing a Service Bus namespace in Azure and need to retrieve the primary connection string for the \"RootManageSharedAccessKey\" authorization rule within the namespace named \"mynamespace\" and the resource group named \"myresourcegroup\". Write the Azure CLI command to achieve this.\n\n```ps\n# Code here\n```",
    "answer": "\n\n```ps\naz servicebus namespace authorization-rule keys list --name RootManageSharedAccessKey --namespace-name mynamespace --resource-group myresourcegroup --query primaryConnectionString\n```\n\nThis command lists the keys for the specified authorization rule within the Service Bus namespace. By specifying the name of the authorization rule, namespace, and resource group, and using the --query parameter to filter the output, you can retrieve the primary connection string for the specified rule.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Service Bus"
  },
  {
    "id": "993fdaade6535491532bca14bfc5157f640476ce9ed90bcce72c779c884fa7bf",
    "question": "You are designing a notification system using Azure Service Bus. Messages contain the following properties:\n\n- `Temperature`: the temperature reading.\n- `Location`: the location of the reading.\n\nYou need to create a subscription that handles messages that are not generated from South America and not above 40 degrees. Which filter type should you implement?",
    "answer": "An SQL Filter allows you to write complex conditions using SQL-like expressions. In this scenario, you need to filter messages based on multiple conditions related to temperature, and location. An SQL Filter can handle this complexity, such as `new SqlRuleFilter(\"Temperature < 40 AND AND Location <> 'South America'\")`.",
    "options": ["SqlRuleFilter", "CorrelationRuleFilter", "TrueRuleFilter", "FalseRuleFilter", "No filter"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "ba9766df72f637c3306cd9dc40bd6a787ccee1748bd32a4ca96280cf752ec163",
    "question": "You are building a ticketing system where messages contain:\n\n- `EventID`: the unique identifier for an event.\n- `Category`: the category of the event (e.g., Music, Sports).\n- `Priority`: the priority level (e.g., High, Low).\n\nYou need to create a subscription that handles only high-priority sports events. Which filter type should you implement?",
    "answer": "A Correlation Filter is suitable for matching messages based on specific properties without complex logic. In this case, you are matching messages based on two specific properties: `Category` and `Priority`. A Correlation Filter can efficiently handle this, such as `new CorrelationRuleFilt- [ ] {Category = \"Sports\", Priority = \"High\"}`.",
    "options": ["SqlRuleFilter", "CorrelationRuleFilter", "TrueRuleFilter", "FalseRuleFilter", "No filter"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "b50afcb5be54abe86dfd613d6e2cbd9be18a4383445c8100378ff5ec715cca42",
    "question": "You are developing a logging system that captures all messages in Azure Service Bus for auditing purposes. The system must store every single message without any filtering. Which filter type should you implement?",
    "answer": "A TrueFilter is a special filter that always returns true, meaning it selects all arriving messages without any conditions. In this scenario, where you need to capture every single message without any filtering, a TrueFilter is the appropriate choice, such as `new TrueRuleFilter()`. It ensures that all messages are selected for the subscription, fulfilling the requirement for complete logging.",
    "options": ["SqlRuleFilter", "CorrelationRuleFilter", "TrueRuleFilter", "FalseRuleFilter", "No filter"],
    "answerIndexes": [2, 4],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "60f4ca5f0e928f6245deb8c4b96615cd20bfca85dfd2c99e2a460ccea038f96e",
    "question": "You are developing a logging system that captures all messages in Azure Service Bus for auditing purposes. The system will be used in the future and should not accept any messages at the moment.",
    "answer": "A FalseRuleFilter is a specific filter that always returns false, meaning it will not select any incoming messages. In this case, where the system should not accept any messages at the moment, a FalseRuleFilter is the most suitable option, such as new `FalseRuleFilter()`. This ensures that no messages are selected for the subscription, aligning with the requirement to not capture any messages currently.",
    "options": ["SqlRuleFilter", "CorrelationRuleFilter", "TrueRuleFilter", "FalseRuleFilter", "No filter"],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "859dc59d5b9f6b4e3d7b3c2590d791389d5f2a23ca11d9735bede2dfae424c2c",
    "question": "You are designing a notification system for a large organization using Azure Service Bus. The system must send updates to multiple subscribers whenever a new policy is published. Which feature of Azure Service Bus should you utilize to ensure that all subscribers receive the new policy notifications?",
    "answer": "Topics in Azure Service Bus are designed for one-to-many communication scenarios where a single message can be sent to multiple subscribers. Queues are used for point-to-point connections, Relay is for hybrid connections, and Event Hub is for event streaming. Therefore, Topics are the correct choice for this scenario.",
    "options": ["Queues", "Topics", "Relay", "Event Hub"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "4498b1d6b698df36fe986721647a3bcd98601679f8962caaadac9e480c7928a4",
    "question": "You are building a customer support system where each support ticket must be processed by exactly one support agent. You want to use Azure Service Bus to handle the distribution of support tickets. Which feature of Azure Service Bus would be most appropriate for ensuring that each ticket is processed by only one agent?",
    "answer": "Queues in Azure Service Bus are used for point-to-point connections, ensuring that each message (in this case, a support ticket) is processed by only one receiver (support agent). Topics are for one-to-many scenarios, Event Hub is for event streaming, and Blob Storage is for unstructured data storage. Therefore, Queues are the correct choice for this scenario.",
    "options": ["Topics", "Queues", "Event Hub", "Relay"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "afa523e65f1c6d4e9b2a625945d84913286323dde52c15c464569a5dd52963ad",
    "question": "You are developing a ticket booking system where the order of ticket booking messages is crucial. You want to ensure that the messages related to a single booking are processed in the order they were sent. Which Azure Service Bus feature should you use?",
    "answer": "Message sessions enable First-In-First-Out (F- [ ] guaranteed handling of related messages sequence. This ensures that messages related to a single booking are processed in the order they were sent.",
    "options": ["Message sessions", "Batching", "Scheduled delivery", "Transactions"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "0ad4189a0a67cc3954c99558550749d6b3a618b1082cf620afd79dc764cbde38",
    "question": "You are building a real-time analytics system that needs to process messages as parallel, long-running streams. Which feature should you use?",
    "answer": "Parallel Stream Processing can process messages as parallel, long-running streams using session ID.",
    "options": ["Autoforwarding", "Transactions", "Message deferral", "Batching", "Parallel Stream Processing"],
    "answerIndexes": [4],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "f8184410a9682c61f4d42cb23f7d8b4f0160cf51ed8f3d9974b1809f573a4f7b",
    "question": "You want to chain a queue to another queue within the same namespace for better message routing. Which feature should you use?",
    "answer": "Autoforwarding allows you to chain a queue or subscription to another within the same namespace.",
    "options": ["Autoforwarding", "Transactions", "Message deferral", "Batching", "Parallel Stream Processing"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "999b30ae11f44ad19ed70c36a0e22b18d17378952010bcee99c5a85f0ae18009",
    "question": "You are building a system where you need to hold messages that can't be delivered for later inspection. Which feature should you use?",
    "answer": "Dead-letter queue holds messages that can't be delivered and allows for their removal and inspection.",
    "options": ["Dead-letter queue", "Message deferral", "Transactions", "Autoforwarding", "Scheduled delivery"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "1fdb75e0114a7535a943fa4feca20ffc11178010c9bca55f06a58977dec3c939",
    "question": "You are developing a system where certain messages should not be processed immediately but should remain in the queue for later retrieval. Which Azure Service Bus feature should you use?",
    "answer": "Message deferral allows you to defer the retrieval of a message until a later time, while the message remains set aside in the queue.  \nScheduled delivery allows for delayed processing but doesn't set the message aside for later retrieval.",
    "options": ["Message deferral", "Scheduled delivery", "Autoforwarding", "Parallel Stream Processing", "Batching"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "eab931a6d8f5a6f9a00da9c7491d44423e19d0c2063de473c2d9299c4b7a7965",
    "question": "You are building a financial system where multiple operations like debit and credit need to be executed as a single unit of work. Which feature should you use?",
    "answer": "Transactions allow you to group multiple operations into an execution scope for a single messaging entity, ensuring that all operations are executed as a single unit of work.  \nBatching delays sending a message for a certain period but doesn't group operations.",
    "options": ["Transactions", "Batching", "Autoforwarding", "Message deferral", "Scheduled delivery"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "b7e4ef421ed2bb0bf83a5af75da64ed7ec3cdcb90d63d110787568a2e8447db0",
    "question": "You are responsible for ensuring that your messaging system continues to function even if a datacenter goes down. Which feature should you use?",
    "answer": "Geo-disaster recovery allows for the continuation of data processing in a different region or datacenter during downtime, ensuring high availability.  \nParallel Stream Processing is for parallel, long-running streams but doesn't handle datacenter downtime.",
    "options": ["Geo-disaster recovery", "Security", "Autodelete on idle", "Parallel Stream Processing"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "659e73a03287b87622068bf33d8f3980c295274147301dd9657eb987fbe42f8b",
    "question": "You are developing a logging system that needs to accumulate messages for a short period before sending them to reduce the number of network calls. Which Azure Service Bus feature should you use?",
    "answer": "Batching delays the sending of a message for a certain period, allowing you to accumulate multiple messages and send them together, thereby reducing the number of network calls.  \nMessage sessions enable FIFO handling but don't delay sending messages.",
    "options": ["Batching", "Message sessions", "Message deferral", "Autoforwarding", "Transactions"],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "7db2aee12c4fd488e706cb680f29114ba3c85f318bcd0ee182e4e1136b27ac0f",
    "question": "You are tasked with building a component that will delay the activation of messages sent to an Azure Service Bus queue. Which of the following options should you use to achieve this functionality?",
    "answer": "The `ScheduledEnqueueTimeUtc` property allows you to specify the time at which the message should be activated and made available in the queue. This ensures that messages are only moved to the Active state after a specified period of time.  \nThe TimeToLive property sets the duration for which a message will be available in the queue before it is automatically removed. It doesn't control the activation time of the message.  \nThe LockDuration property is used to define how long a message is locked for processing and is not related to delaying the activation of messages.",
    "options": [
      "Configure the `LockDuration` property for the queue",
      "Configure the `ScheduledEnqueueTimeUtc` property for the messages",
      "Configure the `TimeToLive` property for the queue messages",
      "Configure the `Label` property for the messages"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "5ec7feb4d65e1edb53824bad5dd17b48e35c51a958b96d25b60fb0373cca845b",
    "question": "Which of the following would be effective for optimizing multiple high-throughput queues?",
    "answer": "Per [best practices](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-performance-improvements?tabs=net-standard-sdk-2#multiple-high-throughput-queues)",
    "options": [
      "Use a single message factory for all queues.",
      "Use different factories for clients interacting with different queues.",
      "Enable batched store access.",
      "Disable batched store access.",
      "Use synchronous operations.",
      "Utilize asynchronous operations."
    ],
    "answerIndexes": [1, 2, 5],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "42794586f8f26cd7329f79432235c86c03d519b6cd14257cf8430d63c417a7b4",
    "question": "What strategies would minimize latency for a queue with small or moderate throughput when using a single client?",
    "answer": "Per [best practices](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-performance-improvements?tabs=net-standard-sdk-2#low-latency-queue)",
    "options": [
      "Use synchronous operations.",
      "Use asynchronous operations.",
      "Enable client-side batching.",
      "Enable batched store access.",
      "Disable client-side batching and batched store access.",
      "Set a high prefetch count.",
      "Disable prefetching."
    ],
    "answerIndexes": [1, 4, 5],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "577425409a3b9a3e33aa15abf9ef209d969050aeb14eb1398e41dc4a597e6cc6",
    "question": "What strategies would minimize latency for a queue with small or moderate throughput when using a multiple clients?",
    "answer": "Per [best practices](https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-performance-improvements?tabs=net-standard-sdk-2#low-latency-queue)",
    "options": [
      "Use synchronous operations.",
      "Use asynchronous operations.",
      "Enable client-side batching.",
      "Enable batched store access.",
      "Disable client-side batching and batched store access.",
      "Set a high prefetch count.",
      "Set the prefetch count to 0."
    ],
    "answerIndexes": [1, 4, 6],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "b1e69a011524e7eb9bb72cd5979c836f24a6c79fd25a7c9cec7ca61268ce0e21",
    "question": "In the engineering firm BuildMasters, various project proposals and contract opportunities are managed. Their custom .NET Core application, known as BidMonitor, notifies a message queue whenever there's a change in the status of any proposal or contract. In line with new security standards, all system-to-system interactions must be password-less. What should be the configuration string for the message queue topic in the BidMonitor application for optimal connectivity?",
    "answer": "This option utilizes Managed Identity for secure, password-less authentication and AMQP for efficient data transfer, meeting the security standards.  \n`SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey` utilizes Shared Access Keys, which require passwords.",
    "options": [
      "`Endpoint=sb://sample-namespace.servicebus.windows.net/;Authentication=ManagedIdentity;TransportType=Sbmp`",
      "`Endpoint=sb://sample-namespace.servicebus.windows.net/;SharedAccessKeyName=KeyName;SharedAccessKey=AccessKey`",
      "`Endpoint=sb://sample-namespace.servicebus.windows.net/;Authentication=ManagedIdentity;TransportType=NetMessaging`",
      "`Endpoint=sb://sample-namespace.servicebus.windows.net/;Authentication=ManagedIdentity;TransportType=Amqp`"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "360b512ac0ab9e78c5e3f2e566bbb99bb7ff778084cb72020ae93f5e1bcc3cc2",
    "question": "A healthcare provider is developing a telemedicine application. The application is designed to connect patients with doctors for online consultations. The application needs to follow this workflow:\n\n1. A patient selects the medical specialty they need consultation in (e.g., Cardiology, Dermatology).\n1. Consultation requests are sent to all available doctors in that specialty.\n1. Only consultation requests for the selected specialty will appear for the doctor.\n1. The first doctor to accept the consultation removes it from the list of available consultations.\n\nWhich of the following actions would you implement for this requirement?",
    "answer": "You should first create a Service Bus Namespace. A single namespace is sufficient to manage all the topics and subscriptions. Creating a namespace for each medical specialty would be a maintenance overhead and difficult to manage programmatically.\n\nA single Service Bus topic is ideal. If you create multiple topics for each medical specialty, you would have to send a consultation request to all topics. Removing a consultation once accepted would become complicated.\n\nCreate a Service Bus Subscription for each medical specialty: This allows doctors to subscribe to topics that are relevant to their specialty. You can create rules based on the doctor's specialty and area to filter messages.",
    "options": [
      "Create a single Service Bus Namespace.",
      "Create a Service Bus Namespace for each medical specialty for which a doctor can receive messages.",
      "Create a single Service Bus topic.",
      "Create a Service Bus topic for each medical specialty for which a doctor can receive messages.",
      "Create a single Service Bus subscription.",
      "Create a Service Bus Subscription for each medical specialty for which a doctor can receive messages."
    ],
    "answerIndexes": [0, 2, 5],
    "hasCode": false,
    "topic": "Service Bus"
  },
  {
    "id": "f4cb742095af5bf955acd36bfaa60ab5317d73ec0fe4bb88c273e203088186de",
    "question": "How can you modify or revoke a stored access policy in Azure Storage?",
    "answer": "Regenerating the SAS token does not affect the stored access policy. The SAS token and the stored access policy are separate entities. The SAS token uses the stored access policy, but regenerating the SAS token does not change the stored access policy.  \nModifying the storage account key does not directly affect the stored access policy. The stored access policy is associated with the storage account, not the key. However, if you regenerate the storage account key that was used to create a SAS token, it will revoke the SAS token, but not the stored access policy itself.",
    "options": [
      "Call the access control list operation for the resource type to replace the existing policy.",
      "Delete the stored access policy.",
      "Rename it by changing the signed identifier.",
      "Change the expiry time to a value in the past.",
      "Regenerate the SAS token.",
      "Modify the storage account key."
    ],
    "answerIndexes": [0, 1, 2, 3],
    "hasCode": false,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "359e140dd0132093a7b3fed0f2c4c939ae9c3f5f01c77d67f736e2ec09557b92",
    "question": "You are developing a .NET application that interacts with Azure Blob Storage. You need to generate a token that will allow a client to have read and write access to a specific blob in your storage account for a period of 24 hours. This token should be generated in a secure manner without sharing your storage account key. The client is authenticated with Microsoft Entra ID. Write the C# code to accomplish this task.\n\n```cs\n// Define your storage account name, container name, and blob name\nstring accountName = \"<storage-account-name>\";\nstring containerName = \"<container-name>\";\nstring blobName = \"<blob-name>\";\n```",
    "answer": "\n\n```cs\n// Define your storage account name, container name, and blob name\nstring accountName = \"<storage-account-name>\";\nstring containerName = \"<container-name>\";\nstring blobName = \"<blob-name>\";\n\nvar credential = new DefaultAzureCredential();\nvar blobServiceClient = new BlobServiceClient(new Uri($\"https://{accountName}.blob.core.windows.net\", credential));\n\n// Get a user delegation key for the Blob service that's valid for 24 hours\nvar delegationKey = await blobServiceClient.GetUserDelegationKey(DateTimeOffset.UtcNow, DateTimeOffset.UtcNow.AddDays(1));\n\nvar sasBuilder = new BlobSasBuilder()\n{\n    BlobContainerName = containerName,\n    BlobName = blobName,\n    Resource = \"b\",\n    StartsOn = DateTimeOffset.UtcNow,\n    ExpiresOn = DateTimeOffset.UtcNow.AddDays(1),\n    Protocol = SasProtocol.Https\n};\nsasBuilder.SetPermissions(BlobSasPermissions.Read | BlobSasPermissions.Write);\n\n// Use the key to get the SAS token\nstring sasToken = sasBuilder.ToSasQueryParameters(delegationKey, accountName).ToString();\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "37cf88ae5742943f05ba2b9e3c736dd2f8fae90f4904860d1acae7f11ec882df",
    "question": "You are developing a .NET application that interacts with Azure Blob Storage. You need to generate a token that will allow a client to have read access to a specific container in your storage account for a period of 48 hours. This token should be generated in a straightforward manner, even if it involves sharing your storage account key. The client is not authenticated with Microsoft Entra ID. Write the C# code to accomplish this task.\n\n```cs\nstring accountName = \"<storage-account-name>\";\nstring accountKey = \"<storage-account-key>\";\nstring containerName = \"<container-name>\";\n\n// Code here\n```",
    "answer": "\n\n```csharp\nstring accountName = \"<storage-account-name>\";\nstring accountKey = \"<storage-account-key>\";\nstring containerName = \"<container-name>\";\n\nvar sharedKeyCredential = new StorageSharedKeyCredential(accountName, accountKey);\n\nvar sasBuilder = new BlobSasBuilder()\n{\n    BlobContainerName = containerName,\n    Resource = \"c\",\n    StartsOn = DateTimeOffset.UtcNow,\n    ExpiresOn = DateTimeOffset.UtcNow.AddDays(2),\n    Protocol = SasProtocol.Https\n};\nsasBuilder.SetPermissions(BlobSasPermissions.Read);\n\n// Use the key to get the SAS token\nstring sasToken = sasBuilder.ToSasQueryParameters(sharedKeyCredential).ToString();\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "7d6c05ea3ff4a40ac6da3a448882f64c3b4f2afff2c12c2a8d683f10c51afe6d",
    "question": "You are developing a .NET application that interacts with Azure Blob Storage. You need to generate a token that will allow a client to have read and write access to both Blob and Queue services in your storage account for a period of 72 hours. This token should be generated using your storage account key and should be applicable at the storage account level. Write the C# code to accomplish this task.\n\n```cs\n// Define your storage account name and key\nstring accountName = \"<storage-account-name>\";\nstring accountKey = \"<storage-account-key>\";\n```",
    "answer": "\n\n```cs\n// Define your storage account name and key\nstring accountName = \"<storage-account-name>\";\nstring accountKey = \"<storage-account-key>\";\n\nvar sharedKeyCredential = new StorageSharedKeyCredential(accountName, accountKey);\n\nvar sasBuilder = new AccountSasBuilder()\n{\n    Services = AccountSasServices.Blobs | AccountSasServices.Queues,\n    ResourceTypes = AccountSasResourceTypes.All,\n    ExpiresOn = DateTimeOffset.UtcNow.AddDays(3),\n    Protocol = SasProtocol.Https\n};\nsasBuilder.SetPermissions(AccountSasPermissions.Read | AccountSasPermissions.Write);\n\n// Use the key to get the SAS token\nstring sasToken = sasBuilder.ToSasQueryParameters(sharedKeyCredential).ToString();\n```",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "8933743f6566aa6bdd827a0f97e0e000d2279e62f715a86f96a800d39396eb0b",
    "question": "You are working with Azure Blob Storage and have a task to provide a client with a URL that grants read access to a specific blob for a limited period of time. The client should only be able to access the blob using HTTPS. The URI to the blob is `https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?` and you have a `sig` parameter with the value `SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D`. The access should start at `2020-01-20T11:42:32Z` and expire at `2020-01-20T19:42:32Z`. Construct the URL that fulfills these requirements.",
    "answer": "`https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&st=2020-01-20T11:42:32Z&se=2020-01-20T19:42:32Z&spr=https&sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D`",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "6d71d201741023bee8915f1081dd8863e06cedd512ed4457d626c750bb7c8369",
    "question": "Which of the following types of shared access signatures (SAS) applies to Blob storage only?",
    "answer": "A user delegation SAS is secured with Microsoft Entra ID credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.  \nAn account SAS delegates access to resources in one or more of the storage services. All of the operations available via a service or user delegation SAS are also available via an account SAS.  \nA service SAS delegates access to a resource in the following Azure Storage services: Blob storage, Queue storage, Table storage, or Azure Files.",
    "options": ["Account SAS", "Service SAS", "User delegation SAS"],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "bc1052c67ae1f93dc6105775e40659a77beac3da43ad371e8fb37d7623f74b47",
    "question": "Which of the following best practices provides the most flexible and secure way to use a service or account shared access signature (SAS)?",
    "answer": "The most flexible and secure way to use a service or account SAS is to associate the SAS tokens with a stored access policy.  \nA user delegation SAS is the most secure SAS, but isn't highly flexible because you must use Microsoft Entra ID to manage credentials.  \nUsing HTTPS prevents man-in-the-middle attacks but isn't the most flexible and secure practice.",
    "options": [
      "Associate SAS tokens with a stored access policy.",
      "Always use HTTPS",
      "Implement a user delegation SAS"
    ],
    "answerIndexes": [0],
    "hasCode": false,
    "topic": "Shared Access Signatures"
  },
  {
    "id": "2f7bae16c41e8b27fde28a809a65f8296a06d40aaf5fd1b7f3a257bc194e24ad",
    "question": "Your company operates in a region with stringent data governance laws that restrict the replication of data outside the country borders. Their core application requires a large volume of data storage but this data can be regenerated easily if any data loss occurs. Recently, they have decided to migrate their application to the Azure cloud and are considering their options for storage redundancy, considering costs.\n\n1. What type of storage redundancy should your company consider for their application? Why?\n1. In their context, what could be a potential issue if they choose geo-redundant storage (GRS) over the recommended storage type?\n1. Considering their data governance requirements, how would the concept of paired regions apply or not apply in their scenario?",
    "answer": "\n\n1. Your company should consider using Locally Redundant Storage (LRS). LRS would be suitable as it allows data replication within the same region which meets the company's data governance requirements. Also, since the data can be regenerated easily in the case of data loss, LRS would be a cost-effective choice.\n1. If they choose Geo-Redundant Storage (GRS), they might violate data governance laws because GRS replicates data to a secondary, geographically distant region, which might be in a different country.\n1. Paired regions don't apply in this scenario because the data can't be replicated outside the country or region due to data governance requirements.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "f36079c31bd3af008796c15c831b7c675a03aae874bd8377a43ed4a664845799",
    "question": "Your company's infrastructure on Azure uses unmanaged disks for their applications. They initially chose Geo-Redundant Storage (GRS) for its perceived benefits of increased protection. However, they are starting to experience consistency issues and are considering a switch to a different redundancy option.\n\n1. What could be causing the consistency issues your company is experiencing with their current storage setup?\n1. What storage redundancy option should your company consider switching to for their unmanaged disks, and why?\n1. What are the potential trade-offs your company might face with this new redundancy option compared to GRS?",
    "answer": "\n\n1. The consistency issues your company is experiencing could be due to the use of GRS with unmanaged disks. This setup is not recommended as it may lead to potential issues with consistency due to the asynchronous nature of GRS.\n1. Your company should consider switching to Locally Redundant Storage (LRS) for their unmanaged disks. LRS is recommended because it avoids the potential consistency issues seen with GRS when used with unmanaged disks.\n1. The trade-off with LRS is that data is not replicated in a secondary, geographically distant region as it is with GRS. Therefore, in the event of a major disaster that affects the entire primary region, your company might lose their data.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "557ba6e7c45df3c921880e8289380bd1bfa62fb5243e75bfc7cd80f6c0bc4978",
    "question": "Your company has an application that generates a significant amount of log data. This log data can be reproduced easily and is not subject to stringent data governance laws. However, your company wants to ensure that the data loss possibility is minimized while keeping costs controlled.\n\n1. What Azure storage redundancy option would you recommend to your company for storing their log data and why?\n1. How would the suggested redundancy type handle data loss scenarios?\n1. If your company decided to go for Geo-Redundant Storage instead, what potential issues might they encounter?",
    "answer": "\n\n1. For your company's scenario, Locally Redundant Storage (LRS) would be recommended. LRS would ensure three copies of their data exist in the same data center, providing durability and redundancy. Since the log data can be easily reproduced, this option would be cost-effective as well.\n1. In a data loss scenario, LRS would provide resilience by having three copies of the data. If one copy fails, the data can still be accessed from the other copies.\n1. If your company opted for Geo-Redundant Storage, it would increase their costs. In addition, they may encounter potential consistency issues since GRS replicates data asynchronously to a secondary region, and in the event of a failover, the data in the secondary region might be slightly out of date compared to the primary region.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "af411ff9e1d8285f255f62f011d9cda2b77c36ffe9b7e7ad156a18f385fe9c1d",
    "question": "Your company operates a high-traffic application hosted on Azure. Their service level agreement with their clients requires a high level of data availability and redundancy. Their data is not restricted by any data governance law, and data loss is something that cannot be afforded.\n\n1. What type of Azure storage redundancy should your company consider to meet their high data availability and redundancy requirements?\n1. How does the chosen storage redundancy option ensure high data availability and protect against data loss?\n1. What potential issues might your company encounter if they opted for Locally Redundant Storage instead of the recommended option?",
    "answer": "\n\n1. Your company should consider using Zone-Redundant Storage (ZRS). ZRS would be a great fit because it synchronously replicates data across different availability zones within the same region, providing high availability and durability.\n1. ZRS ensures high data availability by maintaining three copies of the data across different availability zones in the same region. In the event of a zone failure, ZRS can still serve data from the remaining zones, minimizing the risk of data loss.\n1. If your company opted for Locally Redundant Storage (LRS), they would have a higher risk of data loss and downtime. LRS replicates data within a single data center, so if that data center were to go offline due to a catastrophic event, all data could be lost.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "6f6767f7df4a604894101b76eae2bb791ac1c3cace401c7739ef2a48179034b5",
    "question": "You work in a healthcare company that has stringent data regulations, requiring them to maintain patient data within the same region. They run a critical application on Azure, which can't afford any downtime or data loss. The application generates patient records that can't be recreated.\n\n1. What type of Azure storage redundancy should your company consider to meet their high data availability and data governance requirements?\n1. How does the recommended storage redundancy option work in terms of ensuring data resilience and adherence to data governance rules?\n1. What might be the disadvantages for your company if they decided to choose Geo-Redundant Storage instead?",
    "answer": "\n\n1. Your company should consider Zone-Redundant Storage (ZRS). This option would ensure high data availability and also comply with their data governance requirements, as ZRS keeps all data within the same region.\n1. ZRS replicates data across different availability zones within the same region. This means if one zone experiences issues, the data remains accessible from the other zones. As ZRS doesn't replicate data to a different region, it satisfies your company's data governance rules.\n1. If your company chose Geo-Redundant Storage (GRS), they could potentially violate their data governance requirements. GRS replicates data to a secondary region, which might not be permissible for Epsilon's patient data. Also, GRS performs asynchronous replication, which could lead to potential data loss if a failure occurs before data is replicated.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "746a8697a7597d1225041680e7b82cbde77b7e2b53338520f3691ebb054c5bd9",
    "question": "Your company operates an application that generates non-critical, easily reproducible data. They operate within a region with multiple Availability Zones. They wish to maintain a balance between data redundancy and cost-effectiveness.\n\n1. What type of Azure storage redundancy should your company consider for their requirements?\n1. How does the recommended storage redundancy option strike a balance between data redundancy and cost?\n1. If your company decided to choose Zone-Redundant Storage, what could be the possible implications in terms of cost and data resilience?",
    "answer": "\n\n1. Your company should consider using Locally Redundant Storage (LRS). Since their data is easily reproducible and non-critical, LRS would provide sufficient data redundancy at a lower cost compared to ZRS.\n1. LRS provides three copies of data within the same data center, providing adequate redundancy for your company's needs. As it doesn't replicate data across multiple zones, LRS is more cost-effective than ZRS.\n1. If your company chose Zone-Redundant Storage, they would be paying for higher data redundancy that might be unnecessary given their data is non-critical and easily reproducible. This would lead to higher costs without substantial benefits in terms of data resilience.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "f8f0d586d9b1ff14bb61238d631f984c0da96e8fef3a8ce9fad7b1a181b441ca",
    "question": "Your company is a global e-commerce giant, running a large-scale application on Azure that serves users around the world. They store massive amounts of transactional data that can't be easily regenerated and is crucial for their business. Their operations aren't restricted by any specific data governance regulations.\n\n1. What type of Azure storage redundancy should your company consider for their high data availability and durability requirements?\n1. How does the recommended storage redundancy option ensure high data availability and resilience to data loss?\n1. What potential drawbacks could there be if your company chooses Locally Redundant Storage instead?",
    "answer": "\n\n1. Your company should consider using Geo-Redundant Storage (GRS) because it provides the highest level of data resilience available in Azure. Given the crucial nature of their data, it's essential that they have a high level of redundancy.\n1. GRS replicates data to a secondary, geographically distant Azure region, which ensures data availability even in the case of a complete regional outage. In addition, GRS maintains six copies of the data (three in the primary region and three in the secondary region), which provides a high level of data durability and protection against data loss.\n1. If your company opted for Locally Redundant Storage (LRS), they would risk losing all their data in the event of a catastrophic failure at the data center or a disaster that impacts the entire region.",
    "options": [],
    "answerIndexes": [],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "00e7b9c4db8aae38e7d99d4b2d63ba58d1ad4e3c51c8be62b0c4aac1e62b681e",
    "question": "What options are available for replicating your data only in the primary region?",
    "answer": "GRS and GZRS copies your data asynchronously to secondary region.",
    "options": [
      "Locally redundant storage (LRS)",
      "Zone-redundant storage (ZRS)",
      "Geo-redundant storage (GRS)",
      "Geo-zone-redundant storage (GZRS)"
    ],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "2de2ddacb308535989d3ba63fc6c34da018b45b4ef48f766b5be808b874e5cd2",
    "question": "Beta Enterprises operates an application in Azure that handles sensitive data. Due to data residency laws, they cannot replicate their data outside of their country, but they still want to maintain high availability within their region. In addition, they require read access to their data even if one of the zones in their region goes down. Which storage redundancy option is most suitable for Beta Enterprises?",
    "answer": "ZRS is the best choice for Beta Enterprises as it keeps the data within the same region (complying with data residency laws), but also replicates it across different availability zones for high availability. If one zone goes down, the data is still accessible from the other zones. GRS-RA and GZRS-RA wouldn't comply with Beta's data residency requirements because they replicate data to a secondary region which could be outside of their country. LRS would comply with the data residency laws, but it wouldn't provide the desired level of availability as it only maintains replicas within a single data center.",
    "options": [
      "Locally Redundant Storage (LRS)",
      "Geo-Redundant Storage (GRS-RA)",
      "Geo-Zone-Redundant Storage (GZRS)",
      "Geo-Redundant Storage with Read Access (GRS-RA)",
      "Geo-Zone-Redundant Storage with Read Access (GZRS-RA)",
      "Zone-Redundant Storage (ZRS)"
    ],
    "answerIndexes": [5],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "6f14c5b4dcb8fa653567185882365728003b2424876fccd1151c8067d2914642",
    "question": "Gamma Corp runs an application that needs to read data from a secondary location for performance reasons. They are not restricted by data residency regulations, and they want to ensure their data is highly available. Which of the following storage redundancy should Gamma Corp consider?",
    "answer": "GZRS-RA would best serve Gamma Corp's needs as it not only replicates data across zones in the primary region (for high availability) and to a secondary region (for disaster recovery), but it also provides read access to the secondary region (for performance). GRS does not provide read access to the secondary region, while ZRS and LRS do not replicate data to a secondary region at all.",
    "options": [
      "Locally Redundant Storage (LRS)",
      "Geo-Zone-Redundant Storage (GZRS)",
      "Geo-Zone-Redundant Storage with Read Access (GZRS-RA)",
      "Zone-Redundant Storage (ZRS)"
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "86db58cc09d896d1c7c444f935fba7f4c82278c0938b1b836b4a5231d9975ff8",
    "question": "ViralTrendz Inc. operates a popular platform for sharing short videos (think TikTok on a dose of creative steroids) and stores users' videos in Azure with Geo-Redundant Storage with Read Access (GRS-RA). Suddenly, day-to-day operations are disturbed by a full-blown zombie outbreak in the region of their primary storage. What will happen to users who rush to the app to save their beloved dance-offs and lip-sync masterpieces in this scenario?",
    "answer": "With GRS-RA, data is replicated to a secondary region and users have read access to the replicated data in the secondary region during normal operations. However, in the event of an outage at the primary region, a manual failover to the secondary region needs to be performed by the company. Until this failover is performed, users might experience a short period of downtime.\n\n\"Users will not be able to access their videos until the zombie outbreak is resolved.\" is incorrect because users will eventually be able to access their videos after a manual failover is performed.  \n\"Users will be able to access their videos as usual because the data has been automatically failed over to the secondary region.\" is incorrect because automatic failover does not occur with GRS-RA; a manual failover is required.  \n\"Users will not be able to access their videos because the secondary region only allows read access during normal operations.\" is incorrect because while the secondary region does provide read access during normal operations, it also serves as the failover region where both read and write operations can be performed after a manual failover.  \n\"Choosing GRS-RA was a mistake\" is incorrect because other servers in the same zone might get compromised as well.",
    "options": [
      "Users will not be able to access their videos until the zombie outbreak is resolved.",
      "Users will be able to access their videos as usual because the data has been automatically failed over to the secondary region.",
      "Users will not be able to access their videos because the secondary region only allows read access during normal operations.",
      "Users will experience a short period of downtime until a manual failover is performed by ViralTrendz Inc.",
      "The biggest disaster is ViralTrendz Inc. choosing GRS-RA for this scenario, and now they will have to face angry mob of users as soon as society goes back to normal (if people working there survive)"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "05d923527bb9d62fe2b0d54636f8832c70858f41ad8fe16ee12d5e72ddc5e12b",
    "question": "Facebook, our daily scroll-fest, is powered by Azure's Zone-Redundant Storage (ZRS). However, in an act of divine intervention, the Almighty, having grown disgruntled at humanity's fixation with endless scrolling, decides to express His displeasure. A bolt from the blue strikes and burns down their data center in San Francisco! In this divinely dramatic situation, what could be the fate of Facebook's legions of users, thumbing their way to oblivion? Will they continue to share, like, and comment, or are they due for a surprise digital detox?",
    "answer": "ZRS ensures data is spread across several zones in a region. If one zone goes offline, the others carry on. So, our Facebook loyalists will scroll on, blissfully oblivious to the celestial drama. The other options paint more drastic scenarios than ZRS - a robust choice - would allow.  \n\"A forced hiatus on their social media activities.\" is not quite accurate because even with one data center under the weather (literally), the remaining zones will keep the network alive.  \n\"The remaining zones will buckle under the data weight\" doesn't take into account the resilience of ZRS - the unaffected zones are fully equipped to manage the load.  \n\"A brief moment of digital silence until Facebook Inc. orchestrates a failover to the unaffected zones.\" isn't the case here, as ZRS doesn't require a manual failover.  \n\"Facebook's misguided choice of ZRS\". is incorrect, because ZRS can protect against such occurrences.",
    "options": [
      "A forced hiatus on users' social media activities.",
      "Life on Facebook continues as if the Almighty's intervention was a mere blip. Users will scroll on, undeterred.",
      "Users will find themselves digitally marooned, as the remaining zones, while well-intentioned, buckle under the data weight.",
      "A brief moment of digital silence until Facebook Inc. orchestrates a failover to the unaffected zones.",
      "It turns out, the real misstep might have been Facebook's misguided choice of ZRS for this scenario."
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "503f56f87d0859c53299b1dc8eba9228c39e697546224703dcadf8b8044e6379",
    "question": "A company is using Azure Storage services for their data. They are currently using GRS for their data storage and replication. The company is planning a strategy to enhance disaster recovery capabilities and is considering moving to GZRS. What would be the main advantage for the company to switch from GRS to GZRS?",
    "answer": "The data would be copied across three Azure availability zones in the primary region, improving disaster resilience.\nExplanation: Both GRS and GZRS provide high durability, and both perform synchronous and asynchronous replication. However, GZRS improves resilience by copying data across three Azure availability zones in the primary region, which is beneficial for disaster recovery.  \nBoth GRS and GZRS replicate data asynchronously to a secondary region after a write operation, so this does not provide a unique advantage for GZRS.\nBoth GRS and GZRS copy data synchronously three times within the primary region, so this characteristic doesn't differentiate GZRS as a better choice for enhancing disaster resilience.\nBoth GRS and GZRS provide high durability of 99.99999999999999% (16 9's) over a given year. Therefore, switching to GZRS wouldn't increase this durability.",
    "options": [
      "The data would be replicated asynchronously to the secondary region after a write operation is committed.",
      "The data would be copied synchronously three times within the primary region.",
      "The data would be copied across three Azure availability zones in the primary region, improving disaster resilience.",
      "The durability of the storage resources would increase to at least 99.99999999999999% (16 9's) over a given year."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "ffd1c64a432dc1f934344214bd59c1720ca8d65c06b9c0d81e17e3b463a2450e",
    "question": "Your organization's application uses Azure GZRS storage account. You have observed that read and write operations can still continue even if an availability zone becomes unavailable or unrecoverable. How does GZRS achieve this level of data availability during an outage?",
    "answer": "GZRS improves the availability by storing copies of data across three Azure availability zones. So, even if one zone becomes unavailable, the data is still accessible from the other zones.  \nGZRS does not replicate the data synchronously three times within a single physical location in the primary region.  \nReplicating the data asynchronously to a secondary region is more about ensuring geographical redundancy and doesn't provide immediate availability during an outage in the primary region.  \nCommitting a write operation to the primary location before replicating it using LRS is part of both GRS and GZRS data flow, and doesn't specifically contribute to data availability during an availability zone outage.",
    "options": [
      "By replicating the data synchronously three times within a single physical location in the primary region.",
      "By replicating the data asynchronously to a single physical location in a secondary region.",
      "By committing a write operation to the primary location before replicating it using LRS.",
      "By copying the data across three Azure availability zones in the primary region."
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Storage Redundancy"
  },
  {
    "id": "91d5a7489c4c29b84f70d96a143192ad7804f049f4f6db887b02b92e2db2f89b",
    "question": "You are an engineer at Omega Corp, a company that heavily relies on Azure services. Currently, the company's Azure Storage account `omegaStorageAccount` within the resource group `omegaRG` is set up with Locally Redundant Storage (LRS) for data replication.\n\nRecently, there have been growing concerns about the company's disaster recovery strategy. The business is expanding rapidly, serving customers globally, leading to a requirement for high availability, even in the event of a regional failure.\n\nRecently, there have been several instances where the Omega Corp's global clients were unable to access the services due to regional disruptions. This is unacceptable for the business. Furthermore, your boss expressed a desire to have a kind of \"backup\" option in case the usual data access method fails. The system has to be persistent, to keep trying even if they encounter hiccups.\n\nAs an engineer at Omega Corp, your first task is to update the storage account to meet these new requirements:\n\n```ps\n# Code here\n```\n\nYour second task is to modify the existing code below to make use of these changes:\n\n```cs\nvar accountName = \"omegaStorageAccount\";\nvar primaryAccountUri = new Uri($\"https://{accountName}.blob.core.windows.net/\");\n\nvar blobClientOptions = new BlobClientOptions()\n{\n    // Configure the retry policy to handle high bursts of user activity, transient faults, and network-related issues.\n    // Take into consideration the number of retry attempts (5), delay between retries (1s), maximum waiting time (100s),\n    // and the smart use of the secondary location.\n    Retry = { /* Options */ }\n    // More options\n};\n\nvar blobServiceClient = new BlobServiceClient(primaryAccountUri, new DefaultAzureCredential(), blobClientOptions);\n```\n\nBonus question: Your boss asks you when all of this will be completed, assuming coding will take you no time.",
    "answer": "To meet the new requirements, we need to change the replication option to Read-access geo-zone-redundant storage (GZRS-RA) to provide high availability and read access in case of regional outage. The Azure CLI command for this is:\n\n```ps\naz storage account update --name omegaStorageAccount --resource-group omegaRG --sku Standard_GZRS\n```\n\nGiven the boss's clear directive to ensure persistent operation even during periods of high user activity, we need to implement retry logic in our application. We'll set the maximum number of retries to `5` and use the `Exponential` retry policy to gradually increase the delay between retries if they are necessary. We'll also set the `GeoRedundantSecondaryUri` property to automatically switch to the secondary URI if the primary is unavailable:\n\n```cs\nvar accountName = \"omegaStorageAccount\";\nvar primaryAccountUri = new Uri($\"https://{accountName}.blob.core.windows.net/\");\nvar secondaryAccountUri = new Uri($\"https://{accountName}-secondary.blob.core.windows.net/\");\n\nvar blobClientOptions = new BlobClientOptions()\n{\n    // Determines the policy for how the client should retry its requests upon encountering transient errors\n    Retry =\n    {\n        MaxRetries = 5,\n        Mode = RetryMode.Exponential,\n        Delay = TimeSpan.FromSeconds(1),\n        MaxDelay = TimeSpan.FromSeconds(60),\n        NetworkTimeout = TimeSpan.FromSeconds(100)\n    }\n    // If the secondary Uri response is 404, it won't be used again, indicating possible propagation delay.\n    // Otherwise, retries alternate between primary and secondary Uri.\n    GeoRedundantSecondaryUri = secondaryAccountUri\n};\n\nvar blobServiceClient = new BlobServiceClient(primaryAccountUri, new DefaultAzureCredential(), blobClientOptions);\n```\n\nBonus answer: It takes up to 72 hours for conversion to complete.",
    "options": [],
    "answerIndexes": [],
    "hasCode": true,
    "topic": "Storage Redundancy"
  },
  {
    "id": "a9492443a08e1b8460aa3b11125aa741d06feeb88a46e168489ef7425d597c2e",
    "question": "You are an Azure Architect and you are transferring patient medical data from one hospital to another using Azure services. What method(s) should you use to secure the data?",
    "answer": "Both encryption-at-rest and encryption-in-transit. When dealing with sensitive data, especially healthcare data, it's necessary to ensure maximum security both while the data is in storage and while it's being transferred. Therefore, you would use encryption-at-rest to secure the data when it's stored and encryption-in-transit to secure it while it's moving.",
    "options": ["Encryption-at-rest", "Encryption-in-transit", "No encryption is necessary"],
    "answerIndexes": [0, 1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "82ee8108cb68b0e0aabe15d13ba7a07bf3ff535e00904dc1aa20e2d157c1526b",
    "question": "Your company is migrating all data to Azure Storage and wants to ensure the security of the data both during and after migration. What Azure features should you utilize to ensure the data's security during and after migration?",
    "answer": "Azure Storage Service Encryption (SSE) helps protect data at rest. Transport Layer Security (TLS) helps protect data while it's moving from one location to another (in transit). Azure Key Vault provides secure management of cryptographic keys used for encryption. Together, these ensure the security of data during and after migration.\n\nMicrosoft Entra ID is an important part of securing resources by managing user access, but it doesn't directly ensure the security of data during and after migration in the context of encryption-at-rest and encryption-in-transit. Similarly, Azure Traffic Manager is used for network routing optimization and doesn't directly secure data during transit or at rest. Hence, while these options sound good and are indeed crucial for Azure security in general, they are not correct in the context of this question.",
    "options": [
      "Azure Storage Service Encryption (SSE) for data at rest",
      "Transport Layer Security (TLS) for data in transit",
      "Azure Key Vault for managing cryptographic keys",
      "Microsoft Entra ID for user access control",
      "Azure Traffic Manager for network optimization"
    ],
    "answerIndexes": [0, 1, 2],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "dd6ca98ae4381f821bd28e39e25f4a5d9ab4204cdb43961a16d3651714be893e",
    "question": "Your company is storing sensitive user data in an Azure SQL Database and also needs to securely send this data to a third-party analytics service. To ensure the security of the data, you need to select the appropriate encryption strategies. Which of the following strategies should you adopt?",
    "answer": "Use Transport Layer Security (TLS) for data in transit to the analytics service and Azure Storage Service Encryption (SSE) for data at rest in Azure SQL Database. Transport Layer Security (TLS) is meant to secure data when it's in transit i.e., when it's being moved from one place to another over a network. In this case, it's used when the data is being sent to the analytics service. On the other hand, Azure Storage Service Encryption (SSE) is designed to protect data at rest i.e., when it's stored and not moving. In this case, it's used to secure the data that is stored in Azure SQL Database.  \nOther options are incorrect because they mix-up the roles of encryption-at-rest and encryption-in-transit. For example, Azure Key Vault is not an encryption service but is used for managing cryptographic keys, and TLS is not used for data at rest.",
    "options": [
      "Use Azure Storage Service Encryption (SSE) for data in transit to the analytics service and Azure Key Vault for data at rest in Azure SQL Database.",
      "Use Transport Layer Security (TLS) for data at rest in Azure SQL Database and Azure Storage Service Encryption (SSE) for data in transit to the analytics service.",
      "Use Transport Layer Security (TLS) for data in transit to the analytics service and Azure Storage Service Encryption (SSE) for data at rest in Azure SQL Database.",
      "Use Azure Key Vault for data in transit to the analytics service and Transport Layer Security (TLS) for data at rest in Azure SQL Database."
    ],
    "answerIndexes": [2],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "c1216fff71f2f30b40754277524b0ec80dfb72e1042de2b651bc7c8fb1497c02",
    "question": "Your company is using Azure Storage and wants to serve data to users via a custom domain name while also ensuring encryption in transit. Which service should you utilize for this purpose?",
    "answer": "Azure CDN can be configured to work with a custom domain name and can serve data over HTTPS, enabling encryption in transit.  \nAzure Storage does not natively support HTTPS with a custom domain, hence can't ensure encryption in transit with a custom domain.  \nAzure Key Vault is used for managing encryption keys, not for serving content over a custom domain with encryption.  \nAzure Traffic Manager is for network routing optimization and does not manage encryption or domain names.",
    "options": ["Native Azure Storage functionality", "Azure CDN", "Azure Key Vault", "Azure Traffic Manager"],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "4fddfde94dd301d492144b304b9e5eed4583a3883251896d9d33f953a74d7269",
    "question": "Your global company is using Azure Storage and wants to ensure that data is served to users secureky and quickly regardless of their location. Which service should you use?",
    "answer": "Azure CDN caches content on edge servers that are close to end users, reducing latency when they request content. It is especially beneficial for globally distributed users.  \nAzure Functions are used to run pieces of code or \"functions\", not for distributing content globally.  \nAzure Key Vault is used for managing encryption keys, not for distributing content globally.  \nAzure Virtual Network is for creating isolated networks, not for distributing content globally.",
    "options": [
      "Azure Functions",
      "Azure CDN",
      "Native Azure Storage functionality with TLS",
      "Azure Key Vault",
      "Azure Virtual Network"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "e6ade0fdbb7e6d58aafdea39a4275bea14d43ce4ccdb477193fd1446c3e815d8",
    "question": "Your company is using Azure Storage and needs to securely distribute a large amount of content to numerous users simultaneously (large-scale distribution). Which service should you use?",
    "answer": "If you need to distribute large amounts of content to many users at the same time, a CDN can help to reduce the load on your origin server and ensure that the content is delivered quickly to all users.  \nAzure Storage Account Replication replicates your data within Azure's infrastructure for durability and high availability, but it doesn't directly assist with large-scale content distribution.  \nAzure Data Factory is used for ETL (extract, transform, load) operations, not for distributing content.  \nAzure File Sync is used to centralize file services in Azure while maintaining local access to data, not for large-scale content distribution.",
    "options": [
      "Azure Storage Account Replication",
      "Azure CDN",
      "Native Azure Storage functionality with TLS",
      "Azure Data Factory",
      "Azure File Sync"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "4f3265e0435c476a0087493836e731f1d41fb8d18e61b8b167852b7c4d3e1255",
    "question": "Your company is concerned about DDoS attacks on its Azure Storage. Which service should you use to add a layer of protection against such attacks?",
    "answer": "CDNs can help protect your applications against Distributed Denial of Service (DDoS) attacks by providing a layer of abstraction between the attackers and the application servers.  \nAzure DDoS Protection Standard, while a good choice for DDoS protection in general, does not provide the CDN functionality.  \nAzure Advanced Threat Protection is used to detect and investigate security incidents, not for DDoS protection.  \nAzure Firewall is a managed, cloud-based network security service, but does not provide the caching and DDoS mitigation benefits of a CDN.",
    "options": [
      "Azure DDoS Protection Standard",
      "Azure CDN",
      "Native Azure Storage functionality with TLS",
      "Azure Advanced Threat Protection",
      "Azure Firewall"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "607ab401cde2a88937152ca89a0cf2e56e141a0979d2cd95395ded96026754ec",
    "question": "Your company is using Azure Storage and wants to optimize secure data transfer costs for high volume. Which service should you consider to potentially reduce costs?",
    "answer": "Depending on the specifics of data transfer (like volume and location), the cost of using a CDN can be less than the cost of serving the content directly from Azure Storage.  \nAzure Cost Management provides tools for analyzing and managing costs, but does not affect data transfer costs directly.  \nAzure Reservations offer discounted prices on certain Azure products and resources, but not directly on data transfer costs.  \nAzure Budgets can help manage costs, but do not affect the cost of data transfer directly.",
    "options": [
      "Azure Cost Management",
      "Azure CDN",
      "Native Azure Storage functionality with TLS",
      "Azure Reservations",
      "Azure Budgets"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "a0910d2eb6990e7c77497d5fb75fb3d1d7f92eb16493b8dda86e79b99713e88a",
    "question": "Your company stores data in Azure and needs to ensure the data's secure transmission over the network, but doesn't require a custom domain name. Which service should you use for this requirement?",
    "answer": "When a custom domain is not needed, Azure Storage natively supports encryption in transit using Transport Layer Security (TLS).  \nAzure Storage Service Encryption (SSE) is used for encryption at rest, not for encryption in transit.  \nAzure Key Vault is used for managing encryption keys, not for encryption in transit.  \nAzure CDN is used for content distribution and can provide encryption in transit but is not necessary if a custom domain is not required.",
    "options": [
      "Azure Storage Service Encryption (SSE)",
      "Azure Key Vault",
      "Azure CDN",
      "Native Azure Storage functionality with TLS"
    ],
    "answerIndexes": [3],
    "hasCode": false,
    "topic": "Storage Security"
  },
  {
    "id": "80f211b3a54c8128b78ef128585cb7674137c80ed195fdde956814c144005bfe",
    "question": "Your company uses Azure Storage and wants to serve data from a custom domain. Which service should you use for this requirement?",
    "answer": "Azure CDN can work with a custom domain and caches content on edge servers that are close to end users. This reduces latency when users request content and is especially beneficial for globally distributed users.  \nWhile Azure Storage natively supports encryption in transit using Transport Layer Security (TLS), it does not natively support HTTPS with a custom domain. That's because the SSL certificate, used to serve over HTTPS, must be issued to the exact domain that's being accessed, and Azure Storage does not provide the ability to upload a custom SSL certificate.\nAzure Storage Account Geo-redundancy provides data redundancy across regions, but does not provide the same content delivery speed benefits as Azure CDN.  \nAzure Traffic Manager is for network routing optimization and does not manage content caching for global distribution.  \nAzure Load Balancer is used to distribute network traffic, but doesn't provide content caching like a CDN.",
    "options": [
      "Azure Storage Account Geo-redundancy",
      "Azure CDN",
      "Native Azure Storage functionality with TLS",
      "Azure Traffic Manager",
      "Azure Load Balancer"
    ],
    "answerIndexes": [1],
    "hasCode": false,
    "topic": "Storage Security"
  }
]
