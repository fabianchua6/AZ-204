[
  {
    "question": "You have two Hyper-V hosts named Host1 and Host2. Host1 has an Azure virtual machine named VM1 that was deployed by using a custom Azure Resource Manager template. You need to move VM1 to Host2. What should you do?",
    "options": [
      "From the Update management blade, click Enable.",
      "From the Overview blade, move VM1 to a different subscription.",
      "From the Redeploy blade, click Redeploy.",
      "From the Profile blade, modify the usage location."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources.   jasonsmithss",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "10dee3bb7959d82743ed1c70a10dd93d737c54ee62120ee5d520f7b1fc4b53fd"
  },
  {
    "question": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage from an Azure AD-joined device. The cluster is located in a resource group. Developers have created an application named MyApp. MyApp was packaged into a container image. You need to deploy the YAML manifest file for the application. Solution: You install the Azure CLI on the device and run the kubectl apply `\"f myapp.yaml command. Does this meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "kubectl apply -f myapp.yaml applies a configuration change to a resource from a file or stdin.   Nokaido",
    "topic": "Azure",
    "hasCode": true,
    "isPdf": true,
    "pdfQuestionNumber": 3,
    "id": "ea3551941bbe89987af1b4570d92a344bfc1147eebe16afa94bda6c6ffb91ea4"
  },
  {
    "question": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage from an Azure AD-joined device. The cluster is located in a resource group. Developers have created an application named MyApp. MyApp was packaged into a container image. You need to deploy the YAML manifest file for the application. Solution: You install the docker client on the device and run the docker run -it microsoft/azure-cli:0.10.17 command. Does this meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 4,
    "id": "099a8b841efafc5908be1d2e5c727cc6dab09494aefcaeb0698c1b0bdb8822fd"
  },
  {
    "question": "Your company has a web app named WebApp1. You use the WebJobs SDK to design a triggered App Service background task that automatically invokes a function in the code every time new data is received in a queue. You are preparing to configure the service processes a queue data item. Which of the following is the service you should use?",
    "options": [
      "Logic Apps",
      "WebJobs",
      "Flow",
      "Functions"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  DefaultName2",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 5,
    "id": "e70d072ee7a92b7f9bdc3ef36aac70ed9ece2e333386c878cae32a27f8198cb3"
  },
  {
    "question": "Your company has an Azure subscription. You need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set. You need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance. Which of the following is the value that you should configure for the platformFaultDomainCount property?",
    "options": [
      "10",
      "30",
      "Min Value",
      "Max Value"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "The number of fault domains for managed availability sets varies by region - either two or three per region.   jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 6,
    "id": "9d6086652ad3029a85428e01c37f109a137ef74cde8c9a4dbec2a77f2b8c1090"
  },
  {
    "question": "Your company has an Azure subscription. You need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set. You need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance. Which of the following is the value that you should configure for the platformUpdateDomainCount property?",
    "options": [
      "10",
      "20",
      "30",
      "40"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Each virtual machine in your availability set is assigned an update domain and a fault domain by the underlying Azure platform. For a given availability set, five non-user-configurable update domains are assigned by default (Resource Manager deployments can then be increased to provide up to 20 update domains) to indicate groups of virtual machines and underlying physical hardware that can be rebooted at the same time.   jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 7,
    "id": "e411937444be85abd5024651c3cc73ac5bb77de73e371e01af8a31f34607f5d1"
  },
  {
    "question": "This question requires that you evaluate the underlined text to determine if it is correct. You company has an on-premises deployment of MongoDB, and an Azure Cosmos DB account that makes use of the MongoDB API. You need to devise a strategy to migrate MongoDB to the Azure Cosmos DB account. You include the Data Management Gateway tool in your migration strategy. Instructions: Review the underlined text. If it makes the statement correct, select `No change required.` If the statement is incorrect, select the",
    "options": [
      "No change required",
      "mongorestore",
      "Azure Storage Explorer",
      "AzCopy"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 9,
    "id": "f680e379bf5c0d9870e313fc9e946d4543e3952a3fd7e3ba6227c3797e9b2cef"
  },
  {
    "question": "You are developing an e-Commerce Web App. You want to use Azure Key Vault to ensure that sign-ins to the e-Commerce Web App are secured by using Azure App Service authentication and Azure Active Directory (AAD). What should you do on the e-Commerce Web App?",
    "options": [
      "Run the az keyvault secret command.",
      "Enable Azure AD Connect.",
      "Enable Managed Service Identity (MSI).",
      "Create an Azure AD service principal."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "A managed identity from Azure Active Directory allows your app to easily access other AAD-protected resources such as Azure Key Vault. samples/app-service-msi-keyvault-dotnet/keyvault-msi-appservice-sample/   ZodiaC",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 10,
    "id": "9b813175f84bb2844e170f2999dd7735645608a8e858d73ca973c61b6626a7e2"
  },
  {
    "question": "This question requires that you evaluate the underlined text to determine if it is correct. Your Azure Active Directory Azure (Azure AD) tenant has an Azure subscription linked to it. Your developer has created a mobile application that obtains Azure AD access tokens using the OAuth 2 implicit grant type. The mobile application must be registered in Azure AD. You require a redirect URI from the developer for registration purposes. Instructions: Review the underlined text. If it makes the statement correct, select `No change is needed.` If the statement is incorrect, select the",
    "options": [
      "No change required.",
      "a secret",
      "a login hint",
      "a client ID"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "For Native Applications you need to provide a Redirect URI, which Azure AD will use to return token responses.   jvyas",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 11,
    "id": "71a777e2e4fb6d6531cbf16378b37ce0eb904ceed0d5975eca2c071a3a5f91bd"
  },
  {
    "question": "You manage an Azure SQL database that allows for Azure AD authentication. You need to make sure that database developers can connect to the SQL database via Microsoft SQL Server Management Studio (SSMS). You also need to make sure the developers use their on-premises Active Directory account for authentication. Your strategy should allow for authentication prompts to be kept to a minimum. Which of the following should you implement?",
    "options": [
      "Azure AD token.",
      "Azure Multi-Factor authentication.",
      "Active Directory integrated authentication.",
      "OATH software tokens."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Azure AD can be the initial Azure AD managed domain. Azure AD can also be an on-premises Active Directory Domain Services that is federated with the Azure AD. Using an Azure AD identity to connect using SSMS or SSDT The following procedures show you how to connect to a SQL database with an Azure AD identity using SQL Server Management Studio or SQL Server Database Tools. Active Directory integrated authentication Use this method if you are logged in to Windows using your Azure Active Directory credentials from a federated domain. 1. Start Management Studio or Data Tools and in the Connect to Server (or Connect to Database Engine) dialog box, in the Authentication box, select Active Directory - Integrated. No password is needed or can be entered because your existing credentials will be presented for the connection. 2. Select the Options button, and on the Connection Properties page, in the Connect to database box, type the name of the user database you want to connect to. (The AD domain name or tenant IDג€ option is only supported for Universal with MFA connection options, otherwise it is greyed out.)   MrXBasit",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 14,
    "id": "4b7f4feac21704a1f77d771f39fa239c3dfbfa79809ffa467e7ddf1ce88b7fe8"
  },
  {
    "question": "You are developing an application to transfer data between on-premises file servers and Azure Blob storage. The application stores keys, secrets, and certificates in Azure Key Vault and makes use of the Azure Key Vault APIs. You want to configure the application to allow recovery of an accidental deletion of the key vault or key vault objects for 90 days after deletion. What should you do?",
    "options": [
      "Run the Add-AzKeyVaultKey cmdlet.",
      "Run the az keyvault update --enable-soft-delete true --enable-purge-protection true CLI.",
      "Implement virtual network service endpoints for Azure Key Vault.",
      "Run the az keyvault update --enable-soft-delete false CLI."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "When soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion. Purge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled. When purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed. The default retention period is 90 days, but it is possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. Once the retention policy interval is set and saved it cannot be changed for that vault.   anu_ezio",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 15,
    "id": "82ead21a5bbf1eac3bf688bd8efe41a95120031adb3ee5ed45688a27352c6e54"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment. You need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user. Solution: You include the use of Azure Redis Cache in your design. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  NStanhope",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 17,
    "id": "b5c99de61275f78fb98c578c5c814f51f48586aaf0544c5a3293b8fd16943bf9"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment. You need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user. Solution: You include the use of an Azure Content Delivery Network (CDN) in your design. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  ZodiaC",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 18,
    "id": "13d9fdfcb47d6fb09bf01c2f7d0f49aeea28f6caca95639e923ea8ea2bfb3104"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment. You need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user. Solution: You include the use of a Storage Area Network (SAN) in your design. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  forgetfulalligator",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 19,
    "id": "714dc2f5ef5395979283018b5ab5a0410ff64116f94a9020860a7ebc5c1fcd65"
  },
  {
    "question": "You develop a Web App on a tier D1 app service plan. You notice that page load times increase during periods of peak trafic. You want to implement automatic scaling when CPU load is above 80 percent. Your solution must minimize costs. What should you do first?",
    "options": [
      "Enable autoscaling on the Web App.",
      "Switch to the Premium App Service tier plan.",
      "Switch to the Standard App Service tier plan.",
      "Switch to the Azure App Services consumption plan."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Configure the web app to the Standard App Service Tier. The Standard tier supports auto-scaling, and we should minimize the cost. We can then enable autoscaling on the web app, add a scale rule and add a Scale condition. us/pricing/details/app-service/plans/   examTaker455",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 20,
    "id": "d94c779da2834ed9b11edeea67ba3be0399d6f2319939092d41fb084556cb366"
  },
  {
    "question": "Your company's Azure subscription includes an Azure Log Analytics workspace. Your company has a hundred on-premises servers that run either Windows Server 2012 R2 or Windows Server 2016, and is linked to the Azure Log Analytics workspace. The Azure Log Analytics workspace is set up to gather performance counters associated with security from these linked servers. You must configure alerts based on the information gathered by the Azure Log Analytics workspace. You have to make sure that alert rules allow for dimensions, and that alert creation time should be kept to a minimum. Furthermore, a single alert notification must be created when the alert is created and when the alert is resolved. You need to make use of the necessary signal type when creating the alert rules. Which of the following is the option you should use?",
    "options": [
      "The Activity log signal type.",
      "The Application Log signal type.",
      "The Metric signal type.",
      "The Audit Log signal type."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Metric alerts in Azure Monitor provide a way to get notified when one of your metrics cross a threshold. Metric alerts work on a range of multi- dimensional platform metrics, custom metrics, Application Insights standard and custom metrics. Note: Signals are emitted by the target resource and can be of several types. Metric, Activity log, Application Insights, and Log.   MiraA",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 21,
    "id": "ff20d9670752ecde2d391650ece4e07cef6a7385ef5740fef98146add3abab5e"
  },
  {
    "question": "You are developing a .NET Core MVC application that allows customers to research independent holiday accommodation providers. You want to implement Azure Search to allow the application to search the index by using various criteria to locate documents related to accommodation. You want the application to allow customers to search the index by using regular expressions. What should you do?",
    "options": [
      "Configure the SearchMode property of the SearchParameters class.",
      "Configure the QueryType property of the SearchParameters class.",
      "Configure the Facets property of the SearchParameters class.",
      "Configure the Filter property of the SearchParameters class."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The SearchParameters.QueryType Property gets or sets a value that specifies the syntax of the search query. The default is 'simple'. Use 'full' if your query uses the Lucene query syntax. You can write queries against Azure Search based on the rich Lucene Query Parser syntax for specialized query forms: wildcard, fuzzy search, proximity search, regular expressions are a few examples. us/dotnet/api/microsoft.azure.search.models.searchparameters.querytype   7ack",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 22,
    "id": "d4a6bb843617d0acf031d2e100ff29c7ba4f27a2be1e9e571070ea0e48abac97"
  },
  {
    "question": "You are a developer at your company. You need to update the definitions for an existing Logic App. What should you use?",
    "options": [
      "the Enterprise Integration Pack (EIP)",
      "the Logic App Code View",
      "the API Connections",
      "the Logic Apps Designer"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Edit JSON - Azure portal - 1. Sign in to the Azure portal. 2. From the left menu, choose All services. In the search box, find \"logic apps\", and then from the results, select your logic app. 3. On your logic app's menu, under Development Tools, select Logic App Code View. 4. The Code View editor opens and shows your logic app definition in JSON format. apps/logic-apps-author-definitions   abdou1987",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 23,
    "id": "3b9c713633b8098f12c5914cad85fdea0b2520ab132cfc05168afb9cff272a16"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are developing a solution for a public facing API. The API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end. You must configure back-end authentication for the API Management service instance. Solution: You configure Basic gateway credentials for the Azure resource. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "API Management allows to secure access to the back-end service of an API using client certificates.   lexowe9241",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 24,
    "id": "79efdd7cef53e178ce5c6be109456e3aa59206f800e7821994c3278e312498ed"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are developing a solution for a public facing API. The API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end. You must configure back-end authentication for the API Management service instance. Solution: You configure Client cert gateway credentials for the HTTP(s) endpoint. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The API back end is hosted in an Azure App Service instance. It is an Azure resource and not an HTTP(s) endpoint.   jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 25,
    "id": "7d3ccb59f9175b52da7742ccbf80ec384dde7bbb186b09ca781ee6b5b74490f1"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are developing a solution for a public facing API. The API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end. You must configure back-end authentication for the API Management service instance. Solution: You configure Basic gateway credentials for the HTTP(s) endpoint. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "API Management allows to secure access to the back-end service of an API using client certificates. Furthermore, the API back end is hosted in an Azure App Service instance. It is an Azure resource and not an HTTP(s) endpoint.   jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 26,
    "id": "2822e34f0f90beaf73a4c751b8ebfd6a48693bbcafaf6ae1b53f856d3d306e67"
  },
  {
    "question": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements. You are developing a solution for a public facing API. The API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end. You must configure back-end authentication for the API Management service instance. Solution: You configure Client cert gateway credentials for the Azure resource. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "API Management allows to secure access to the back-end service of an API using client certificates.   jay158",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 27,
    "id": "49b1d79ee7d87394259e8f2ef0c8952b60de57f4dbe24b4d586540da39df6a5c"
  },
  {
    "question": "You are developing a .NET Core MVC application that allows customers to research independent holiday accommodation providers. You want to implement Azure Search to allow the application to search the index by using various criteria to locate documents related to accommodation venues. You want the application to list holiday accommodation venues that fall within a specific price range and are within a specified distance to an airport. What should you do?",
    "options": [
      "Configure the SearchMode property of the SearchParameters class.",
      "Configure the QueryType property of the SearchParameters class.",
      "Configure the Facets property of the SearchParameters class.",
      "Configure the Filter property of the SearchParameters class."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "The Filter property gets or sets the OData $filter expression to apply to the search query. us/dotnet/api/microsoft.azure.search.models.searchparameters.querytype   7ack",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 28,
    "id": "b3def25c2679ffdb371595f4b4a91a74058f08efcc42da1d601e0c21dd31b3ab"
  },
  {
    "question": "You are a developer at your company. You need to edit the workfiows for an existing Logic App. What should you use?",
    "options": [
      "the Enterprise Integration Pack (EIP)",
      "the Logic App Code View",
      "the API Connections",
      "the Logic Apps Designer"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "For business-to-business (B2B) solutions and seamless communication between organizations, you can build automated scalable enterprise integration workfiows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps. apps/logic-apps-author-definitions   Nokaido",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 29,
    "id": "614b507f0cbd3204eafeb912c1720618de3183d790aff8e9adf381c790704105"
  },
  {
    "question": "You are developing an application that applies a set of governance policies for internal and external services, as well as for applications. You develop a stateful ASP.NET Core 2.1 web application named PolicyApp and deploy it to an Azure App Service Web App. The PolicyApp reacts to events from Azure Event Grid and performs policy actions based on those events. You have the following requirements: ✑ Authentication events must be used to monitor users when they sign in and sign out. ✑ All authentication events must be processed by PolicyApp. ✑ Sign outs must be processed as fast as possible. What should you do?",
    "options": [
      "Create a new Azure Event Grid subscription for all authentication events. Use the subscription to process sign-out events.",
      "Create a separate Azure Event Grid handler for sign-in and sign-out events.",
      "Create separate Azure Event Grid topics and subscriptions for sign-in and sign-out events.",
      "Add a subject prefix to sign-out events. Create an Azure Event Grid subscription. Configure the subscription to use the subjectBeginsWith filter."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "  ning",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 31,
    "id": "9932d1901651fca3d646902ea97619394e09950e234139f69e4bf6f9f5923535"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2. When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile- friendly version of the image must start in less than one minute. You need to design the process that starts the photo processing. Solution: Trigger the photo processing from Blob storage events. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "You need to catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload. Note: Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file-oriented workfiow. Events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. However, the processing must start in less than one minute. Note: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.   YahyaSonmez",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 4,
    "id": "8805c4d0a635f890a2e7343bcb4ac0e7b9462406be168078050ec9b360e5f51e"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot. You need to ensure that scripts run and resources are available before a swap operation occurs. Solution: Update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts. Does the solution meet the goal?",
    "options": [
      "No",
      "Yes"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Specify custom warm-up. Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment. <system.webServer> <applicationInitialization> <add initializationPage=\"/\" hostName=\"[app hostname]\" /> <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" /> </applicationInitialization> </system.webServer>   GMartinez",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 5,
    "id": "70ee42342560001564596e6dde0bc16b27b5b1697c14d48588729f6383cca4c2"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot. You need to ensure that scripts run and resources are available before a swap operation occurs. Solution: Enable auto swap for the Testing slot. Deploy the app to the Testing slot. Does the solution meet the goal?",
    "options": [
      "No",
      "Yes"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts. Note: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment. <system.webServer> <applicationInitialization> <add initializationPage=\"/\" hostName=\"[app hostname]\" /> <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" /> </applicationInitialization> </system.webServer>   GMartinez",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 6,
    "id": "b383135e899a0b13a13ff79a147798405309b53e64877874076aaffe1081c076"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot. You need to ensure that scripts run and resources are available before a swap operation occurs. Solution: Disable auto swap. Update the app with a method named statuscheck to run the scripts. Re-enable auto swap and deploy the app to the Production slot. Does the solution meet the goal?",
    "options": [
      "No",
      "Yes"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts. Note: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment. <system.webServer> <applicationInitialization> <add initializationPage=\"/\" hostName=\"[app hostname]\" /> <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" /> </applicationInitialization> </system.webServer>   GMartinez",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 7,
    "id": "767d8dfdfab16dd43ae85265ec49c36e3780b19afbf70ff0d4a37ca19abf96de"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2. When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile- friendly version of the image must start in less than one minute. You need to design the process that starts the photo processing. Solution: Convert the Azure Storage account to a BlockBlobStorage storage account. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Not necessary to convert the account, instead move photo processing to an Azure Function triggered from the blob upload.. Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file- oriented workfiow. Note: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.   oleks",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 8,
    "id": "a61c0d82ab590e4564c139ed8ecd28026d7ba7366712caff90a9bad4d2a0a7b9"
  },
  {
    "question": "You develop a website. You plan to host the website in Azure. You expect the website to experience high trafic volumes after it is published. You must ensure that the website remains available and responsive while minimizing cost. You need to deploy the website. What should you do?",
    "options": [
      "Deploy the website to a virtual machine. Configure the virtual machine to automatically scale when the CPU load is high.",
      "Deploy the website to an App Service that uses the Shared service tier. Configure the App Service plan to automatically scale when the CPU load is high.",
      "Deploy the website to a virtual machine. Configure a Scale Set to increase the virtual machine instance count when the CPU load is high.",
      "Deploy the website to an App Service that uses the Standard service tier. Configure the App Service plan to automatically scale when the CPU load is high."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Windows Azure Web Sites (WAWS) offers 3 modes: Standard, Free, and Shared. Standard mode carries an enterprise-grade SLA (Service Level Agreement) of 99.9% monthly, even for sites with just one instance. Standard mode runs on dedicated instances, making it different from the other ways to buy Windows Azure Web Sites. Incorrect Answers: B: Shared and Free modes do not offer the scaling fiexibility of Standard, and they have some important limits. Shared mode, just as the name states, also uses shared Compute resources, and also has a CPU limit. So, while neither Free nor Shared is likely to be the best choice for your production environment due to these limits.   profesorklaus",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 13,
    "id": "2842d364589877ac3e7d16568b1f1db100d6395866cb64b0cce56828977e387f"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob. The app continues to time out after four minutes. The app must process the blob data. You need to ensure the app does not time out and processes the blob data. Solution: Use the Durable Function async pattern to process the blob data. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response. Note: Large, long-running functions can cause unexpected timeout issues. General best practices include: Whenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the HTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.   sasisang",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 16,
    "id": "f9d27779a00e0558aa8d5ea15fe6df54739ad167ab527ad2afad30ca8443ce8d"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob. The app continues to time out after four minutes. The app must process the blob data. You need to ensure the app does not time out and processes the blob data. Solution: Pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Large, long-running functions can cause unexpected timeout issues. General best practices include: Whenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the HTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.   msdevanms",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 17,
    "id": "83b8c4062351a30079ae5d3204a1c5a25a90404c6830f85cd67813484d9f087c"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob. The app continues to time out after four minutes. The app must process the blob data. You need to ensure the app does not time out and processes the blob data. Solution: Configure the app to use an App Service hosting plan and enable the Always On setting. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response. Note: Large, long-running functions can cause unexpected timeout issues. General best practices include: Whenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the HTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.   00avatar",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 18,
    "id": "244082fe432d7769be0d5e66d6d1314a9f55e9fb938b7fc1c8b4d7ca3cab38e6"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2. When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile- friendly version of the image must start in less than one minute. You need to design the process that starts the photo processing. Solution: Move photo processing to an Azure Function triggered from the blob upload. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file- oriented workfiow. Events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Note: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.   AnkanG",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 19,
    "id": "db20b2275e55080b27914ef1dcf94c27ed064766aa1bd4a9478a38083fe401bc"
  },
  {
    "question": "You are developing an application that uses Azure Blob storage. The application must read the transaction logs of all the changes that occur to the blobs and the blob metadata in the storage account for auditing purposes. The changes must be in the order in which they occurred, include only create, update, delete, and copy operations and be retained for compliance reasons. You need to process the transaction logs asynchronously. What should you do?",
    "options": [
      "Process all Azure Blob storage events by using Azure Event Grid with a subscriber Azure Function app.",
      "Enable the change feed on the storage account and process all changes for available events.",
      "Process all Azure Storage Analytics logs for successful blob events.",
      "Use the Azure Monitor HTTP Data Collector API and scan the request body for successful blob events."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Change feed support in Azure Blob Storage The purpose of the change feed is to provide transaction logs of all the changes that occur to the blobs and the blob metadata in your storage account. The change feed provides ordered, guaranteed, durable, immutable, read-only log of these changes. Client applications can read these logs at any time, either in streaming or in batch mode. The change feed enables you to build eficient and scalable solutions that process change events that occur in your Blob Storage account at a low cost.   Ummara",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 20,
    "id": "a103d7d5761d00cb06ad14cc688a6c194b4788eaee5949f021eb9fe0e9fd8654"
  },
  {
    "question": "You are developing an Azure Function App that processes images that are uploaded to an Azure Blob container. Images must be processed as quickly as possible after they are uploaded, and the solution must minimize latency. You create code to process images when the Function App is triggered. You need to configure the Function App. What should you do?",
    "options": [
      "Use an App Service plan. Configure the Function App to use an Azure Blob Storage input trigger.",
      "Use a Consumption plan. Configure the Function App to use an Azure Blob Storage trigger.",
      "Use a Consumption plan. Configure the Function App to use a Timer trigger.",
      "Use an App Service plan. Configure the Function App to use an Azure Blob Storage trigger.",
      "Use a Consumption plan. Configure the Function App to use an Azure Blob Storage input trigger."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The Blob storage trigger starts a function when a new or updated blob is detected. The blob contents are provided as input to the function. The Consumption plan limits a function app on one virtual machine (VM) to 1.5 GB of memory.   Kitkit",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 22,
    "id": "63b1d3a6a67d01115cae093c8d6aae4a41cb453df3b053af3855778c99ce8a8d"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2. When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile- friendly version of the image must start in less than one minute. You need to design the process that starts the photo processing. Solution: Create an Azure Function app that uses the Consumption hosting model and that is triggered from the blob upload. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "In the Consumption hosting plan, resources are added dynamically as required by your functions.   AndresMza",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 32,
    "id": "38ea10d7e222e96ed7ecb09de9ccc5322da89b4c58b5d6bd5ad4eb0547766a4d"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot. You need to ensure that scripts run and resources are available before a swap operation occurs. Solution: Update the app with a method named statuscheck to run the scripts. Update the app settings for the app. Set the WEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES with a path to the new method and appropriate response codes. Does the solution meet the goal?",
    "options": [
      "No",
      "Yes"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "These are valid warm-up behavior options, but are not helpful in fixing swap problems. Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts. Note: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment. <system.webServer> <applicationInitialization> <add initializationPage=\"/\" hostName=\"[app hostname]\" /> <add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" /> </applicationInitialization> </system.webServer>   Carlous",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 33,
    "id": "54bb25fcee5c512be670d1e284e4900b7a6e1e13330fdcb0eba8b580319ac9e6"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure Storage Blob storage. The storage account type is General-purpose V2. When photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile- friendly version of the image must start in less than one minute. You need to design the process that starts the photo processing. Solution: Use the Azure Blob Storage change feed to trigger photo processing. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The change feed is a log of changes that are organized into hourly segments but appended to and updated every few minutes. These segments are created only when there are blob change events that occur in that hour. Instead catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload. us/azure/storage/blobs/storage-blob-event-overview   finnishr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 38,
    "id": "a2e5f013946329683880005cc0ba6937211a8ad5289c13c910c1159fb0dbdaa3"
  },
  {
    "question": "You are developing a web application that runs as an Azure Web App. The web application stores data in Azure SQL Database and stores files in an Azure Storage account. The web application makes HTTP requests to external services as part of normal operations. The web application is instrumented with Application Insights. The external services are OpenTelemetry compliant. You need to ensure that the customer ID of the signed in user is associated with all operations throughout the overall system. What should you do?",
    "options": [
      "Add the customer ID for the signed in user to the CorrelationContext in the web application",
      "On the current SpanContext, set the TraceId to the customer ID for the signed in user",
      "Set the header Ocp-Apim-Trace to the customer ID for the signed in user",
      "Create a new SpanContext with the TraceFlags value set to the customer ID for the signed in user"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  willchenxa",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 39,
    "id": "fae94fc31b09d2e5a135af2ef6fb1c99868ae92f459d50fe22aadba3ca041b8c"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob. The app continues to time out after four minutes. The app must process the blob data. You need to ensure the app does not time out and processes the blob data. Solution: Update the functionTimeout property of the host.json project file to 10 minutes. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response. Note: Large, long-running functions can cause unexpected timeout issues. General best practices include: Whenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the HTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.   stfnmrr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 42,
    "id": "0f4711b654347926808acd1fb052107f9ee87bb98f171403f486d33fca916725"
  },
  {
    "question": "You develop Azure Durable Functions to manage vehicle loans. The loan process includes multiple actions that must be run in a specified order. One of the actions includes a customer credit check process, which may require multiple days to process. You need to implement Azure Durable Functions for the loan process. Which Azure Durable Functions type should you use?",
    "options": [
      "orchestrator",
      "client",
      "entity",
      "activity"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  imanonion",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 46,
    "id": "fdecd58dd563792c13816042684716ea18737c1290d50cdde7183251f3267b47"
  },
  {
    "question": "You are developing several microservices to run on Azure Container Apps. External HTTP ingress trafic has been enabled for the microservices. The microservices must be deployed to the same virtual network and write logs to the same Log Analytics workspace. You need to deploy the microservices. What should you do?",
    "options": [
      "Enable single revision mode.",
      "Use a separate environment for each container.",
      "Use a private container registry image and single image for all containers.",
      "Use a single environment for all containers.",
      "Enable multiple revision mode."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  MikeM27",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 52,
    "id": "5458359d5976d593045f8b22893360e96831accfc87cc1cc1288aa00c3ae786a"
  },
  {
    "question": "You are developing an ASP.NET Core app hosted in Azure App Service. The app requires custom claims to be returned from Microsoft Entra ID for user authorization. The claims must be removed when the app registration is removed. You need to include the custom claims in the user access token. What should you do?",
    "options": [
      "Require the scope during authentication.",
      "Configure the app to use the OAuth 2.0 authorization code fiow.",
      "Implement custom middleware to retrieve role information from Azure AD.",
      "Add the groups to the groupMembershipClaims attribute in the app manifest.",
      "Add the roles to the appRoles attribute in the app manifest."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  FeriAZ 1 month, 2 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 57,
    "id": "4511bd293eef7906efce2dadcd55e936b0bc4b6799a3dad559a3a31f75ca9176"
  },
  {
    "question": "You are developing a microservice to run on Azure Container Apps for a company. External HTTP ingress trafic has been enabled. The company requires that updates to the microservice must not cause downtime. You need to deploy an update to the microservices. What should you do?",
    "options": [
      "Enable single revision mode.",
      "Use multiple environments for each container.",
      "Use a private container registry and single image for all containers.",
      "Use a single environment for all containers.",
      "Enable multiple revision mode."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  AzDeveloper",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 58,
    "id": "ee4dd2a1cc430b7af4b26fdc204ab1b4efe71ee6b76fe1c4a6998e5c5ca148aa"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You deploy an Azure Container Apps app and disable ingress on the container app. Users report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances. You need to resolve the issue with the container app. Solution: Enable ingress, create an HTTP scale rule, and apply the rule to the container app. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  MarcosAn",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 60,
    "id": "811699fe62807549c036baa800066ba08eef3ad232b17f8cd12cd7b7cb7a2a8a"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You deploy an Azure Container Apps app and disable ingress on the container app. Users report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances. You need to resolve the issue with the container app. Solution: Enable ingress, create a custom scale rule, and apply the rule to the container app. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  jobolesonihal 2 weeks, 6 days ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 61,
    "id": "938955b2e7efcc7c01f48115cbbae765f11987972fd42b93340dbd92d54e4ac1"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You deploy an Azure Container Apps app and disable ingress on the container app. Users report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances. You need to resolve the issue with the container app. Solution: Enable ingress and configure the minimum replicas to 1 for the container app. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  Mahesh1222 2 weeks, 3 days ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 62,
    "id": "18d6d2e493dedaefef658dfed34357e1c75454d3cc93ca8ecdf973af08825710"
  },
  {
    "question": "You are building a website that uses Azure Blob storage for data storage. You configure Azure Blob storage lifecycle to move all blobs to the archive tier after 30 days. Customers have requested a service-level agreement (SLA) for viewing data older than 30 days. You need to document the minimum SLA for data recovery. Which SLA should you use?",
    "options": [
      "at least two days",
      "between one and 15 hours",
      "at least one day",
      "between zero and 60 minutes"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The archive access tier has the lowest storage cost. But it has higher data retrieval costs compared to the hot and cool tiers. Data in the archive tier can take several hours to retrieve depending on the priority of the rehydration. For small objects, a high priority rehydrate may retrieve the object from archive in under 1 hour.   homimi6115",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "2ae1306a467ce587dca1d401ba47c0a4efaffd2f5ba64bb37c2a4875105a83b5"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data. You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future. You need to implement a solution to receive the device data. Solution: Provision an Azure Event Grid. Configure the machine identifier as the partition key and enable capture. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  bbijls",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 7,
    "id": "0162bd58822da2035a325ddd76dc53f9ce73c4e79dbe7b4c0950a1bd1cb54432"
  },
  {
    "question": "You develop Azure solutions. A .NET application needs to receive a message each time an Azure virtual machine finishes processing data. The messages must NOT persist after being processed by the receiving application. You need to implement the .NET object that will receive the messages. Which object should you use?",
    "options": [
      "QueueClient",
      "SubscriptionClient",
      "TopicClient",
      "CloudQueueClient"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "A queue allows processing of a message by a single consumer. Need a CloudQueueClient to access the Azure VM. Incorrect Answers: B, C: In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern. It's useful for scaling to large numbers of recipients.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 8,
    "id": "3310838ffd983bbacc6d4aa3010e1c78eb1edbc9c82dce04bffd241ee8584e75"
  },
  {
    "question": "You develop Azure solutions. You must connect to a No-SQL globally-distributed database by using the .NET API. You need to create an object to configure and execute requests in the database. Which code segment should you use?",
    "options": [
      "new Container(EndpointUri, PrimaryKey);",
      "new Database(EndpointUri, PrimaryKey);",
      "new CosmosClient(EndpointUri, PrimaryKey);"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Example: // Create a new instance of the Cosmos Client this.cosmosClient = new CosmosClient(EndpointUri, PrimaryKey) //ADD THIS PART TO YOUR CODE await this.CreateDatabaseAsync();   Mr2302682",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 10,
    "id": "5a76eba7fa779ec702aa8dd26ae335b5e2c11d17a747de5a338511ff12871304"
  },
  {
    "question": "You have an existing Azure storage account that stores large volumes of data across multiple containers. You need to copy all data from the existing storage account to a new storage account. The copy process must meet the following requirements: ✑ Automate data movement. ✑ Minimize user input required to perform the operation. ✑ Ensure that the data movement process is recoverable. What should you use?",
    "options": [
      "AzCopy",
      "Azure Storage Explorer",
      "Azure portal",
      ".NET Storage Client Library"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "You can copy blobs, directories, and containers between storage accounts by using the AzCopy v10 command-line utility. The copy operation is synchronous so when the command returns, that indicates that all files have been copied.   kondapaturi",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 11,
    "id": "3842d4dc6bccec28f4b45e2df716bab7022c5cee7f2f0ee20f8eee0e62c23cf9"
  },
  {
    "question": "You develop and deploy a web application to Azure App Service. The application accesses data stored in an Azure Storage account. The account contains several containers with several blobs with large amounts of data. You deploy all Azure resources to a single region. You need to move the Azure Storage account to the new region. You must copy all data to the new region. What should you do first?",
    "options": [
      "Export the Azure Storage account Azure Resource Manager template",
      "Initiate a storage account failover",
      "Configure object replication for all blobs",
      "Use the AzCopy command line tool",
      "Create a new Azure Storage account in the current region F. Create a new subscription in the current region"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "To move a storage account, create a copy of your storage account in another region. Then, move your data to that account by using AzCopy, or another tool of your choice and finally, delete the resources in the source region. To get started, export, and then modify a Resource Manager template.   BhavikaSNN",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 23,
    "id": "d2abd1dfa0765fb7f76d918bafa7fca9609573b1c7769417d41dce2580b9d89c"
  },
  {
    "question": "An organization deploys Azure Cosmos DB. You need to ensure that the index is updated as items are created, updated, or deleted. What should you do?",
    "options": [
      "Set the indexing mode to Lazy.",
      "Set the value of the automatic property of the indexing policy to False.",
      "Set the value of the EnableScanInQuery option to True.",
      "Set the indexing mode to Consistent."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Azure Cosmos DB supports two indexing modes: Consistent: The index is updated synchronously as you create, update or delete items. This means that the consistency of your read queries will be the consistency configured for the account. None: Indexing is disabled on the container.   finnishr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 27,
    "id": "0b97931d8e50a39f70880ac1d6b5950cfc00eae3d2673b4056ecc40d8255511d"
  },
  {
    "question": "You create and publish a new Azure App Service web app. User authentication and authorization must use Azure Active Directory (Azure AD). You need to configure authentication and authorization. What should you do first?",
    "options": [
      "Add an identity provider.",
      "Map an existing custom DNS name.",
      "Create and configure a new app setting.",
      "Add a private certificate.",
      "Create and configure a managed identity."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  Firo",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 47,
    "id": "39ab79ee10d89cd124c46ceb2f684a930f053ef5d0585995cb96abe80641a86a"
  },
  {
    "question": "You have a Linux container-based console application that uploads image files from customer sites all over the world. A back-end system that runs on Azure virtual machines processes the images by using the Azure Blobs API. You are not permitted to make changes to the application. Some customer sites only have phone-based internet connections. You need to configure the console application to access the images. What should you use?",
    "options": [
      "Azure BlobFuse",
      "Azure Disks",
      "Azure Storage Network File System (NFS) 3.0 support",
      "Azure Files"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "  130nk3r5",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 49,
    "id": "efcd855020162e32d9755a19d06400e9181a128bff9bf1f474a799c6f3d5d3df"
  },
  {
    "question": "You create an Azure Cosmos DB for NoSQL database. You plan to use the Azure Cosmos DB .NET SDK v3 API for NoSQL to upload the following files: You receive the following error message when uploading the files: “413 Entity too large”. You need to determine which files you can upload to the Azure Cosmos DB for NoSQL database. Which files can you upload?",
    "options": [
      "File1, File2, File3, File4, and File5",
      "File1 and File2 only",
      "File1, File2, and File3 only",
      "File1, File2, File3, and File4 only",
      "File1 only"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  Jedi",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 53,
    "id": "1522af97717f259201b28b81b7b8a95679101e7678bc37d67e716f576a695f18"
  },
  {
    "question": "You are developing a Java application that uses Cassandra to store key and value data. You plan to use a new Azure Cosmos DB resource and the Cassandra API in the application. You create an Azure Active Directory (Azure AD) group named Cosmos DB Creators to enable provisioning of Azure Cosmos accounts, databases, and containers. The Azure AD group must not be able to access the keys that are required to access the data. You need to restrict access to the Azure AD group. Which role-based access control should you use?",
    "options": [
      "DocumentDB Accounts Contributor",
      "Cosmos Backup Operator",
      "Cosmos DB Operator",
      "Cosmos DB Account Reader"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Azure Cosmos DB now provides a new RBAC role, Cosmos DB Operator. This new role lets you provision Azure Cosmos accounts, databases, and containers, but can't access the keys that are required to access the data. This role is intended for use in scenarios where the ability to grant access to Azure Active Directory service principals to manage deployment operations for Cosmos DB is needed, including the account, database, and containers.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "86b798ea96ecae8d544387f74b4e12e9983f9a23d079b3da1417ca10823da102"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials. You plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level. You need to configure authorization. Solution: Configure the Azure Web App for the website to allow only authenticated requests and require Azure AD log on. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead in the Azure AD application's manifest, set value of the groupMembershipClaims option to All.   fadikh",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "07b93601ffdad94c63b3f12faeeacd8ef2974ef215447c7f1ddcba9bb292b540"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials. You plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level. You need to configure authorization. Solution: ✑ Create a new Azure AD application. In the application's manifest, set value of the groupMembershipClaims option to All. ✑ In the website, use the value of the groups claim from the JWT for the user to determine permissions. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "To configure Manifest to include Group Claims in Auth Token 1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory, and go to App registrations to find your application: 2. Click on your application (or search for it if you have a lot of apps) and edit the Manifest by clicking on it. 3. Locate the ג€groupMembershipClaimsג€ setting. Set its value to either ג€SecurityGroupג€ or ג€Allג€. To help you decide which: ✑ ג€SecurityGroupג€ - groups claim will contain the identifiers of all security groups of which the user is a member. ✑ ג€Allג€ - groups claim will contain the identifiers of all security groups and all distribution lists of which the user is a member Now your application will include group claims in your manifest and you can use this fact in your code.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 3,
    "id": "052f086bd334e76c25fa40ee0409edeb8462880da46bbe4e5b33ccf66832b3ce"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials. You plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level. You need to configure authorization. Solution: ✑ Create a new Azure AD application. In the application's manifest, define application roles that match the required permission levels for the application. ✑ Assign the appropriate Azure AD group to each role. In the website, use the value of the roles claim from the JWT for the user to determine permissions. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "To configure Manifest to include Group Claims in Auth Token 1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory, and go to App registrations to find your application: 2. Click on your application (or search for it if you have a lot of apps) and edit the Manifest by clicking on it. 3. Locate the ג€groupMembershipClaimsג€ setting. Set its value to either ג€SecurityGroupג€ or ג€Allג€. To help you decide which: ✑ ג€SecurityGroupג€ - groups claim will contain the identifiers of all security groups of which the user is a member. ✑ ג€Allג€ - groups claim will contain the identifiers of all security groups and all distribution lists of which the user is a member Now your application will include group claims in your manifest and you can use this fact in your code.   [Removed]",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 4,
    "id": "c4f42e1029c96f47d7d3f0d59cfa6a3b509cbd22a424170da0ef4909398aa2e2"
  },
  {
    "question": "You have an application that includes an Azure Web app and several Azure Function apps. Application secrets including connection strings and certificates are stored in Azure Key Vault. Secrets must not be stored in the application or application runtime environment. Changes to Azure Active Directory (Azure AD) must be minimized. You need to design the approach to loading application secrets. What should you do?",
    "options": [
      "Create a single user-assigned Managed Identity with permission to access Key Vault and configure each App Service to use that Managed Identity.",
      "Create a single Azure AD Service Principal with permission to access Key Vault and use a client secret from within the App Services to access Key Vault.",
      "Create a system assigned Managed Identity in each App Service with permission to access Key Vault.",
      "Create an Azure AD Service Principal with Permissions to access Key Vault for each App Service and use a certificate from within the App Services to access Key Vault."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Use Key Vault references for App Service and Azure Functions. Key Vault references currently only support system-assigned managed identities. User-assigned identities cannot be used.   AssilAbdulrahim",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 9,
    "id": "57be620b35befbb014711d02c9b9068d4b7fb513dc3e776a902c49abc2bbe457"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a medical records document management website. The website is used to store scanned copies of patient intake forms. If the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised. You need to store the intake forms according to the requirements. Solution: 1. Create an Azure Key Vault key named skey. 2. Encrypt the intake forms using the public key portion of skey. 3. Store the encrypted data in Azure Blob storage. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  pac1311",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 10,
    "id": "9c497390a3e40d1001bccab7798c62e2fa7a0c97650e7f47640cb51bd01f090e"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a medical records document management website. The website is used to store scanned copies of patient intake forms. If the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised. You need to store the intake forms according to the requirements. Solution: 1. Create an Azure Cosmos DB database with Storage Service Encryption enabled. 2. Store the intake forms in the Azure Cosmos DB database. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead use an Azure Key vault and public key encryption. Store the encrypted from in Azure Storage Blob storage.   cbn",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 11,
    "id": "2d8f5888b14e8ea0ef5032c4ab47d5f5c4701040ce73ad0ef948b7d9a34d10dc"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a medical records document management website. The website is used to store scanned copies of patient intake forms. If the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised. You need to store the intake forms according to the requirements. Solution: Store the intake forms as Azure Key Vault secrets. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead use an Azure Key vault and public key encryption. Store the encrypted from in Azure Storage Blob storage.   clarionprogrammer",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 12,
    "id": "a1a9c3872e6046383397fdbc01fd564547cc1b25d91e5b536555168f13a611a1"
  },
  {
    "question": "Your company is developing an Azure API hosted in Azure. You need to implement authentication for the Azure API to access other Azure resources. You have the following requirements: ✑ All API calls must be authenticated. ✑ Callers to the API must not send credentials to the API. Which authentication mechanism should you use?",
    "options": [
      "Basic",
      "Anonymous",
      "Managed identity",
      "Client certificate"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Azure Active Directory Managed Service Identity (MSI) gives your code an automatically managed identity for authenticating to Azure services, so that you can keep credentials out of your code. Note: Use the authentication-managed-identity policy to authenticate with a backend service using the managed identity. This policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme. Incorrect Answers: A: Use the authentication-basic policy to authenticate with a backend service using Basic authentication. This policy effectively sets the HTTP Authorization header to the value corresponding to the credentials provided in the policy. B: Anonymous is no authentication at all. D: Your code needs credentials to authenticate to cloud services, but you want to limit the visibility of those credentials as much as possible. Ideally, they never appear on a developer's workstation or get checked-in to source control. Azure Key Vault can store credentials securely so they aren't in your code, but to retrieve them you need to authenticate to Azure Key Vault. To authenticate to Key Vault, you need a credential! A classic bootstrap problem.   clarionprogrammer",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 14,
    "id": "74dd29422ce631f3baf736974cf0914c4e98868bdd18368529a40ffdb658fef0"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop Azure solutions. You must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager. You need to obtain an Azure Resource Manager access token. Solution: Use an X.509 certificate to authenticate the VM with Azure Resource Manager. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.   KeerthiKP",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 16,
    "id": "5457048b965833a45abd1d24ee7e4ac4070c484f94d31128c9c5d4999f4d2494"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop Azure solutions. You must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager. You need to obtain an Azure Resource Manager access token. Solution: Use the Reader role-based access control (RBAC) role to authenticate the VM with Azure Resource Manager. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 17,
    "id": "98154e1feeafbe3571b39476092d2333d394ce80994c2ca9af01435fe1d87e6a"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials. You plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level. You need to configure authorization. Solution: ✑ Configure and use Integrated Windows Authentication in the website. ✑ In the website, query Microsoft Graph API to load the groups to which the user is a member. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Microsoft Graph is a RESTful web API that enables you to access Microsoft Cloud service resources. Instead in the Azure AD application's manifest, set value of the groupMembershipClaims option to All. In the website, use the value of the groups claim from the JWT for the user to determine permissions.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 19,
    "id": "96ded5cce7c327998160c3bb7a89077489429be43ca0ee84c71764441b935ad2"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You develop Azure solutions. You must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager. You need to obtain an Azure Resource Manager access token. Solution: Run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Get an access token using the VM's system-assigned managed identity and use it to call Azure Resource Manager You will need to use PowerShell in this portion. 1. In the portal, navigate to Virtual Machines and go to your Windows virtual machine and in the Overview, click Connect. 2. Enter in your Username and Password for which you added when you created the Windows VM. 3. Now that you have created a Remote Desktop Connection with the virtual machine, open PowerShell in the remote session. 4. Using the Invoke-WebRequest cmdlet, make a request to the local managed identity for Azure resources endpoint to get an access token for Azure Resource Manager. Example: $response = Invoke-WebRequest -Uri ' management.azure.com/' -Method GET -Headers @{Metadata=\"true\"}   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 20,
    "id": "d90a5d6bcb7ba545544999703ef6b67f815e6751f52b0a8aff4152f6b98f87a1"
  },
  {
    "question": "You develop an app that allows users to upload photos and videos to Azure storage. The app uses a storage REST API call to upload the media to a blob storage account named Account1. You have blob storage containers named Container1 and Container2. Uploading of videos occurs on an irregular basis. You need to copy specific blobs from Container1 to Container2 when a new video is uploaded. What should you do?",
    "options": [
      "Copy blobs to Container2 by using the Put Blob operation of the Blob Service REST API",
      "Create an Event Grid topic that uses the Start-AzureStorageBlobCopy cmdlet",
      "Use AzCopy with the Snapshot switch to copy blobs to Container2",
      "Download the blob to a virtual machine and then upload the blob to Container2"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The Start-AzureStorageBlobCopy cmdlet starts to copy a blob. Example 1: Copy a named blob - C:\\PS>Start-AzureStorageBlobCopy -SrcBlob \"ContosoPlanning2015\" -DestContainer \"ContosoArchives\" -SrcContainer \"ContosoUploads\" This command starts the copy operation of the blob named ContosoPlanning2015 from the container named ContosoUploads to the container named ContosoArchives.   AnonymousJhb",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 22,
    "id": "101dc65355ff41694e662bea97cbef72b456dee6ab0cc258d31f0b7b2a97df0c"
  },
  {
    "question": "You are developing an ASP.NET Core website that uses Azure FrontDoor. The website is used to build custom weather data sets for researchers. Data sets are downloaded by users as Comma Separated Value (CSV) files. The data is refreshed every 10 hours. Specific files must be purged from the FrontDoor cache based upon Response Header values. You need to purge individual assets from the Front Door cache. Which type of cache purge should you use?",
    "options": [
      "single path",
      "wildcard",
      "root domain"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "These formats are supported in the lists of paths to purge: ✑ Single path purge: Purge individual assets by specifying the full path of the asset (without the protocol and domain), with the file extension, for example, / [1] ✑ Wildcard purge: Asterisk (*) may be used as a wildcard. Purge all folders, subfolders, and files under an endpoint with /* in the path or purge all subfolders and files under a specific folder by specifying the folder followed by /*, for example, /pictures/*. ✑ Root domain purge: Purge the root of the endpoint with \"/\" in the path.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 23,
    "id": "0bcc5fa126073d4b8e9fe6ec0e7313bcda932d8d39027c074abc8bc5c58233ab"
  },
  {
    "question": "Your company is developing an Azure API. You need to implement authentication for the Azure API. You have the following requirements: All API calls must be secure. ✑ Callers to the API must not send credentials to the API. Which authentication mechanism should you use?",
    "options": [
      "Basic",
      "Anonymous",
      "Managed identity",
      "Client certificate"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Use the authentication-managed-identity policy to authenticate with a backend service using the managed identity of the API Management service. This policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme.   MasDen",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 24,
    "id": "004d392d62e790fbd445b200e6dc194fd0bf211eea5ed55fcbfafbc2f90066c9"
  },
  {
    "question": "You are a developer for a SaaS company that offers many web services. All web services for the company must meet the following requirements: ✑ Use API Management to access the services ✑ Use OpenID Connect for authentication ✑ Prevent anonymous usage A recent security audit found that several web services can be called without any authentication. Which API Management policy should you implement?",
    "options": [
      "jsonp",
      "authentication-certificate",
      "check-header",
      "validate-jwt"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Add the validate-jwt policy to validate the OAuth token for every incoming request. Incorrect Answers: A: The jsonp policy adds JSON with padding (JSONP) support to an operation or an API to allow cross-domain calls from JavaScript browser- based clients. JSONP is a method used in JavaScript programs to request data from a server in a different domain. JSONP bypasses the limitation enforced by most web browsers where access to web pages must be in the same domain. JSONP - Adds JSON with padding (JSONP) support to an operation or an API to allow cross-domain calls from JavaScript browser-based clients.   Cornholioz",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 25,
    "id": "bdaa93814205e1126e3536422f98608cebc444d16880e7bd10497f6c86f5ac58"
  },
  {
    "question": "You develop and deploy an Azure Logic app that calls an Azure Function app. The Azure Function app includes an OpenAPI (Swagger) definition and uses an Azure Blob storage account. All resources are secured by using Azure Active Directory (Azure AD). The Azure Logic app must securely access the Azure Blob storage account. Azure AD resources must remain if the Azure Logic app is deleted. You need to secure the Azure Logic app. What should you do?",
    "options": [
      "Create a user-assigned managed identity and assign role-based access controls.",
      "Create an Azure AD custom role and assign the role to the Azure Blob storage account.",
      "Create an Azure Key Vault and issue a client certificate.",
      "Create a system-assigned managed identity and issue a client certificate.",
      "Create an Azure AD custom role and assign role-based access controls."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "To give a managed identity access to an Azure resource, you need to add a role to the target resource for that identity. Note: To easily authenticate access to other resources that are protected by Azure Active Directory (Azure AD) without having to sign in and provide credentials or secrets, your logic app can use a managed identity (formerly known as Managed Service Identity or MSI). Azure manages this identity for you and helps secure your credentials because you don't have to provide or rotate secrets. If you set up your logic app to use the system-assigned identity or a manually created, user-assigned identity, the function in your logic app can also use that same identity for authentication. management/api-management-howto-mutual-certificates-for-clients   aradice",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 31,
    "id": "57abaef30a6418a358423333db3ade411f0332622a487e4ec33f14750ca4f3c8"
  },
  {
    "question": "You deploy an Azure App Service web app. You create an app registration for the app in Azure Active Directory (Azure AD) and Twitter. The app must authenticate users and must use SSL for all communications. The app must use Twitter as the identity provider. You need to validate the Azure AD request in the app code. What should you validate?",
    "options": [
      "ID token header",
      "ID token signature",
      "HTTP response code",
      "Tenant ID"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  RaghavMGupta",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 37,
    "id": "ec8dc0d76bb5fefd74c6427529ec2668aaea38d05341f83115545e8f134ba24e"
  },
  {
    "question": "A development team is creating a new REST API. The API will store data in Azure Blob storage. You plan to deploy the API to Azure App Service. Developers must access the Azure Blob storage account to develop the API for the next two months. The Azure Blob storage account must not be accessible by the developers after the two-month time period. You need to grant developers access to the Azure Blob storage account. What should you do?",
    "options": [
      "Generate a shared access signature (SAS) for the Azure Blob storage account and provide the SAS to all developers.",
      "Create and apply a new lifecycle management policy to include a last accessed date value. Apply the policy to the Azure Blob storage account.",
      "Provide all developers with the access key for the Azure Blob storage account. Update the API to include the Coordinated Universal Time (UTC) timestamp for the request header.",
      "Grant all developers access to the Azure Blob storage account by assigning role-based access control (RBAC) roles."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  alexein74",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 38,
    "id": "79c07e216bea4daf06aebcce4e3e20e8889db1509fe6115b83bff46739dd7d48"
  },
  {
    "question": "You manage a data processing application that receives requests from an Azure Storage queue. You need to manage access to the queue. You have the following requirements: ✑ Provide other applications access to the Azure queue. ✑ Ensure that you can revoke access to the queue without having to regenerate the storage account keys. ✑ Specify access at the queue level and not at the storage account level. Which type of shared access signature (SAS) should you use?",
    "options": [
      "Service SAS with a stored access policy",
      "Account SAS",
      "User Delegation SAS",
      "Service SAS with ad hoc SAS"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "A service SAS is secured with the storage account key. A service SAS delegates access to a resource in only one of the Azure Storage services: Blob storage, Queue storage, Table storage, or Azure Files. Stored access policies give you the option to revoke permissions for a service SAS without having to regenerate the storage account keys. Incorrect Answers: Account SAS: Account SAS is specified at the account level. It is secured with the storage account key. User Delegation SAS: A user delegation SAS applies to Blob storage only.   sghaha",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 44,
    "id": "86708fdd0f930287b86cc487a12e852c6661609544cdd84e586815636f4ccc94"
  },
  {
    "question": "You are building a web application that uses the Microsoft identity platform for user authentication. You are implementing user identification for the web application. You need to retrieve a claim to uniquely identify a user. Which claim type should you use?",
    "options": [
      "aud",
      "nonce",
      "oid",
      "idp"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "oid -The object identifier for the user in Azure AD. This value is the immutable and non-reusable identifier of the user. Use this value, not email, as a unique identifier for users; email addresses can change. If you use the Azure AD Graph API in your app, object ID is that value used to query profile information. Incorrect: Not A: aud - Who the token was issued for. This will be the application's client ID.   serpevi",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 46,
    "id": "e7e124b1c40d5dbd425129542cc7ca061195205ddf350d1297eea5116dac084a"
  },
  {
    "question": "You are developing an Azure Function that calls external APIs by providing an access token for the API. The access token is stored in a secret named token in an Azure Key Vault named mykeyvault. You need to ensure the Azure Function can access to the token. Which value should you store in the Azure Function App configuration?",
    "options": [
      "KeyVault:mykeyvault;Secret:token",
      "App:Settings:Secret:mykeyvault:token",
      "AZUREKVCONNSTR_",
      "@Microsoft.KeyVault(SecretUri="
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Add Key Vault secrets reference in the Function App configuration. Syntax: @Microsoft.KeyVault(SecretUri={copied identifier for the username secret})   lorenaizzo",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 47,
    "id": "801cca0b90ffa9bf08200d27c17f24c661ade37ac0d106875ff37802daa757d6"
  },
  {
    "question": "A company maintains multiple web and mobile applications. Each application uses custom in-house identity providers as well as social identity providers. You need to implement single sign-on (SSO) for all the applications. What should you do?",
    "options": [
      "Use Azure Active Directory B2C (Azure AD B2C) with custom policies.",
      "Use Azure Active Directory B2B (Azure AD B2B) and enable external collaboration.",
      "Use Azure Active Directory B2C (Azure AD B2C) with user fiows.",
      "Use Azure Active Directory B2B (Azure AD B2B)."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "You can add Google as an identity provider for B2B guest users. Federation with SAML/WS-Fed identity providers for guest users. Make sure your organization's external collaboration settings are configured such that you're allowed to invite guests. Note 1: As a user who is assigned any of the limited administrator directory roles, you can use the Azure portal to invite B2B collaboration users. You can invite guest users to the directory, to a group, or to an application. After you invite a user through any of these methods, the invited user's account is added to Azure Active Directory (Azure AD), with a user type of Guest. Note 2: Direct federation in Azure Active Directory is now referred to as SAML/WS-Fed identity provider (IdP) federation. directory/external-identities/add-users-administrator   tcybu",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 48,
    "id": "ac9e4d73629412e8534d73897722e02087f02edbf5ca0cabaa529b39e8bd7427"
  },
  {
    "question": "You are developing a user portal for a company. You need to create a report for the portal that lists information about employees who are subject matter experts for a specific topic. You must ensure that administrators have full control and consent over the data. Which technology should you use?",
    "options": [
      "Microsoft Graph data connect",
      "Microsoft Graph API",
      "Microsoft Graph connectors"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Data Connect grants a more granular control and consent model: you can manage data, see who is accessing it, and request specific properties of an entity. This enhances the Microsoft Graph model, which grants or denies applications access to entire entities. Microsoft Graph Data Connect augments Microsoft Graph's transactional model with an intelligent way to access rich data at scale. The data covers how workers communicate, collaborate, and manage their time across all the applications and services in Microsoft 365. Incorrect: Not B: The Microsoft Graph API is a RESTful web API that enables you to access Microsoft Cloud service resources. After you register your app and get authentication tokens for a user or service, you can make requests to the Microsoft Graph API. A simplistic definition of a Graph API is an API that models the data in terms of nodes and edges (objects and relationships) and allows the client to interact with multiple nodes in a single request. Not C: Microsoft Graph connectors, your organization can index third-party data so that it appears in Microsoft Search results. With Microsoft Graph connectors, your organization can index third-party data so that it appears in Microsoft Search results.   nvtienanh",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 50,
    "id": "07995e4d1ef2452ca05edf78c38631abcfbe0b97c9cbf2ddf6b5a30a8d3b966d"
  },
  {
    "question": "You are developing a web application that uses the Microsoft identity platform for user and resource authentication. The web application calls several REST APIs. A REST API call must read the user’s calendar. The web application requires permission to send an email as the user. You need to authorize the web application and the API. Which parameter should you use?",
    "options": [
      "tenant",
      "code_challenge",
      "state",
      "client_id",
      "scope"
    ],
    "answerIndexes": [
      4
    ],
    "answer": "  alexein74",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 55,
    "id": "8d6b3d93355561d4032c2ed19882b685239a2033028db8f30ecface8668d9065"
  },
  {
    "question": "You develop and deploy an Azure App Service web app named App1. You create a new Azure Key Vault named Vault1. You import several API keys, passwords, certificates, and cryptographic keys into Vault1. You need to grant App1 access to Vault1 and automatically rotate credentials. Credentials must not be stored in code. What should you do?",
    "options": [
      "Enable App Service authentication for Appl. Assign a custom RBAC role to Vault1.",
      "Add a TLS/SSL binding to App1.",
      "Upload a self-signed client certificate to Vault1. Update App1 to use the client certificate.",
      "Assign a managed identity to App1."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "  alexein74",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 58,
    "id": "22ccb52eb58a041f8f02fe7d0a37dc01e6402ab2462358b3e62cb407be44833a"
  },
  {
    "question": "You are developing a Java application to be deployed in Azure. The application stores sensitive data in Azure Cosmos DB. You need to configure Always Encrypted to encrypt the sensitive data inside the application. What should you do first?",
    "options": [
      "Create a new container to include an encryption policy with the JSON properties to be encrypted.",
      "Create a customer-managed key (CMK) and store the key in a new Azure Key Vault instance.",
      "Create a data encryption key (DEK) by using the Azure Cosmos DB SDK and store the key in Azure Cosmos DB.",
      "Create an Azure AD managed identity and assign the identity to a new Azure Key Vault instance."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  alexein74",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 59,
    "id": "7a6299a668d36f59d316e3833344780eb8412ae299738a3726de443375ade959"
  },
  {
    "question": "You are developing several Azure API Management (APIM) hosted APIs. You must transform the APIs to hide private backend information and obscure the technology stack used to implement the backend processing. You need to protect all APIs. What should you do?",
    "options": [
      "Configure and apply a new inbound policy scoped to a product.",
      "Configure and apply a new outbound policy scoped to the operation.",
      "Configure and apply a new outbound policy scoped to global.",
      "Configure and apply a new backend policy scoped to global."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  Ciupaz 3 months, 3 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 66,
    "id": "09affa0e0ec946f8ff2403d400f8b5d2bc9742eb8cd5d7e392c5c963f6a0d4ad"
  },
  {
    "question": "Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Munson’s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms. Current Environment - The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff. Corporate website - • The company hosts a public website located at The site supports farmers and distributors who request agricultural production resources. Farms - • The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications. Distributors - • Distributors integrate their applications with data that is accessible by using APIs hosted at to receive and update resource data. Requirements - The application components must meet the following requirements: Corporate website - • The site must be migrated to Azure App Service. • Costs must be minimized when hosting in Azure. • Applications must automatically scale independent of the compute resources. • All code changes must be validated by internal staff before release to production. • File transfer speeds must improve, and webpage-load performance must increase. • All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit. • A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests. Farms - • Farmers must authenticate to applications by using Microsoft Entra ID. Distributors - • The company must track a custom telemetry value with each API call and monitor performance of all APIs. • API telemetry values must be charted to evaluate variations and trends for resource data. Internal staff - • App and API updates must be validated before release to production. • Staff must be able to select a link to direct them back to the production app when validating an app or API update. • Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID. Security - • All web communications must be secured by using TLS/HTTPS. • Web content must be restricted by country/region to support corporate compliance standards. • The principle of least privilege must be applied when providing any user rights or process access rights. • Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication. Issues - Corporate website - • Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high. • Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking trafic are high. • Internal staff report webpage load sizes are large and take a long time to load. • Developers receive authentication errors to Service Bus when they debug locally. Distributors - • Many API telemetry values are sent in a short period of time. Telemetry trafic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs. You need to secure the corporate website to meet the security requirements. What should you do?",
    "options": [
      "Create an Azure Cache for Redis instance. Update the code to support the cache.",
      "Create an Azure Content Delivery Network profile and endpoint. Configure the endpoint. С. Create an App Service instance with a standard plan. Configure the custom domain with a TLS/SSL certificate.",
      "Create an Azure Application Gateway with a Web Application Firewall (WAF). Configure end-to-end TLS encryption and the WAF."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 70,
    "id": "7106bacff00ab53196f405adfca1e8991f10c588cb7951280f02b05028466559"
  },
  {
    "question": "You are developing an application that uses keys stored in Azure Key Vault. You need to enforce a specific cryptographic algorithm and key size for keys stored in the vault. What should you use?",
    "options": [
      "Secret versioning",
      "Azure Policy",
      "Key Vault Firewall",
      "Access policies"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  FeriAZ 2 months, 3 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 73,
    "id": "5337f9cf36b1619a08e4dac38dac7d2a54c167d3b200210ff770e6c50729594a"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals. You are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output. You must use a storage mechanism with the following requirements: ✑ Share session state across all ASP.NET web applications. ✑ Support controlled, concurrent access to the same session state data for multiple readers and a single writer. ✑ Save full HTTP responses for concurrent requests. You need to store the information. Proposed Solution: Enable Application Request Routing (ARR). Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead deploy and configure Azure Cache for Redis. Update the web applications.   gematsaljoa",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "758f984bb03dbb203aa75a9b6b94ad6a4953ed9059720733684c5a1193887d3f"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals. You are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output. You must use a storage mechanism with the following requirements: ✑ Share session state across all ASP.NET web applications. ✑ Support controlled, concurrent access to the same session state data for multiple readers and a single writer. ✑ Save full HTTP responses for concurrent requests. You need to store the information. Proposed Solution: Deploy and configure an Azure Database for PostgreSQL. Update the web applications. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead deploy and configure Azure Cache for Redis. Update the web applications.   profesorklaus",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 3,
    "id": "708543950582b15baaa6fbc271c68f9c091324049e1a8e9f972d12e8fbb4103e"
  },
  {
    "question": "You develop a gateway solution for a public facing news API. The news API back end is implemented as a RESTful service and uses an OpenAPI specification. You need to ensure that you can access the news API by using an Azure API Management service instance. Which Azure PowerShell command should you run?",
    "options": [
      "Import-AzureRmApiManagementApi -Context $ApiMgmtContext -SpecificationFormat \"Swagger\" -SpecificationPath $SwaggerPath -Path $Path",
      "New-AzureRmApiManagementBackend -Context $ApiMgmtContext -Url $Url -Protocol http",
      "New-AzureRmApiManagement -ResourceGroupName $ResourceGroup -Name $Name ג€\"Location $Location -Organization $Org - AdminEmail $AdminEmail",
      "New-AzureRmApiManagementBackendProxy -Url $ApiUrl"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "New-AzureRmApiManagementBackendProxy creates a new Backend Proxy Object which can be piped when creating a new Backend entity. Example: Create a Backend Proxy In-Memory Object PS C:\\>$secpassword = ConvertTo-SecureString \"PlainTextPassword\" -AsPlainText -Force PS C:\\>$proxyCreds = New-Object System.Management.Automation.PSCredential (\"foo\", $secpassword) PS C:\\>$credential = New-AzureRmApiManagementBackendProxy -Url \" -ProxyCredential $proxyCreds PS C:\\>$apimContext = New-AzureRmApiManagementContext -ResourceGroupName \"Api-Default-WestUS\" -ServiceName \"contoso\" PS C:\\>$backend = New-AzureRmApiManagementBackend -Context $apimContext -BackendId 123 -Url ' - Protocol http -Title \"first backend\" -SkipCertificateChainValidation $true -Proxy $credential -Description \"backend with proxy server\" Creates a Backend Proxy Object and sets up Backend Incorrect Answers: A: The Import-AzureRmApiManagementApi cmdlet imports an Azure API Management API from a file or a URL in Web Application Description Language (WADL), Web Services Description Language (WSDL), or Swagger format. B: New-AzureRmApiManagementBackend creates a new backend entity in Api Management. C: The New-AzureRmApiManagement cmdlet creates an API Management deployment in Azure API Management. 6.13.0   Vano6k",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 7,
    "id": "893ad293958d3cfadf5bd2804a18529114921c5fd3966f7508fda95dcd33e86c"
  },
  {
    "question": "You are developing an Azure function that connects to an Azure SQL Database instance. The function is triggered by an Azure Storage queue. You receive reports of numerous System.InvalidOperationExceptions with the following message: `Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.` You need to prevent the exception. What should you do?",
    "options": [
      "In the host.json file, decrease the value of the batchSize option",
      "Convert the trigger to Azure Event Hub",
      "Convert the Azure Function to the Premium plan",
      "In the function.json file, change the value of the type option to queueScaling"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "With the Premium plan the max outbound connections per instance is unbounded compared to the 600 active (1200 total) in a Consumption plan. Note: The number of available connections is limited partly because a function app runs in a sandbox environment. One of the restrictions that the sandbox imposes on your code is a limit on the number of outbound connections, which is currently 600 active (1,200 total) connections per instance. When you reach this limit, the functions runtime writes the following message to the logs: Host thresholds exceeded: Connections. functions/functions-scale#service-limits   VK7Az204",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 9,
    "id": "1aba737b4b7ca23d40037b84bdaa7a2b4e559373560fa857229e973abc978beb"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals. You are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output. You must use a storage mechanism with the following requirements: ✑ Share session state across all ASP.NET web applications. ✑ Support controlled, concurrent access to the same session state data for multiple readers and a single writer. ✑ Save full HTTP responses for concurrent requests. You need to store the information. Proposed Solution: Deploy and configure Azure Cache for Redis. Update the web applications. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "The session state provider for Azure Cache for Redis enables you to share session information between different instances of an ASP.NET web application. The same connection can be used by multiple concurrent threads. Redis supports both read and write operations. The output cache provider for Azure Cache for Redis enables you to save the HTTP responses generated by an ASP.NET web application. Note: Using the Azure portal, you can also configure the eviction policy of the cache, and control access to the cache by adding users to the roles provided. These roles, which define the operations that members can perform, include Owner, Contributor, and Reader. For example, members of the Owner role have complete control over the cache (including security) and its contents, members of the Contributor role can read and write information in the cache, and members of the Reader role can only retrieve data from the cache.   Prakash4691",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 10,
    "id": "cbb2c0a8e31b91463a7786545582a1568be6af19ed0ef4c323a61574188b0a4e"
  },
  {
    "question": "You are developing applications for a company. You plan to host the applications on Azure App Services. The company has the following requirements: ✑ Every five minutes verify that the websites are responsive. ✑ Verify that the websites respond within a specified time threshold. Dependent requests such as images and JavaScript files must load properly. ✑ Generate alerts if a website is experiencing issues. ✑ If a website fails to load, the system must attempt to reload the site three more times. You need to implement this process with the least amount of effort. What should you do?",
    "options": [
      "Create a Selenium web test and configure it to run from your workstation as a scheduled task.",
      "Set up a URL ping test to query the home page.",
      "Create an Azure function to query the home page.",
      "Create a multi-step web test to query the home page.",
      "Create a Custom Track Availability Test to query the home page."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "You can monitor a recorded sequence of URLs and interactions with a website via multi-step web tests. Incorrect Answers: A: Selenium is an umbrella project for a range of tools and libraries that enable and support the automation of web browsers. It provides extensions to emulate user interaction with browsers, a distribution server for scaling browser allocation, and the infrastructure for implementations of the W3C WebDriver specification that lets you write interchangeable code for all major web browsers.   kishe",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 15,
    "id": "e14128eba77a40001aa8d3f579aed8c1ad700d7e50c0061cc47bc6730ec5be43"
  },
  {
    "question": "You develop and add several functions to an Azure Function app that uses the latest runtime host. The functions contain several REST API endpoints secured by using SSL. The Azure Function app runs in a Consumption plan. You must send an alert when any of the function endpoints are unavailable or responding too slowly. You need to monitor the availability and responsiveness of the functions. What should you do?",
    "options": [
      "Create a URL ping test.",
      "Create a timer triggered function that calls TrackAvailability() and send the results to Application Insights.",
      "Create a timer triggered function that calls GetMetric(\"Request Size\") and send the results to Application Insights.",
      "Add a new diagnostic setting to the Azure Function app. Enable the FunctionAppLogs and Send to Log Analytics options."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "You can create an Azure Function with TrackAvailability() that will run periodically according to the configuration given in TimerTrigger function with your own business logic. The results of this test will be sent to your Application Insights resource, where you will be able to query for and alert on the availability results data. This allows you to create customized tests similar to what you can do via Availability Monitoring in the portal. Customized tests will allow you to write more complex availability tests than is possible using the portal UI, monitor an app inside of your Azure VNET, change the endpoint address, or create an availability test even if this feature is not available in your region.   mlantonis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 16,
    "id": "ac6d26b01a9aaf112dbecaf56c9bb6ddd15ed31fd03f64911bafc9def7576c78"
  },
  {
    "question": "An organization hosts web apps in Azure. The organization uses Azure Monitor. You discover that configuration changes were made to some of the web apps. You need to identify the configuration changes. Which Azure Monitor log should you review?",
    "options": [
      "AppServiceAppLogs",
      "AppServiceEnvironmentPlatformlogs",
      "AppServiceConsoleLogs",
      "AppServiceAuditLogs"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "The log type AppServiceEnvironmentPlatformLogs handles the App Service Environment: scaling, configuration changes, and status logs. Incorrect: AppServiceAppLogs contains logs generated through your application. AppServiceAuditLogs logs generated when publishing users successfully log on via one of the App Service publishing protocols.",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 28,
    "id": "8faf5242bf7e96f2975a2d97c3f99f8c7331dc353bb981d4de86322c270231d9"
  },
  {
    "question": "You develop and deploy an Azure App Service web app to a production environment. You enable the Always On setting and the Application Insights site extensions. You deploy a code update and receive multiple failed requests and exceptions in the web app. You need to validate the performance and failure counts of the web app in near real time. Which Application Insights tool should you use?",
    "options": [
      "Profiler",
      "Smart Detection",
      "Live Metrics Stream",
      "Application Map",
      "Snapshot Debugger"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Live Metrics Stream - Deploying the latest build can be an anxious experience. If there are any problems, you want to know about them right away, so that you can back out if necessary. Live Metrics Stream gives you key metrics with a latency of about one second. With Live Metrics Stream, you can: * Validate a fix while it's released, by watching performance and failure counts. * Etc. Incorrect: * Profiler Azure Application Insights Profiler provides performance traces for applications running in production in Azure. Profiler: Captures the data automatically at scale without negatively affecting your users. Helps you identify the ג€hotג€ code path spending the most time handling a particular web request. * Snapshot debugger When an exception occurs, you can automatically collect a debug snapshot from your live web application. The snapshot shows the state of source code and variables at the moment the exception was thrown. The Snapshot Debugger in Azure Application Insights monitors exception telemetry from your web app. It collects snapshots on your top-throwing exceptions so that you have the information you need to diagnose issues in production.",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 29,
    "id": "2911efc34443ba7465e73b15efba9c47dcfb43593f42b98371c2eb782f60c4f5"
  },
  {
    "question": "You are developing an Azure-based web application. The application goes ofiine periodically to perform ofiine data processing. While the application is ofiine, numerous Azure Monitor alerts fire which result in the on-call developer being paged. The application must always log when the application is ofiine for any reason. You need to ensure that the on-call developer is not paged during ofiine processing. What should you do?",
    "options": [
      "Add Azure Monitor alert processing rules to suppress notifications.",
      "Disable Azure Monitor Service Health Alerts during ofiine processing.",
      "Create an Azure Monitor Metric Alert.",
      "Build an Azure Monitor action group that suppresses the alerts."
    ],
    "answerIndexes": [
      3
    ],
    "answer": "You can use alert processing rules to add action groups or remove (suppress) action groups from your fired alerts.   finnishr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 32,
    "id": "2bcb781762abe2608a75fc6f48bdb6d53ba4de61bd1e6336857a8aeec82f8d4a"
  },
  {
    "question": "You are developing an online game that includes a feature that allows players to interact with other players on the same team within a certain distance. The calculation to determine the players in range occurs when players move and are cached in an Azure Cache for Redis instance. The system should prioritize players based on how recently they have moved and should not prioritize players who have logged out of the game. You need to select an eviction policy. Which eviction policy should you use?",
    "options": [
      "allkeys-Iru",
      "volatile-Iru",
      "allkeys-lfu",
      "volatile-ttl"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  baroo1",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 33,
    "id": "08414b4ea941cfe84fa7c570f35ca3e16fcc1ad60aab9057fa0c4ed65dd0f666"
  },
  {
    "question": "You develop an Azure App Service web app and deploy to a production environment. You enable Application Insights for the web app. The web app is throwing multiple exceptions in the environment. You need to examine the state of the source code and variables when the exceptions are thrown. Which Application Insights feature should you configure?",
    "options": [
      "Smart detection",
      "Profiler",
      "Snapshot Debugger",
      "Standard test"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "  alexein74",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 34,
    "id": "b06fcf4231534cb61e2e4d37f9fd8b3771268d3cdff52e70fcaafde3cfa5586c"
  },
  {
    "question": "You are building an application to track cell towers that are available to phones in near real time. A phone will send information to the application by using the Azure Web PubSub service. The data will be processed by using an Azure Functions app. Trafic will be transmitted by using a content delivery network (CDN). The Azure function must be protected against misconfigured or unauthorized invocations. You need to ensure that the CDN allows for the Azure function protection. Which HTTP header should be on the allowed list?",
    "options": [
      "Authorization",
      "WebHook-Request-Callback",
      "Resource",
      "WebHook-Request-Origin"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "  halfway",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 40,
    "id": "5719b0689547a0a680dfdb475e4fdc7c6a6049ba9b5178b5bef9bb461a76d6bd"
  },
  {
    "question": "You have an Azure API Management (APIM) Standard tier instance named APIM1 that uses a managed gateway. You plan to use APIM1 to publish an API named API1 that uses a backend database that supports only a limited volume of requests per minute. You also need a policy for API1 that will minimize the possibility that the number of requests to the backend database from an individual IP address you specify exceeds the supported limit. You need to identify a policy for API1 that will meet the requirements. Which policy should you use?",
    "options": [
      "ip-filter",
      "quota-by-key",
      "rate-limit-by-key",
      "rate-limit"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "  dddddd111 6 months ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 44,
    "id": "401c551f73787129b90ca0a282bdc5527e60eae62b31541d6962ac107e6c4816"
  },
  {
    "question": "You develop a web application that sells access to last-minute openings for child camps that run on the weekends. The application uses Azure Application Insights for all alerting and monitoring. The application must alert operators when a technical issue is preventing sales to camps. You need to build an alert to detect technical issues. Which alert type should you use?",
    "options": [
      "Metric alert using multiple time series",
      "Metric alert using dynamic thresholds",
      "Log alert using multiple time series",
      "Log alert using dynamic thresholds"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  Ciupaz",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 45,
    "id": "7df428b8091c50761fda20f3b5f0f7ad83cfd5f5696100d01f6db2e009f8c496"
  },
  {
    "question": "Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Munson’s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms. Current Environment - The company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff. Corporate website - • The company hosts a public website located at The site supports farmers and distributors who request agricultural production resources. Farms - • The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications. Distributors - • Distributors integrate their applications with data that is accessible by using APIs hosted at to receive and update resource data. Requirements - The application components must meet the following requirements: Corporate website - • The site must be migrated to Azure App Service. • Costs must be minimized when hosting in Azure. • Applications must automatically scale independent of the compute resources. • All code changes must be validated by internal staff before release to production. • File transfer speeds must improve, and webpage-load performance must increase. • All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit. • A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests. Farms - • Farmers must authenticate to applications by using Microsoft Entra ID. Distributors - • The company must track a custom telemetry value with each API call and monitor performance of all APIs. • API telemetry values must be charted to evaluate variations and trends for resource data. Internal staff - • App and API updates must be validated before release to production. • Staff must be able to select a link to direct them back to the production app when validating an app or API update. • Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID. Security - • All web communications must be secured by using TLS/HTTPS. • Web content must be restricted by country/region to support corporate compliance standards. • The principle of least privilege must be applied when providing any user rights or process access rights. • Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication. Issues - Corporate website - • Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high. • Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking trafic are high. • Internal staff report webpage load sizes are large and take a long time to load. • Developers receive authentication errors to Service Bus when they debug locally. Distributors - • Many API telemetry values are sent in a short period of time. Telemetry trafic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs. You need to implement an aggregate of telemetry values for distributor API calls. Which Application Insights API method should you use?",
    "options": [
      "TrackEvent",
      "TrackDependency",
      "TrackMetric",
      "TrackException",
      "TrackTrace"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "  AzDeveloper 3 months, 1 week ago It seems C is correct answer",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 46,
    "id": "72852e3f16cc6d11187f4c1dbb2a186e5ffb2fffadc79f223962e02faf22b9b4"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data. You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future. You need to implement a solution to receive the device data. Solution: Provision an Azure Service Bus. Configure a topic to receive the device data by using a correlation filter. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "A message is raw data produced by a service to be consumed or stored elsewhere. The Service Bus is for high-value enterprise messaging, and is used for order processing and financial transactions.   Spooky7",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "43fed998fea6af9326407409b0de458af62688d4d1648b658017ccee815b01f7"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data. You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future. You need to implement a solution to receive the device data. Solution: Provision an Azure Event Grid. Configure event filtering to evaluate the device identifier. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead use an Azure Service Bus, which is used order processing and financial transactions. Note: An event is a lightweight notification of a condition or a state change. Event hubs is usually used reacting to status changes.   xRiot007",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "d003730c67be1ce647f043a1303dcad18a6392ffa0603639568304a1ac1e7272"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently. You have the following requirements: ✑ Queue size must not grow larger than 80 gigabytes (GB). ✑ Use first-in-first-out (FIFO) ordering of messages. ✑ Minimize Azure costs. You need to implement the messaging solution. Solution: Use the .Net API to add a message to an Azure Storage Queue from the mobile application. Create an Azure Function App that uses an Azure Storage Queue trigger. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Create an Azure Function App that uses an Azure Service Bus Queue trigger.   Laaptu",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 7,
    "id": "99fe8f411142d9807b9b00948bfe39e7cb92828b091994023b4908b4818710ff"
  },
  {
    "question": "A company is implementing a publish-subscribe (Pub/Sub) messaging component by using Azure Service Bus. You are developing the first subscription application. In the Azure portal you see that messages are being sent to the subscription for each topic. You create and initialize a subscription client object by supplying the correct details, but the subscription application is still not consuming the messages. You need to ensure that the subscription client processes all messages. Which code segment should you use?",
    "options": [
      "await subscriptionClient.AddRuleAsync(new RuleDescription(RuleDescription.DefaultRuleName, new TrueFilter()));",
      "subscriptionClient = new SubscriptionClient(ServiceBusConnectionString, TopicName, SubscriptionName);",
      "await subscriptionClient.CloseAsync();",
      "subscriptionClient.RegisterMessageHandler(ProcessMessagesAsync, messageHandlerOptions);"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Using topic client, call RegisterMessageHandler which is used to receive messages continuously from the entity. It registers a message handler and begins a new thread to receive messages. This handler is waited on every time a new message is received by the receiver. subscriptionClient.RegisterMessageHandler(ReceiveMessagesAsync, messageHandlerOptions);   Ritesh073",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 15,
    "id": "291f3c1870805dde99098c713108a6a0e89b95bf26112155e2421390157d61cf"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently. You have the following requirements: ✑ Queue size must not grow larger than 80 gigabytes (GB). ✑ Use first-in-first-out (FIFO) ordering of messages. ✑ Minimize Azure costs. You need to implement the messaging solution. Solution: Use the .Net API to add a message to an Azure Storage Queue from the mobile application. Create an Azure VM that is triggered from Azure Storage Queue events. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.   Lkk51",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 16,
    "id": "94dfa3c079e838ba300c83f63fb61d3e39bf0f559dd077a1df869f97ec89106c"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently. You have the following requirements: ✑ Queue size must not grow larger than 80 gigabytes (GB). ✑ Use first-in-first-out (FIFO) ordering of messages. ✑ Minimize Azure costs. You need to implement the messaging solution. Solution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Windows VM that is triggered from Azure Service Bus Queue. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.   cbn",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 17,
    "id": "8462011280f761f9c00aa6ec7ed7800177aa66deef0449e319f55854a98e8023"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data. You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future. You need to implement a solution to receive the device data. Solution: Provision an Azure Event Hub. Configure the machine identifier as the partition key and enable capture. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  edengoforit",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 20,
    "id": "5641576915a015a17b314da6305f1b8d1144826ecb7579abe6beaace0fa4815d"
  },
  {
    "question": "You are developing an e-commerce solution that uses a microservice architecture. You need to design a communication backplane for communicating transactional messages between various parts of the solution. Messages must be communicated in first-in-first-out (FIFO) order. What should you use?",
    "options": [
      "Azure Storage Queue",
      "Azure Event Hub",
      "Azure Service Bus",
      "Azure Event Grid"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "As a solution architect/developer, you should consider using Service Bus queues when: ✑ Your solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.   rahulrai19",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 26,
    "id": "b69f0df04eb3eaf7024dc959d0a6b730706b35db4caf0ea7fc453ed70945a2eb"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently. You have the following requirements: ✑ Queue size must not grow larger than 80 gigabytes (GB). ✑ Use first-in-first-out (FIFO) ordering of messages. ✑ Minimize Azure costs. You need to implement the messaging solution. Solution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Function App that uses an Azure Service Bus Queue trigger. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "You can create a function that is triggered when messages are submitted to an Azure Storage queue.   gunencali",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 28,
    "id": "2dbfa7a9f0fe763044760082c92e738e8af7bb9f8efbe6428c1b05c79e933aee"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data. You must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future. You need to implement a solution to receive the device data. Solution: Provision an Azure Notification Hub. Register all devices with the hub. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Instead use an Azure Service Bus, which is used order processing and financial transactions.   Justing_Gao",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 29,
    "id": "eb585a2c60b8be5ac2b331df4b26ce41fde041616d4590cd06b2469ded3154e0"
  },
  {
    "question": "You are building a loyalty program for a major snack producer. When customers buy a snack at any of 100 participating retailers the event is recorded in Azure Event Hub. Each retailer is given a unique identifier that is used as the primary identifier for the loyalty program. Retailers must be able to be added or removed at any time. Retailers must only be able to record sales for themselves. You need to ensure that retailers can record sales. What should you do?",
    "options": [
      "Use publisher policies for retailers.",
      "Create a partition for each retailer.",
      "Define a namespace for each retailer."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Event Hubs enables granular control over event publishers through publisher policies. Publisher policies are run-time features designed to facilitate large numbers of independent event publishers. With publisher policies, each publisher uses its own unique identifier when publishing events to an event hub. Incorrect: Not C: An Event Hubs namespace is a management container for event hubs (or topics, in Kafka parlance). It provides DNS-integrated network endpoints and a range of access control and network integration management features such as IP filtering, virtual network service endpoint, and Private Link.   finnishr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 30,
    "id": "7d8c77c39485488d5c6f488f2ff1e21b27684beb690c831eb3b470f742ea4c97"
  },
  {
    "question": "You are developing a road tollway tracking application that sends tracking events by using Azure Event Hubs using premium tier. Each road must have a throttling policy uniquely assigned. You need to configure the event hub to allow for per-road throttling. What should you do?",
    "options": [
      "Use a unique consumer group for each road.",
      "Ensure each road stores events in a different partition.",
      "Ensure each road has a unique connection string.",
      "Use a unique application group for each road."
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  le129",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 33,
    "id": "b9028953149540657ec848e12cddcc16fd5e64a792c7a3f958068dc8c95a62b3"
  },
  {
    "question": "You are building a B2B web application that uses Azure B2B collaboration for authentication. Paying customers authenticate to Azure B2B using federation. The application allows users to sign up for trial accounts using any email address. When a user converts to a paying customer, the data associated with the trial should be kept, but the user must authenticate using federation. You need to update the user in Azure Active Directory (Azure AD) when they convert to a paying customer. Which Graph API parameter is used to change authentication from one-time passcodes to federation?",
    "options": [
      "resetRedemption",
      "Status",
      "userFlowType",
      "invitedUser"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  finnishr",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 35,
    "id": "016aaa31f3aad6ad3bf07fd49638801cf97fa24ee04f5af8464a2681d971c207"
  },
  {
    "question": "You are developing several Azure API Management (APIM) hosted APIs. The APIs have the following requirements: • Require a subscription key to access all APIs. • Include terms of use that subscribers must accept to use the APIs. • Administrators must review and accept or reject subscription attempts. • Limit the count of multiple simultaneous subscriptions. You need to implement the APIs. What should you do?",
    "options": [
      "Configure and apply header-based versioning.",
      "Create and publish a product.",
      "Configure and apply query string-based versioning.",
      "Add a new revision to all APIs. Make the revisions current and add a change log entry."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "  HoneyMax",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 43,
    "id": "d5c97d6e84e8a000f0ef117a9cff807df38ef9a4b11bbf497c9c94c8d05a6ea8"
  },
  {
    "question": "You are developing several Azure API Management (APIM) hosted APIs. You must make several minor and non-breaking changes to one of the APIs. The API changes include the following requirements: • Must not disrupt callers of the API. • Enable roll back if you find issues. • Documented to enable developers to understand what is new. • Tested before publishing. You need to update the API. What should you do?",
    "options": [
      "Configure and apply header-based versioning.",
      "Create and publish a product.",
      "Configure and apply a custom policy.",
      "Add a new revision to the API.",
      "Configure and apply query string-based versioning."
    ],
    "answerIndexes": [
      4
    ],
    "answer": "  james2033 4 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 50,
    "id": "0c1500d09f95925a275a75eeb3c2efcb1607040f93db2ddaba56b243796b19a0"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are implementing an application by using Azure Event Grid to push near-real-time information to customers. You have the following requirements: • You must send events to thousands of customers that include hundreds of various event types. • The events must be filtered by event type before processing. • Authentication and authorization must be handled by using Microsoft Entra ID. • The events must be published to a single endpoint. You need to implement Azure Event Grid. Solution: Publish events to an event domain. Create a custom topic for each customer. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  FeriAZ 2 months, 1 week ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 56,
    "id": "7b026b24ea9bb09dc6a6b2cd2559d93241f40955277f778d7a948668ef53ccb0"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are implementing an application by using Azure Event Grid to push near-real-time information to customers. You have the following requirements: • You must send events to thousands of customers that include hundreds of various event types. • The events must be filtered by event type before processing. • Authentication and authorization must be handled by using Microsoft Entra ID. • The events must be published to a single endpoint. You need to implement Azure Event Grid. Solution: Publish events to a custom topic. Create an event subscription for each customer. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  FeriAZ 1 month, 3 weeks ago Azure Event Grid Domains Event subscriptions within the domain can then be configured for each customer or customer group, with filtering rules applied to ensure that subscribers only receive the events that are relevant to them. This architecture effectively addresses the need for event filtering before processing.",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 57,
    "id": "2bc2a87b10511fa5a15da0a19469d9b051e7aff1656d74ac219771555a669d6b"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are implementing an application by using Azure Event Grid to push near-real-time information to customers. You have the following requirements: • You must send events to thousands of customers that include hundreds of various event types. • The events must be filtered by event type before processing. • Authentication and authorization must be handled by using Microsoft Entra ID. • The events must be published to a single endpoint. You need to implement Azure Event Grid. Solution: Enable ingress, create a TCP scale rule, and apply the rule to the container app. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  james2033 4 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 58,
    "id": "7e9e6e7b208e1f5a5038a2063888b226579f270ba285a8c286bce26ea3cd3b75"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are implementing an application by using Azure Event Grid to push near-real-time information to customers. You have the following requirements: • You must send events to thousands of customers that include hundreds of various event types. • The events must be filtered by event type before processing. • Authentication and authorization must be handled by using Microsoft Entra ID. • The events must be published to a single endpoint. You need to implement Azure Event Grid. Solution: Publish events to a partner topic. Create an event subscription for each customer. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "  james2033 4 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 60,
    "id": "ea202b969daad5e4c40a896514570b3b5d5510066f55d77968f83ad4cf8838f7"
  },
  {
    "question": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution. After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are implementing an application by using Azure Event Grid to push near-real-time information to customers. You have the following requirements: • You must send events to thousands of customers that include hundreds of various event types. • The events must be filtered by event type before processing. • Authentication and authorization must be handled by using Microsoft Entra ID. • The events must be published to a single endpoint. You need to implement Azure Event Grid. Solution: Publish events to a system topic. Create an event subscription for each customer. Does the solution meet the goal?",
    "options": [
      "Yes",
      "No"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "  james2033 4 weeks ago",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 61,
    "id": "a69c7c0a07de57349afae81b4384d7fa53a3f68bcc044e2a96b7cecaf4d70d03"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - VanArsdel, Ltd. is a global ofice supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services. Current environment - Corporate website - The company provides a public website located at The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions. Retail Store Locations - The company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information. Requirements - The application components must meet the following requirements: Corporate website - Secure the website by using SSL. Minimize costs for data storage and hosting. Implement native GitHub workfiows for continuous integration and continuous deployment (CI/CD). Distribute the website content globally for local use. Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification. The website must have 99.95 percent uptime. Retail store locations - Azure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries. Audit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory. Delivery services - Store service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates. Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website. Inventory services - The company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data. Security - All Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key. Authentication and authorization must use Azure AD and services must use managed identities where possible. Issues - Retail Store Locations - You must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data. Azure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling. Question You need to grant access to the retail store location data for the inventory service development effort. What should you use?",
    "options": [
      "Azure AD access token",
      "Azure RBAC role",
      "Shared access signature (SAS) token",
      "Azure AD ID token",
      "Azure AD refresh token"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "A shared access signature (SAS) provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example: What resources the client may access. What permissions they have to those resources. How long the SAS is valid. Note: Inventory services: The company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.   kampatra",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "3a12b1d92340c6851985862c012220e93230b71ac6ce05b6a3f09b094368d4b8"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Wide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible. Current environment - Windows Server 2016 virtual machine This virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workfiows: Ocean Transport `\" This workfiow gathers and validates container information including container contents and arrival notices at various shipping ports. Inland Transport `\" This workfiow gathers and validates trucking information including fuel usage, number of stops, and routes. The VM supports the following REST API calls: Container API `\" This API provides container information including weight, contents, and other attributes. Location API `\" This API provides location information regarding shipping ports of call and trucking stops. Shipping REST API `\" This API provides shipping information for use and display on the shipping website. Shipping Data - The application uses MongoDB JSON document storage database for all container and transport information. Shipping Web Site - The site displays shipping container tracking information and container contents. The site is located at Proposed solution - The on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below: Requirements - Shipping Logic app - The Shipping Logic app must meet the following requirements: Support the ocean transport and inland transport workfiows by using a Logic App. Support industry-standard protocol X12 message format for various messages including vessel content details and arrival notices. Secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model. Maintain on-premises connectivity to support legacy applications and final BizTalk migrations. Shipping Function app - Implement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD). REST APIs - The REST API's that support the solution must meet the following requirements: Secure resources to the corporate VNet. Allow deployment to a testing location within Azure while not incurring additional costs. Automatically scale to double capacity during peak shipping times while not causing application downtime. Minimize costs when selecting an Azure payment model. Shipping data - Data migration from on-premises to Azure must minimize costs and downtime. Shipping website - Use Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs. Issues - Windows Server 2016 VM - The VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure. Shipping website and REST APIs - The following error message displays while you are testing the website: Failed to load No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin ' is therefore not allowed access. Question You need to secure the Shipping Logic App. What should you use?",
    "options": [
      "Azure App Service Environment (ASE)",
      "Integration Service Environment (ISE)",
      "VNet service endpoint",
      "Azure AD B2B integration"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Scenario: The Shipping Logic App requires secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model. You can access to Azure Virtual Network resources from Azure Logic Apps by using integration service environments (ISEs). Sometimes, your logic apps and integration accounts need access to secured resources, such as virtual machines (VMs) and other systems or services, that are inside an Azure virtual network. To set up this access, you can create an integration service environment (ISE) where you can run your logic apps and create your integration accounts.   NKnab",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "6955125d9fd33d698274d774f1af78b2a75f5b0997a76c8fcb7c816f8d262bd1"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - You are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App. Overall architecture - Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement. Receipt processing - Employees may upload receipts in two ways: Uploading using an Azure Files mounted folder Uploading using the web application Data Storage - Receipt and employee information is stored in an Azure SQL database. Documentation - Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder. Solution details - Users table - Web Application - You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity. Processing - Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user. Logging - Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled. Application Insights must always contain all log messages. Requirements - Receipt processing - Concurrent processing of a receipt must be prevented. Disaster recovery - Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date. Security - User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins. All certificates and secrets used to secure data must be stored in Azure Key Vault. You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function. All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI). Receipt data must always be encrypted at rest. All data must be protected in transit. User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured. In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system. Issues - Upload format issue - Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page. Capacity issue - During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application. Log capacity issue - Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages. Application code - Processing.cs - Database.cs - ReceiptUploader.cs - ConfigureSSE.ps1 - Question You need to ensure the security policies are met. What code do you add at line CS07 of ConfigureSSE.ps1?",
    "options": [
      "ג€\"PermissionsToKeys create, encrypt, decrypt",
      "ג€\"PermissionsToCertificates create, encrypt, decrypt",
      "ג€\"PermissionsToCertificates wrapkey, unwrapkey, get",
      "ג€\"PermissionsToKeys wrapkey, unwrapkey, get"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Scenario: All certificates and secrets used to secure data must be stored in Azure Key Vault. You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function. The Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToKeys specifies an array of key operation permissions to grant to a user or service principal. The acceptable values for this parameter: decrypt, encrypt, unwrapKey, wrapKey, verify, sign, get, list, update, create, import, delete, backup, restore, recover, purge Incorrect Answers: A, C: The Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToCertificates specifies an array of certificate permissions to grant to a user or service principal. The acceptable values for this parameter: get, list, delete, create, import, update, managecontacts, getissuers, listissuers, setissuers, deleteissuers, manageissuers, recover, purge, backup, restore",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "66af76f80f266ee755dd12c70b6c74aed5780891d2fc66ad88b67b8865920af5"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Overview - You are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage. You are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA: * Messages are sent to ContentUploadService. * Content is processed by ContentAnalysisService. * After processing is complete, the content is posted to the social network or a rejection message is posted in its place. The ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages. The solution will use eight CPU cores. Azure Active Directory - Contoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts. Requirements - ContentAnalysisService - The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd. You must create an Azure Function named CheckUserContent to perform the content checks. Costs - You must minimize costs for all Azure services. Manual review - To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes. High availability - All services must run in multiple regions. The failure of any service in a region must not impact overall application availability. Monitoring - An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores. Security - You have the following security requirements: Any web service accessible over the Internet must be protected from cross site scripting attacks. All websites and services must use SSL from a valid root certificate authority. Azure Storage access keys must only be stored in memory and must be available only to the service. All Internal services must only be accessible from internal Virtual Networks (VNets). All parts of the system must support inbound and outbound trafic restrictions. All service calls must be authenticated by using Azure AD. User agreements - When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses. Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour. Validation testing - When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version. Issues - Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. Code - ContentUploadService - ApplicationManifest - Question You need to monitor ContentUploadService according to the requirements. Which command should you use?",
    "options": [
      "az monitor metrics alert create ג€\"n alert ג€\"g ג€¦ - -scopes ג€¦ - -condition \"avg Percentage CPU > 8\"",
      "az monitor metrics alert create ג€\"n alert ג€\"g ג€¦ - -scopes ג€¦ - -condition \"avg Percentage CPU > 800\"",
      "az monitor metrics alert create ג€\"n alert ג€\"g ג€¦ - -scopes ג€¦ - -condition \"CPU Usage > 800\"",
      "az monitor metrics alert create ג€\"n alert ג€\"g ג€¦ - -scopes ג€¦ - -condition \"CPU Usage > 8\""
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Scenario: An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.   anirbanzeus",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "14deee1f780476f9af16ed101116da5718d6dd316fc437910d05f8f00e92b8ff"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Overview - You are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage. You are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA: * Messages are sent to ContentUploadService. * Content is processed by ContentAnalysisService. * After processing is complete, the content is posted to the social network or a rejection message is posted in its place. The ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages. The solution will use eight CPU cores. Azure Active Directory - Contoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts. Requirements - ContentAnalysisService - The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd. You must create an Azure Function named CheckUserContent to perform the content checks. Costs - You must minimize costs for all Azure services. Manual review - To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes. High availability - All services must run in multiple regions. The failure of any service in a region must not impact overall application availability. Monitoring - An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores. Security - You have the following security requirements: Any web service accessible over the Internet must be protected from cross site scripting attacks. All websites and services must use SSL from a valid root certificate authority. Azure Storage access keys must only be stored in memory and must be available only to the service. All Internal services must only be accessible from internal Virtual Networks (VNets). All parts of the system must support inbound and outbound trafic restrictions. All service calls must be authenticated by using Azure AD. User agreements - When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses. Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour. Validation testing - When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version. Issues - Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. Code - ContentUploadService - ApplicationManifest - Question You need to investigate the http server log output to resolve the issue with the ContentUploadService. Which command should you use first?",
    "options": [
      "az webapp log",
      "az ams live-output",
      "az monitor activity-log",
      "az container attach"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Scenario: Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. \"502 bad gateway\" and \"503 service unavailable\" are common errors in your app hosted in Azure App Service. Microsoft Azure publicizes each time there is a service interruption or performance degradation. The az monitor activity-log command manages activity logs. Note: Troubleshooting can be divided into three distinct tasks, in sequential order: 1. Observe and monitor application behavior 2. Collect data 3. Mitigate the issue   Whirly",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "29aef849879bc22b74ad27cfafae4c9d260160c8c9364e668a22b25cdf8ce8c3"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - City Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure. Current environment - Architecture overview - The company has a public website located at The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content. API Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and Event Grid handle messaging and events. The solution uses Application Insights, Azure Monitor, and Azure Key Vault. Architecture diagram - The company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below. User authentication - The following steps detail the user authentication process: 1. The user selects Sign in in the website. 2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page. 3. The user signs in. 4. Azure AD redirects the user's session back to the web application. The URL includes an access token. 5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token. 6. The back-end API validates the access token. Requirements - Corporate website - Communications and content must be secured by using SSL. Communications must use HTTPS. Data must be replicated to a secondary region and three availability zones. Data storage costs must be minimized. Azure Database for PostgreSQL - The database connection string is stored in Azure Key Vault with the following attributes: Azure Key Vault name: cpandlkeyvault Secret name: PostgreSQLConn Id: 80df3e46ffcd4f1cb187f79905e9a1e8 The connection information is updated frequently. The application must always use the latest information to connect to the database. Azure Service Bus and Azure Event Grid Azure Event Grid must use Azure Service Bus for queue-based load leveling. Events in Azure Event Grid must be routed directly to Service Bus queues for use in buffering. Events from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing. Security - All SSL certificates and credentials must be stored in Azure Key Vault. File access must restrict access by IP, protocol, and Azure AD rights. All user accounts and processes must receive only those privileges which are essential to perform their intended function. Compliance - Auditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons. Issues - Corporate website - While testing the site, the following error message displays: CryptographicException: The system cannot find the file specified. Function app - You perform local testing for the RequestUserApproval function. The following error message displays: 'Timeout value of 00:10:00 exceeded by function: RequestUserApproval' The same error message displays when you test the function in an Azure development environment when you run the following Kusto query: FunctionAppLogs - | where FunctionName = = \"RequestUserApproval\" Logic app - You test the Logic app in a development environment. The following error message displays: '400 Bad Request' Troubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function. Code - Corporate website - Security.cs: Function app - RequestUserApproval.cs: Question You need to investigate the Azure Function app error message in the development environment. What should you do?",
    "options": [
      "Connect Live Metrics Stream from Application Insights to the Azure Function app and filter the metrics.",
      "Create a new Azure Log Analytics workspace and instrument the Azure Function app with Application Insights.",
      "Update the Azure Function app with extension methods from Microsoft.Extensions.Logging to log events by using the log instance.",
      "Add a new diagnostic setting to the Azure Function app to send logs to Log Analytics."
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Azure Functions offers built-in integration with Azure Application Insights to monitor functions. The following areas of Application Insights can be helpful when evaluating the behavior, performance, and errors in your functions: Live Metrics: View metrics data as it's created in near real-time. Failures - Performance - Metrics -   Deep007",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "c7205d1d1b7851bfc37d58f5b8919e87b694ea1b8bb86857776296dad89cc0d6"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - City Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure. Current environment - Architecture overview - The company has a public website located at The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content. API Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and Event Grid handle messaging and events. The solution uses Application Insights, Azure Monitor, and Azure Key Vault. Architecture diagram - The company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below. User authentication - The following steps detail the user authentication process: 1. The user selects Sign in in the website. 2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page. 3. The user signs in. 4. Azure AD redirects the user's session back to the web application. The URL includes an access token. 5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token. 6. The back-end API validates the access token. Requirements - Corporate website - Communications and content must be secured by using SSL. Communications must use HTTPS. Data must be replicated to a secondary region and three availability zones. Data storage costs must be minimized. Azure Database for PostgreSQL - The database connection string is stored in Azure Key Vault with the following attributes: Azure Key Vault name: cpandlkeyvault Secret name: PostgreSQLConn Id: 80df3e46ffcd4f1cb187f79905e9a1e8 The connection information is updated frequently. The application must always use the latest information to connect to the database. Azure Service Bus and Azure Event Grid Azure Event Grid must use Azure Service Bus for queue-based load leveling. Events in Azure Event Grid must be routed directly to Service Bus queues for use in buffering. Events from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing. Security - All SSL certificates and credentials must be stored in Azure Key Vault. File access must restrict access by IP, protocol, and Azure AD rights. All user accounts and processes must receive only those privileges which are essential to perform their intended function. Compliance - Auditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons. Issues - Corporate website - While testing the site, the following error message displays: CryptographicException: The system cannot find the file specified. Function app - You perform local testing for the RequestUserApproval function. The following error message displays: 'Timeout value of 00:10:00 exceeded by function: RequestUserApproval' The same error message displays when you test the function in an Azure development environment when you run the following Kusto query: FunctionAppLogs - | where FunctionName = = \"RequestUserApproval\" Logic app - You test the Logic app in a development environment. The following error message displays: '400 Bad Request' Troubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function. Code - Corporate website - Security.cs: Function app - RequestUserApproval.cs: Question You need to correct the RequestUserApproval Function app error. What should you do?",
    "options": [
      "Update line RA13 to use the async keyword and return an HttpRequest object value.",
      "Configure the Function app to use an App Service hosting plan. Enable the Always On setting of the hosting plan.",
      "Update the function to be stateful by using Durable Functions to process the request payload.",
      "Update the functionTimeout property of the host.json project file to 15 minutes."
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Async operation tracking - The HTTP response mentioned previously is designed to help implement long-running HTTP async APIs with Durable Functions. This pattern is sometimes referred to as the polling consumer pattern. Both the client and server implementations of this pattern are built into the Durable Functions HTTP APIs. Function app - You perform local testing for the RequestUserApproval function. The following error message displays: 'Timeout value of 00:10:00 exceeded by function: RequestUserApproval' The same error message displays when you test the function in an Azure development environment when you run the following Kusto query: FunctionAppLogs - | where FunctionName = = \"RequestUserApproval\"   rdemontis",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "d2e9a2f3a8ec5017f3a859c743c35248ccd8885aebfc9e359e517f315caa2f4c"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - You are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality. Requirements - Policy service - You develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events. The application must include the Event Grid Event ID field in all Application Insights telemetry. Policy service must use Application Insights to automatically scale with the number of policy actions that it is performing. Policies - Log policy - All Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days. Authentication events - Authentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible. PolicyLib - You have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must: Exclude non-user actions from Application Insights telemetry. Provide methods that allow a web service to scale itself. Ensure that scaling actions do not disrupt application usage. Other - Anomaly detection service - You have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook. Health monitoring - All web applications and services have health monitoring at the /health service endpoint. Issues - Policy loss - When you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment. Performance issue - When under heavy load, the anomaly detection service undergoes slowdowns and rejects connections. Notification latency - Users report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected. App code - EventGridController.cs - Relevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong. LoginEvent.cs - Relevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong. Question You need to ensure that the solution can meet the scaling requirements for Policy Service. Which Azure Application Insights data model should you use?",
    "options": [
      "an Application Insights dependency",
      "an Application Insights event",
      "an Application Insights trace",
      "an Application Insights metric"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "Application Insights provides three additional data types for custom telemetry: Trace - used either directly, or through an adapter to implement diagnostics logging using an instrumentation framework that is familiar to you, such as Log4Net or System.Diagnostics. Event - typically used to capture user interaction with your service, to analyze usage patterns. Metric - used to report periodic scalar measurements. Scenario: Policy service must use Application Insights to automatically scale with the number of policy actions that it is performing.   Kitkit",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "5a5f302c2ad6973b393f894a8284d486569d036bfbb68065b4abe14247985f8b"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Background - You are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App. Overall architecture - Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement. Receipt processing - Employees may upload receipts in two ways: Uploading using an Azure Files mounted folder Uploading using the web application Data Storage - Receipt and employee information is stored in an Azure SQL database. Documentation - Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder. Solution details - Users table - Web Application - You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity. Processing - Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user. Logging - Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled. Application Insights must always contain all log messages. Requirements - Receipt processing - Concurrent processing of a receipt must be prevented. Disaster recovery - Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date. Security - User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins. All certificates and secrets used to secure data must be stored in Azure Key Vault. You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function. All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI). Receipt data must always be encrypted at rest. All data must be protected in transit. User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured. In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system. Issues - Upload format issue - Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page. Capacity issue - During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application. Log capacity issue - Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages. Application code - Processing.cs - Database.cs - ReceiptUploader.cs - ConfigureSSE.ps1 - Question You need to ensure receipt processing occurs correctly. What should you do?",
    "options": [
      "Use blob properties to prevent concurrency problems",
      "Use blob SnapshotTime to prevent concurrency problems",
      "Use blob metadata to prevent concurrency problems",
      "Use blob leases to prevent concurrency problems"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "You can create a snapshot of a blob. A snapshot is a read-only version of a blob that's taken at a point in time. Once a snapshot has been created, it can be read, copied, or deleted, but not modified. Snapshots provide a way to back up a blob as it appears at a moment in time. Scenario: Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "0cb6760c0feced86c78bac0c9045d651f985ed17d0b36c53e806ad612db964ce"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Background - You are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App. Overall architecture - Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement. Receipt processing - Employees may upload receipts in two ways: Uploading using an Azure Files mounted folder Uploading using the web application Data Storage - Receipt and employee information is stored in an Azure SQL database. Documentation - Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder. Solution details - Users table - Web Application - You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity. Processing - Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user. Logging - Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled. Application Insights must always contain all log messages. Requirements - Receipt processing - Concurrent processing of a receipt must be prevented. Disaster recovery - Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date. Security - User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins. All certificates and secrets used to secure data must be stored in Azure Key Vault. You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function. All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI). Receipt data must always be encrypted at rest. All data must be protected in transit. User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured. In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system. Issues - Upload format issue - Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page. Capacity issue - During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application. Log capacity issue - Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages. Application code - Processing.cs - Database.cs - ReceiptUploader.cs - ConfigureSSE.ps1 - Question You need to resolve the capacity issue. What should you do?",
    "options": [
      "Convert the trigger on the Azure Function to an Azure Blob storage trigger",
      "Ensure that the consumption plan is configured correctly to allow scaling",
      "Move the Azure Function to a dedicated App Service Plan",
      "Update the loop starting on line PC09 to process items in parallel"
    ],
    "answerIndexes": [
      3
    ],
    "answer": "If you want to read the files in parallel, you cannot use forEach. Each of the async callback function calls does return a promise. You can await the array of promises that you'll get with Promise.all. Scenario: Capacity issue: During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.   trance13",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "1c5b2179a9cc6fdc69f18d78638255cb559543a6994d5fde6c397eb366789c1b"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question. Background - You are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web API project that is deployed as an Azure Web App. Overall architecture - Employees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement. Receipt processing - Employees may upload receipts in two ways: Uploading using an Azure Files mounted folder Uploading using the web application Data Storage - Receipt and employee information is stored in an Azure SQL database. Documentation - Employees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for Azure File upload, and instructions on how to configure the mounted folder. Solution details - Users table - Web Application - You enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity. Processing - Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user. Logging - Azure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled. Application Insights must always contain all log messages. Requirements - Receipt processing - Concurrent processing of a receipt must be prevented. Disaster recovery - Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date. Security - User's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins. All certificates and secrets used to secure data must be stored in Azure Key Vault. You must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function. All access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI). Receipt data must always be encrypted at rest. All data must be protected in transit. User's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured. In the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system. Issues - Upload format issue - Employees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File Share, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal Server error page. Capacity issue - During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application. Log capacity issue - Developers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages. Application code - Processing.cs - Database.cs - ReceiptUploader.cs - ConfigureSSE.ps1 - Question You need to resolve the log capacity issue. What should you do?",
    "options": [
      "Create an Application Insights Telemetry Filter",
      "Change the minimum log level in the host.json file for the function",
      "Implement Application Insights Sampling",
      "Set a LogCategoryFilter during startup"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Scenario, the log capacity issue: Developers report that the number of log message in the trace output for the processor is too high, resulting in lost log messages. Sampling is a feature in Azure Application Insights. It is the recommended way to reduce telemetry trafic and storage, while preserving a statistically correct analysis of application data. The filter selects items that are related, so that you can navigate between items when you are doing diagnostic investigations. When metric counts are presented to you in the portal, they are renormalized to take account of the sampling, to minimize any effect on the statistics. Sampling reduces trafic and data costs, and helps you avoid throttling.",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 3,
    "id": "2d6e941c1fb90aa72817b1927f12241333c853518c46649f2b918b8157978c73"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Wide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible. Current environment - Windows Server 2016 virtual machine This virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workfiows: Ocean Transport `\" This workfiow gathers and validates container information including container contents and arrival notices at various shipping ports. Inland Transport `\" This workfiow gathers and validates trucking information including fuel usage, number of stops, and routes. The VM supports the following REST API calls: Container API `\" This API provides container information including weight, contents, and other attributes. Location API `\" This API provides location information regarding shipping ports of call and trucking stops. Shipping REST API `\" This API provides shipping information for use and display on the shipping website. Shipping Data - The application uses MongoDB JSON document storage database for all container and transport information. Shipping Web Site - The site displays shipping container tracking information and container contents. The site is located at Proposed solution - The on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below: Requirements - Shipping Logic app - The Shipping Logic app must meet the following requirements: Support the ocean transport and inland transport workfiows by using a Logic App. Support industry-standard protocol X12 message format for various messages including vessel content details and arrival notices. Secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model. Maintain on-premises connectivity to support legacy applications and final BizTalk migrations. Shipping Function app - Implement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD). REST APIs - The REST API's that support the solution must meet the following requirements: Secure resources to the corporate VNet. Allow deployment to a testing location within Azure while not incurring additional costs. Automatically scale to double capacity during peak shipping times while not causing application downtime. Minimize costs when selecting an Azure payment model. Shipping data - Data migration from on-premises to Azure must minimize costs and downtime. Shipping website - Use Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs. Issues - Windows Server 2016 VM - The VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure. Shipping website and REST APIs - The following error message displays while you are testing the website: Failed to load No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin ' is therefore not allowed access. Question You need to support the requirements for the Shipping Logic App. What should you use?",
    "options": [
      "Azure Active Directory Application Proxy",
      "Site-to-Site (S2S) VPN connection",
      "On-premises Data Gateway",
      "Point-to-Site (P2S) VPN connection"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "Before you can connect to on-premises data sources from Azure Logic Apps, download and install the on-premises data gateway on a local computer. The gateway works as a bridge that provides quick data transfer and encryption between data sources on premises (not in the cloud) and your logic apps. The gateway supports BizTalk Server 2016. Note: Microsoft have now fully incorporated the Azure BizTalk Services capabilities into Logic Apps and Azure App Service Hybrid Connections. Logic Apps Enterprise Integration pack bring some of the enterprise B2B capabilities like AS2 and X12, EDI standards support Scenario: The Shipping Logic app must meet the following requirements: ✑ Support the ocean transport and inland transport workfiows by using a Logic App. ✑ Support industry-standard protocol X12 message format for various messages including vessel content details and arrival notices. ✑ Secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model. ✑ Maintain on-premises connectivity to support legacy applications and final BizTalk migrations.   programmingbot",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "1d57bea73481f16ceae0bc00c636e4fc0fa36d33aff3c5c82c2efe5115c80b86"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - City Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure. Current environment - Architecture overview - The company has a public website located at The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content. API Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and Event Grid handle messaging and events. The solution uses Application Insights, Azure Monitor, and Azure Key Vault. Architecture diagram - The company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below. User authentication - The following steps detail the user authentication process: 1. The user selects Sign in in the website. 2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page. 3. The user signs in. 4. Azure AD redirects the user's session back to the web application. The URL includes an access token. 5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token. 6. The back-end API validates the access token. Requirements - Corporate website - Communications and content must be secured by using SSL. Communications must use HTTPS. Data must be replicated to a secondary region and three availability zones. Data storage costs must be minimized. Azure Database for PostgreSQL - The database connection string is stored in Azure Key Vault with the following attributes: Azure Key Vault name: cpandlkeyvault Secret name: PostgreSQLConn Id: 80df3e46ffcd4f1cb187f79905e9a1e8 The connection information is updated frequently. The application must always use the latest information to connect to the database. Azure Service Bus and Azure Event Grid Azure Event Grid must use Azure Service Bus for queue-based load leveling. Events in Azure Event Grid must be routed directly to Service Bus queues for use in buffering. Events from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing. Security - All SSL certificates and credentials must be stored in Azure Key Vault. File access must restrict access by IP, protocol, and Azure AD rights. All user accounts and processes must receive only those privileges which are essential to perform their intended function. Compliance - Auditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons. Issues - Corporate website - While testing the site, the following error message displays: CryptographicException: The system cannot find the file specified. Function app - You perform local testing for the RequestUserApproval function. The following error message displays: 'Timeout value of 00:10:00 exceeded by function: RequestUserApproval' The same error message displays when you test the function in an Azure development environment when you run the following Kusto query: FunctionAppLogs - | where FunctionName = = \"RequestUserApproval\" Logic app - You test the Logic app in a development environment. The following error message displays: '400 Bad Request' Troubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function. Code - Corporate website - Security.cs: Function app - RequestUserApproval.cs: Question You need to ensure that all messages from Azure Event Grid are processed. What should you use?",
    "options": [
      "Azure Event Grid topic",
      "Azure Service Bus topic",
      "Azure Service Bus queue",
      "Azure Storage queue",
      "Azure Logic App custom connector"
    ],
    "answerIndexes": [
      2
    ],
    "answer": "As a solution architect/developer, you should consider using Service Bus queues when: ✑ Your solution needs to receive messages without having to poll the queue. With Service Bus, you can achieve it by using a long-polling receive operation using the TCP-based protocols that Service Bus supports.   BrettusMaximus",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "6d5e8c03fa7910adfb2f4a6fc5165a3303a750189498bd14588f0ff428b746c9"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Overview - You are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage. You are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA: Messages are sent to ContentUploadService. Content is processed by ContentAnalysisService. After processing is complete, the content is posted to the social network or a rejection message is posted in its place. The ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages. The solution will use eight CPU cores. Azure Active Directory - Contoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts. Requirements - ContentAnalysisService - The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd. You must create an Azure Function named CheckUserContent to perform the content checks. Costs - You must minimize costs for all Azure services. Manual review - To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes. High availability - All services must run in multiple regions. The failure of any service in a region must not impact overall application availability. Monitoring - An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores. Security - You have the following security requirements: Any web service accessible over the Internet must be protected from cross site scripting attacks. All websites and services must use SSL from a valid root certificate authority. Azure Storage access keys must only be stored in memory and must be available only to the service. All Internal services must only be accessible from internal Virtual Networks (VNets). All parts of the system must support inbound and outbound trafic restrictions. All service calls must be authenticated by using Azure AD. User agreements - When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses. Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour. Validation testing - When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version. Issues - Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. Code - ContentUploadService - ApplicationManifest - Question You need to deploy the CheckUserContent Azure Function. The solution must meet the security and cost requirements. Which hosting model should you use?",
    "options": [
      "Premium plan",
      "App Service plan",
      "Consumption plan"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Scenario: You must minimize costs for all Azure services. All Internal services must only be accessible from internal Virtual Networks (VNets). Best for long-running scenarios where Durable Functions can't be used. Consider an App Service plan in the following situations: ✑ You have existing, underutilized VMs that are already running other App Service instances. ✑ You want to provide a custom image on which to run your functions. ✑ Predictive scaling and costs are required. Note: When you create a function app in Azure, you must choose a hosting plan for your app. There are three basic hosting plans available for Azure Functions: Consumption plan, Premium plan, and Dedicated (App Service) plan. Incorrect Answers: A: A Premium plan would be more costly. C: Need the VNET functionality.   ning",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 2,
    "id": "d16a1a90b4a94ee88cc6a1c53f4db3b37664cf0c55acdd9a017ccc3514a1e940"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Overview - You are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage. You are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA: * Messages are sent to ContentUploadService. * Content is processed by ContentAnalysisService. * After processing is complete, the content is posted to the social network or a rejection message is posted in its place. The ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages. The solution will use eight CPU cores. Azure Active Directory - Contoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts. Requirements - ContentAnalysisService - The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd. You must create an Azure Function named CheckUserContent to perform the content checks. Costs - You must minimize costs for all Azure services. Manual review - To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes. High availability - All services must run in multiple regions. The failure of any service in a region must not impact overall application availability. Monitoring - An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores. Security - You have the following security requirements: Any web service accessible over the Internet must be protected from cross site scripting attacks. All websites and services must use SSL from a valid root certificate authority. Azure Storage access keys must only be stored in memory and must be available only to the service. All Internal services must only be accessible from internal Virtual Networks (VNets). All parts of the system must support inbound and outbound trafic restrictions. All service calls must be authenticated by using Azure AD. User agreements - When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses. Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour. Validation testing - When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version. Issues - Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. Code - ContentUploadService - ApplicationManifest - Question You need to store the user agreements. Where should you store the agreement after it is completed?",
    "options": [
      "Azure Storage queue",
      "Azure Event Hub",
      "Azure Service Bus topic",
      "Azure Event Grid topic"
    ],
    "answerIndexes": [
      1
    ],
    "answer": "Azure Event Hub is used for telemetry and distributed data streaming. This service provides a single solution that enables rapid data retrieval for real-time processing as well as repeated replay of stored raw data. It can capture the streaming data into a file for processing and analysis. It has the following characteristics: ✑ low latency ✑ capable of receiving and processing millions of events per second ✑ at least once delivery   perry230",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 1,
    "id": "17f83e30fd53570c878a7b24463b51ddb40b86f8e23b2c780b363c9aab1d1131"
  },
  {
    "question": "Introductory Info Case study - This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided. To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study. At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section. To start the case study - To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question. Background - Overview - You are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage. You are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA: * Messages are sent to ContentUploadService. * Content is processed by ContentAnalysisService. * After processing is complete, the content is posted to the social network or a rejection message is posted in its place. The ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages. The solution will use eight CPU cores. Azure Active Directory - Contoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts. Requirements - ContentAnalysisService - The company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd. You must create an Azure Function named CheckUserContent to perform the content checks. Costs - You must minimize costs for all Azure services. Manual review - To review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using React and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes. High availability - All services must run in multiple regions. The failure of any service in a region must not impact overall application availability. Monitoring - An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores. Security - You have the following security requirements: Any web service accessible over the Internet must be protected from cross site scripting attacks. All websites and services must use SSL from a valid root certificate authority. Azure Storage access keys must only be stored in memory and must be available only to the service. All Internal services must only be accessible from internal Virtual Networks (VNets). All parts of the system must support inbound and outbound trafic restrictions. All service calls must be authenticated by using Azure AD. User agreements - When a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses. Information regarding agreements is used by multiple divisions within Contoso, Ltd. User responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour. Validation testing - When a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version. Issues - Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages. Code - ContentUploadService - ApplicationManifest - Question You need to configure the ContentUploadService deployment. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.",
    "options": [
      "Add the following markup to line CS23: type: Private",
      "Add the following markup to line CS24: osType: Windows",
      "Add the following markup to line CS24: osType: Linux",
      "Add the following markup to line CS23: type: Public"
    ],
    "answerIndexes": [
      0
    ],
    "answer": "Scenario: All Internal services must only be accessible from Internal Virtual Networks (VNets) There are three Network Location types ג€\" Private, Public and Domain   Mo_Mo_01",
    "topic": "Azure",
    "hasCode": false,
    "isPdf": true,
    "pdfQuestionNumber": 3,
    "id": "088fd0cdfe10a13f689dd1fd66b6d376cc7a805051c31deec122da6d92061319"
  }
]